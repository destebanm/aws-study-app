[
  {
    "id": "q1",
    "questionText": "Which AWS service offers a simple and scalable file storage for use with Amazon EC2 instances, based on the NFS protocol?",
    "options": [
      {
        "text": "Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Amazon EBS",
        "isCorrect": false
      },
      {
        "text": "Amazon EFS",
        "isCorrect": true
      },
      {
        "text": "Amazon Glacier",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon EFS (Elastic File System) provides a scalable network file system (NFS) that can be mounted on multiple EC2 instances simultaneously.",
    "awsService": "EFS"
  },
  {
    "id": "q2",
    "questionText": "A company wants to run a high-performance computing (HPC) workload. Which EC2 purchasing option is MOST cost-effective for a short-term, interruptible job?",
    "options": [
      {
        "text": "On-Demand Instances",
        "isCorrect": false
      },
      {
        "text": "Reserved Instances",
        "isCorrect": false
      },
      {
        "text": "Spot Instances",
        "isCorrect": true
      },
      {
        "text": "Dedicated Hosts",
        "isCorrect": false
      }
    ],
    "explanation": "Spot Instances offer the largest discounts on EC2 compute capacity and are ideal for fault-tolerant, interruptible workloads like HPC.",
    "awsService": "EC2"
  },
  {
    "id": "q3",
    "questionText": "What is the primary purpose of an Internet Gateway (IGW) in an Amazon VPC?",
    "options": [
      {
        "text": "To provide a private connection between a VPC and another VPC.",
        "isCorrect": false
      },
      {
        "text": "To enable instances in the VPC to access the internet.",
        "isCorrect": true
      },
      {
        "text": "To provide a secure connection to an on-premises network.",
        "isCorrect": false
      },
      {
        "text": "To load balance traffic between multiple EC2 instances.",
        "isCorrect": false
      }
    ],
    "explanation": "An Internet Gateway is a VPC component that allows communication between your VPC and the internet.",
    "awsService": "VPC"
  },
  {
    "id": "q4",
    "questionText": "Which service is designed for durable, long-term, low-cost storage for data archiving?",
    "options": [
      {
        "text": "Amazon S3 Standard",
        "isCorrect": false
      },
      {
        "text": "Amazon EBS",
        "isCorrect": false
      },
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Glacier Deep Archive",
        "isCorrect": true
      }
    ],
    "explanation": "Amazon S3 Glacier Deep Archive is Amazon S3â€™s lowest-cost storage class for long-term retention of data that is accessed rarely.",
    "awsService": "S3"
  },
  {
    "id": "q5",
    "questionText": "To ensure high availability for an application on EC2, where should you deploy the instances?",
    "options": [
      {
        "text": "In a single, large EC2 instance.",
        "isCorrect": false
      },
      {
        "text": "Across multiple Availability Zones.",
        "isCorrect": true
      },
      {
        "text": "Across multiple AWS Regions.",
        "isCorrect": false
      },
      {
        "text": "In a single placement group.",
        "isCorrect": false
      }
    ],
    "explanation": "Deploying instances across multiple Availability Zones (AZs) ensures that if one AZ fails, the application remains available in other AZs.",
    "awsService": "EC2"
  },
  {
    "id": "q6",
    "questionText": "What AWS service allows you to manage users and their access to AWS services and resources?",
    "options": [
      {
        "text": "AWS Shield",
        "isCorrect": false
      },
      {
        "text": "AWS Key Management Service (KMS)",
        "isCorrect": false
      },
      {
        "text": "AWS Identity and Access Management (IAM)",
        "isCorrect": true
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      }
    ],
    "explanation": "IAM is the service used to securely control access to AWS resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.",
    "awsService": "IAM"
  },
  {
    "id": "q7",
    "questionText": "A web application needs a managed relational database. Which AWS service is the most appropriate choice?",
    "options": [
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      },
      {
        "text": "Amazon RDS",
        "isCorrect": true
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Relational Database Service (RDS) is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud.",
    "awsService": "RDS"
  },
  {
    "id": "q8",
    "questionText": "Which service can be used to deliver web content to users with low latency by caching it at edge locations?",
    "options": [
      {
        "text": "Amazon Route 53",
        "isCorrect": false
      },
      {
        "text": "Amazon CloudFront",
        "isCorrect": true
      },
      {
        "text": "Elastic Load Balancing",
        "isCorrect": false
      },
      {
        "text": "AWS Direct Connect",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon CloudFront is a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds.",
    "awsService": "CloudFront"
  },
  {
    "id": "q9",
    "questionText": "What is the function of a NAT Gateway in a VPC?",
    "options": [
      {
        "text": "To allow instances in a public subnet to access the internet.",
        "isCorrect": false
      },
      {
        "text": "To allow instances in a private subnet to access the internet, while preventing the internet from initiating connections with those instances.",
        "isCorrect": true
      },
      {
        "text": "To filter traffic between subnets.",
        "isCorrect": false
      },
      {
        "text": "To create a VPN connection to an on-premises network.",
        "isCorrect": false
      }
    ],
    "explanation": "A NAT Gateway allows instances in a private subnet to connect to the internet or other AWS services, but prevents the internet from initiating a connection with those instances.",
    "awsService": "VPC"
  },
  {
    "id": "q10",
    "questionText": "Which AWS service provides a fully managed NoSQL database service?",
    "options": [
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon Aurora",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": true
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q11",
    "questionText": "An EC2 instance needs to access an S3 bucket securely without storing AWS credentials on the instance. What is the recommended approach?",
    "options": [
      {
        "text": "Store credentials in a file and restrict file permissions.",
        "isCorrect": false
      },
      {
        "text": "Use an IAM Role attached to the EC2 instance.",
        "isCorrect": true
      },
      {
        "text": "Hardcode the credentials in the application.",
        "isCorrect": false
      },
      {
        "text": "Pass credentials to the instance via User Data.",
        "isCorrect": false
      }
    ],
    "explanation": "IAM Roles for EC2 instances provide temporary security credentials that applications can use to make AWS calls, which is the most secure and recommended method.",
    "awsService": "IAM"
  },
  {
    "id": "q12",
    "questionText": "What is the purpose of an Auto Scaling group?",
    "options": [
      {
        "text": "To manually scale EC2 instances up or down.",
        "isCorrect": false
      },
      {
        "text": "To automatically adjust the number of EC2 instances in response to traffic or a schedule.",
        "isCorrect": true
      },
      {
        "text": "To distribute traffic across multiple instances.",
        "isCorrect": false
      },
      {
        "text": "To create a backup of an EC2 instance.",
        "isCorrect": false
      }
    ],
    "explanation": "Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to conditions you define.",
    "awsService": "EC2"
  },
  {
    "id": "q13",
    "questionText": "Which S3 storage class is best for data with unpredictable access patterns?",
    "options": [
      {
        "text": "S3 Standard",
        "isCorrect": false
      },
      {
        "text": "S3 Intelligent-Tiering",
        "isCorrect": true
      },
      {
        "text": "S3 Standard-IA",
        "isCorrect": false
      },
      {
        "text": "S3 One Zone-IA",
        "isCorrect": false
      }
    ],
    "explanation": "S3 Intelligent-Tiering is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.",
    "awsService": "S3"
  },
  {
    "id": "q14",
    "questionText": "What component of a VPC controls inbound and outbound traffic for an EC2 instance?",
    "options": [
      {
        "text": "Route Table",
        "isCorrect": false
      },
      {
        "text": "Network ACL",
        "isCorrect": false
      },
      {
        "text": "Security Group",
        "isCorrect": true
      },
      {
        "text": "Internet Gateway",
        "isCorrect": false
      }
    ],
    "explanation": "A Security Group acts as a virtual firewall for your instance to control inbound and outbound traffic. It is stateful.",
    "awsService": "VPC"
  },
  {
    "id": "q15",
    "questionText": "Which service allows you to run code without provisioning or managing servers?",
    "options": [
      {
        "text": "Amazon EC2",
        "isCorrect": false
      },
      {
        "text": "Amazon Lightsail",
        "isCorrect": false
      },
      {
        "text": "AWS Lambda",
        "isCorrect": true
      },
      {
        "text": "Amazon EKS",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. You pay only for the compute time you consume.",
    "awsService": "Lambda"
  },
  {
    "id": "q16",
    "questionText": "You need to create a decoupled architecture for an application that processes messages. Which service is a fully managed message queuing service?",
    "options": [
      {
        "text": "Amazon SNS",
        "isCorrect": false
      },
      {
        "text": "Amazon SQS",
        "isCorrect": true
      },
      {
        "text": "Amazon MQ",
        "isCorrect": false
      },
      {
        "text": "AWS Step Functions",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.",
    "awsService": "SQS"
  },
  {
    "id": "q17",
    "questionText": "What is the difference between a public subnet and a private subnet in a VPC?",
    "options": [
      {
        "text": "A public subnet has a route to an Internet Gateway, while a private subnet does not.",
        "isCorrect": true
      },
      {
        "text": "A public subnet can contain EC2 instances, while a private subnet cannot.",
        "isCorrect": false
      },
      {
        "text": "A public subnet is in one Availability Zone, a private subnet is in another.",
        "isCorrect": false
      },
      {
        "text": "A public subnet uses public IP addresses, a private subnet uses private IP addresses.",
        "isCorrect": false
      }
    ],
    "explanation": "The key differentiator is routing. If a subnet's route table has a route to an Internet Gateway (IGW), it is a public subnet. Otherwise, it is a private subnet.",
    "awsService": "VPC"
  },
  {
    "id": "q18",
    "questionText": "Which AWS service provides a way to model and provision your entire cloud environment as code?",
    "options": [
      {
        "text": "AWS Config",
        "isCorrect": false
      },
      {
        "text": "AWS CloudFormation",
        "isCorrect": true
      },
      {
        "text": "AWS OpsWorks",
        "isCorrect": false
      },
      {
        "text": "AWS Service Catalog",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CloudFormation provides a common language to describe and provision all the infrastructure resources in your cloud environment.",
    "awsService": "CloudFormation"
  },
  {
    "id": "q19",
    "questionText": "Which type of Elastic Load Balancer is best suited for load balancing HTTP and HTTPS traffic with advanced request routing?",
    "options": [
      {
        "text": "Classic Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Network Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Application Load Balancer",
        "isCorrect": true
      },
      {
        "text": "Gateway Load Balancer",
        "isCorrect": false
      }
    ],
    "explanation": "Application Load Balancer (ALB) operates at the application layer (Layer 7) and allows you to define routing rules based on content, such as path or hostname.",
    "awsService": "ELB"
  },
  {
    "id": "q20",
    "questionText": "What is the most resilient and lowest-cost S3 storage class for data that is accessed less than once a month but requires millisecond access?",
    "options": [
      {
        "text": "S3 Standard",
        "isCorrect": false
      },
      {
        "text": "S3 Standard-Infrequent Access (S3 Standard-IA)",
        "isCorrect": true
      },
      {
        "text": "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
        "isCorrect": false
      },
      {
        "text": "S3 Glacier Flexible Retrieval",
        "isCorrect": false
      }
    ],
    "explanation": "S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. It offers the high durability, throughput, and low latency of S3 Standard, with a low per-GB storage price and per-GB retrieval fee.",
    "awsService": "S3"
  },
  {
    "id": "q21",
    "questionText": "Which AWS service helps you to protect your web applications from common web exploits like SQL injection and cross-site scripting?",
    "options": [
      {
        "text": "AWS Shield",
        "isCorrect": false
      },
      {
        "text": "Amazon GuardDuty",
        "isCorrect": false
      },
      {
        "text": "AWS WAF",
        "isCorrect": true
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      }
    ],
    "explanation": "AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources.",
    "awsService": "WAF"
  },
  {
    "id": "q22",
    "questionText": "What is the concept of an 'Availability Zone' in AWS?",
    "options": [
      {
        "text": "A separate geographic area, like North America or Europe.",
        "isCorrect": false
      },
      {
        "text": "One or more discrete data centers with redundant power, networking, and connectivity within an AWS Region.",
        "isCorrect": true
      },
      {
        "text": "A specific location where content is cached for low latency.",
        "isCorrect": false
      },
      {
        "text": "A virtual private network in the AWS cloud.",
        "isCorrect": false
      }
    ],
    "explanation": "An Availability Zone (AZ) is a distinct location within an AWS Region that is insulated from failures in other AZs, and provides inexpensive, low-latency network connectivity to other AZs in the same Region.",
    "awsService": "Global Infrastructure"
  },
  {
    "id": "q23",
    "questionText": "You need to monitor the performance of your EC2 instances, such as CPU utilization. Which service provides this data by default?",
    "options": [
      {
        "text": "AWS CloudTrail",
        "isCorrect": false
      },
      {
        "text": "Amazon CloudWatch",
        "isCorrect": true
      },
      {
        "text": "AWS Config",
        "isCorrect": false
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real time. It collects metrics like CPU utilization, network I/O, and disk I/O for EC2 instances.",
    "awsService": "CloudWatch"
  },
  {
    "id": "q24",
    "questionText": "Which AWS database service is a MySQL and PostgreSQL-compatible relational database built for the cloud, that provides the performance and availability of commercial-grade databases at 1/10th the cost?",
    "options": [
      {
        "text": "Amazon RDS for MySQL",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      },
      {
        "text": "Amazon Aurora",
        "isCorrect": true
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Aurora is a relational database service that combines the performance and availability of high-end commercial databases with the simplicity and cost-effectiveness of open source databases.",
    "awsService": "Aurora"
  },
  {
    "id": "q25",
    "questionText": "What is the purpose of a Network ACL in a VPC?",
    "options": [
      {
        "text": "To act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level.",
        "isCorrect": true
      },
      {
        "text": "To act as a firewall for associated EC2 instances.",
        "isCorrect": false
      },
      {
        "text": "To route traffic between subnets.",
        "isCorrect": false
      },
      {
        "text": "To provide a DNS service for the VPC.",
        "isCorrect": false
      }
    ],
    "explanation": "A Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. It is stateless.",
    "awsService": "VPC"
  },
  {
    "id": "q26",
    "questionText": "A company wants to establish a dedicated, private network connection from their on-premises data center to AWS. Which service should they use?",
    "options": [
      {
        "text": "AWS VPN",
        "isCorrect": false
      },
      {
        "text": "AWS Direct Connect",
        "isCorrect": true
      },
      {
        "text": "VPC Peering",
        "isCorrect": false
      },
      {
        "text": "Internet Gateway",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS, which can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections.",
    "awsService": "Direct Connect"
  },
  {
    "id": "q27",
    "questionText": "Which AWS service is used to send notifications to a large number of subscribers through multiple protocols like HTTP, email, and SQS?",
    "options": [
      {
        "text": "Amazon SQS",
        "isCorrect": false
      },
      {
        "text": "Amazon Kinesis",
        "isCorrect": false
      },
      {
        "text": "Amazon SNS",
        "isCorrect": true
      },
      {
        "text": "AWS Lambda",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Simple Notification Service (SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. It uses a publish/subscribe model.",
    "awsService": "SNS"
  },
  {
    "id": "q28",
    "questionText": "What is an EBS Volume?",
    "options": [
      {
        "text": "A network file system for use with EC2 instances.",
        "isCorrect": false
      },
      {
        "text": "A durable, block-level storage device that you can attach to a single EC2 instance.",
        "isCorrect": true
      },
      {
        "text": "Object storage for files, accessible from anywhere.",
        "isCorrect": false
      },
      {
        "text": "A managed relational database.",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Elastic Block Store (EBS) provides persistent block storage volumes for use with Amazon EC2 instances. Each EBS volume is automatically replicated within its Availability Zone to protect you from component failure.",
    "awsService": "EBS"
  },
  {
    "id": "q29",
    "questionText": "Which Route 53 routing policy would you use to route traffic to multiple resources in proportions that you specify?",
    "options": [
      {
        "text": "Simple routing policy",
        "isCorrect": false
      },
      {
        "text": "Failover routing policy",
        "isCorrect": false
      },
      {
        "text": "Weighted routing policy",
        "isCorrect": true
      },
      {
        "text": "Latency routing policy",
        "isCorrect": false
      }
    ],
    "explanation": "Weighted routing lets you associate multiple resources with a single domain name and choose how much traffic is routed to each resource. This can be useful for A/B testing.",
    "awsService": "Route 53"
  },
  {
    "id": "q30",
    "questionText": "To save costs, a company wants to shut down EC2 instances that are not in use during weekends. Which service or feature can automate this process?",
    "options": [
      {
        "text": "AWS Config",
        "isCorrect": false
      },
      {
        "text": "Amazon CloudWatch Alarms",
        "isCorrect": false
      },
      {
        "text": "Auto Scaling with a scheduled scaling policy",
        "isCorrect": true
      },
      {
        "text": "AWS CloudTrail",
        "isCorrect": false
      }
    ],
    "explanation": "Scheduled scaling policies for Auto Scaling Groups allow you to scale your application in response to predictable load changes. You can create a policy to scale down to zero instances on Friday evening and scale back up on Monday morning.",
    "awsService": "Auto Scaling"
  },
  {
    "id": "q31",
    "questionText": "What does 'S3 Cross-Region Replication' do?",
    "options": [
      {
        "text": "It encrypts all data in an S3 bucket automatically.",
        "isCorrect": false
      },
      {
        "text": "It automatically copies objects across S3 buckets in different AWS Regions.",
        "isCorrect": true
      },
      {
        "text": "It provides a CDN for S3 objects.",
        "isCorrect": false
      },
      {
        "text": "It archives S3 objects to a lower-cost storage tier.",
        "isCorrect": false
      }
    ],
    "explanation": "Cross-Region Replication (CRR) is an Amazon S3 feature that automatically replicates data from a source S3 bucket in one AWS Region to a destination bucket in a different AWS Region for compliance, disaster recovery, or latency reduction.",
    "awsService": "S3"
  },
  {
    "id": "q32",
    "questionText": "Which IAM entity should be used to grant an application running on an EC2 instance permissions to access other AWS resources?",
    "options": [
      {
        "text": "IAM User",
        "isCorrect": false
      },
      {
        "text": "IAM Group",
        "isCorrect": false
      },
      {
        "text": "IAM Role",
        "isCorrect": true
      },
      {
        "text": "IAM Policy",
        "isCorrect": false
      }
    ],
    "explanation": "An IAM Role is an IAM identity that you can create in your account that has specific permissions. Instead of creating and sharing long-term credentials, you can attach a role to an EC2 instance to give it temporary permissions.",
    "awsService": "IAM"
  },
  {
    "id": "q33",
    "questionText": "Which AWS service is a data warehouse that makes it simple and cost-effective to analyze all your data using standard SQL?",
    "options": [
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": true
      },
      {
        "text": "Amazon Athena",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. It is optimized for high-performance analysis and reporting of large datasets.",
    "awsService": "Redshift"
  },
  {
    "id": "q34",
    "questionText": "What is the purpose of a VPC Endpoint?",
    "options": [
      {
        "text": "To allow secure, private connectivity to AWS services from within a VPC without traversing the public internet.",
        "isCorrect": true
      },
      {
        "text": "To connect two VPCs together.",
        "isCorrect": false
      },
      {
        "text": "To establish a VPN connection to an on-premises data center.",
        "isCorrect": false
      },
      {
        "text": "To provide a public IP address to an EC2 instance.",
        "isCorrect": false
      }
    ],
    "explanation": "VPC Endpoints enable you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.",
    "awsService": "VPC"
  },
  {
    "id": "q35",
    "questionText": "You need to ensure that objects uploaded to an S3 bucket cannot be deleted or overwritten by any user for a specific period. Which S3 feature should you use?",
    "options": [
      {
        "text": "S3 Versioning",
        "isCorrect": false
      },
      {
        "text": "S3 Object Lock",
        "isCorrect": true
      },
      {
        "text": "S3 Bucket Policies",
        "isCorrect": false
      },
      {
        "text": "S3 Access Control Lists (ACLs)",
        "isCorrect": false
      }
    ],
    "explanation": "S3 Object Lock can help you prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. It uses a write-once-read-many (WORM) model and is useful for compliance and data protection.",
    "awsService": "S3"
  },
  {
    "id": "q36",
    "questionText": "Which type of EBS volume is recommended for the boot volume of most general-purpose EC2 instances?",
    "options": [
      {
        "text": "Provisioned IOPS SSD (io1/io2)",
        "isCorrect": false
      },
      {
        "text": "General Purpose SSD (gp2/gp3)",
        "isCorrect": true
      },
      {
        "text": "Throughput Optimized HDD (st1)",
        "isCorrect": false
      },
      {
        "text": "Cold HDD (sc1)",
        "isCorrect": false
      }
    ],
    "explanation": "General Purpose SSD volumes (gp2 and gp3) offer a balance of price and performance for a wide variety of transactional workloads, making them the default and recommended choice for boot volumes.",
    "awsService": "EBS"
  },
  {
    "id": "q37",
    "questionText": "Which service would you use to register a new domain name and manage its DNS records?",
    "options": [
      {
        "text": "Amazon CloudFront",
        "isCorrect": false
      },
      {
        "text": "Elastic Load Balancing",
        "isCorrect": false
      },
      {
        "text": "Amazon Route 53",
        "isCorrect": true
      },
      {
        "text": "AWS Certificate Manager",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Route 53 is a scalable and highly available Domain Name System (DNS) web service. You can use it to register domain names and route end users to internet applications.",
    "awsService": "Route 53"
  },
  {
    "id": "q38",
    "questionText": "What is the AWS Shared Responsibility Model?",
    "options": [
      {
        "text": "AWS is responsible for all aspects of security.",
        "isCorrect": false
      },
      {
        "text": "The customer is responsible for all aspects of security.",
        "isCorrect": false
      },
      {
        "text": "AWS is responsible for security 'of' the cloud, while the customer is responsible for security 'in' the cloud.",
        "isCorrect": true
      },
      {
        "text": "Security responsibilities are negotiated on a per-service basis.",
        "isCorrect": false
      }
    ],
    "explanation": "The Shared Responsibility Model dictates that AWS manages the security of the underlying cloud infrastructure (hardware, software, networking), while the customer is responsible for securing their data, applications, and configurations within the cloud.",
    "awsService": "Security"
  },
  {
    "id": "q39",
    "questionText": "How can you provide short-term, expiring access to an S3 object for a user without AWS credentials?",
    "options": [
      {
        "text": "Make the object public.",
        "isCorrect": false
      },
      {
        "text": "Create an IAM user for them.",
        "isCorrect": false
      },
      {
        "text": "Use an S3 Pre-Signed URL.",
        "isCorrect": true
      },
      {
        "text": "Use an S3 Bucket Policy.",
        "isCorrect": false
      }
    ],
    "explanation": "A pre-signed URL gives you access to the object identified in the URL, provided that the creator of the pre-signed URL has permissions to access that object. It has a configurable expiration time.",
    "awsService": "S3"
  },
  {
    "id": "q40",
    "questionText": "Which AWS service provides a managed, in-memory cache service?",
    "options": [
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "isCorrect": false
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": true
      },
      {
        "text": "Amazon EFS",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. It supports two open-source in-memory caching engines: Redis and Memcached.",
    "awsService": "ElastiCache"
  },
  {
    "id": "q41",
    "questionText": "What is the primary benefit of using an Application Load Balancer (ALB) over a Classic Load Balancer (CLB)?",
    "options": [
      {
        "text": "ALB can handle TCP traffic at any port.",
        "isCorrect": false
      },
      {
        "text": "ALB supports content-based routing rules, such as path-based routing.",
        "isCorrect": true
      },
      {
        "text": "ALB has a fixed, static IP address.",
        "isCorrect": false
      },
      {
        "text": "ALB is free of charge.",
        "isCorrect": false
      }
    ],
    "explanation": "The key advantage of an ALB is its ability to operate at the application layer (Layer 7) and route traffic based on the content of the request, such as the URL path or hostname, allowing for more flexible application architectures.",
    "awsService": "ELB"
  },
  {
    "id": "q42",
    "questionText": "An organization needs to audit all API calls made to their AWS account for security analysis and compliance. Which service should be used?",
    "options": [
      {
        "text": "Amazon CloudWatch",
        "isCorrect": false
      },
      {
        "text": "AWS CloudTrail",
        "isCorrect": true
      },
      {
        "text": "AWS Config",
        "isCorrect": false
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It logs every API call, providing a detailed history of who did what, when, and from where.",
    "awsService": "CloudTrail"
  },
  {
    "id": "q43",
    "questionText": "Which feature of Amazon RDS allows for improved database availability by creating a synchronous standby replica in a different Availability Zone?",
    "options": [
      {
        "text": "Read Replicas",
        "isCorrect": false
      },
      {
        "text": "Multi-AZ Deployment",
        "isCorrect": true
      },
      {
        "text": "Automated Backups",
        "isCorrect": false
      },
      {
        "text": "Database Snapshots",
        "isCorrect": false
      }
    ],
    "explanation": "RDS Multi-AZ deployments provide enhanced availability and durability for RDS DB instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB instance, Amazon RDS automatically creates a primary DB instance and synchronously replicates the data to a standby instance in a different AZ.",
    "awsService": "RDS"
  },
  {
    "id": "q44",
    "questionText": "What is the most efficient way to transfer 100 TB of data from an on-premises data center to Amazon S3 with a slow and unreliable internet connection?",
    "options": [
      {
        "text": "AWS Direct Connect",
        "isCorrect": false
      },
      {
        "text": "AWS Snowball",
        "isCorrect": true
      },
      {
        "text": "AWS DataSync",
        "isCorrect": false
      },
      {
        "text": "S3 Transfer Acceleration",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud. It is ideal for situations where network connectivity is a bottleneck.",
    "awsService": "Snowball"
  },
  {
    "id": "q45",
    "questionText": "Which AWS service can be used to create a serverless API with endpoints that trigger AWS Lambda functions?",
    "options": [
      {
        "text": "Elastic Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Amazon API Gateway",
        "isCorrect": true
      },
      {
        "text": "Amazon SQS",
        "isCorrect": false
      },
      {
        "text": "AWS Step Functions",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It can act as a 'front door' for applications to access data, business logic, or functionality from your back-end services, such as Lambda functions.",
    "awsService": "API Gateway"
  },
  {
    "id": "q46",
    "questionText": "A solutions architect needs to design a solution that can scale to handle thousands of write requests per second for a non-relational database. Which service is the best fit?",
    "options": [
      {
        "text": "Amazon RDS with Multi-AZ",
        "isCorrect": false
      },
      {
        "text": "Amazon Aurora",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": true
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon DynamoDB is a NoSQL database designed for high-performance, single-digit millisecond latency at any scale. It can easily handle thousands of read and write requests per second by provisioning the required capacity.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q47",
    "questionText": "What is the purpose of an S3 bucket policy?",
    "options": [
      {
        "text": "To control access to EC2 instances.",
        "isCorrect": false
      },
      {
        "text": "To define access permissions for an entire S3 bucket.",
        "isCorrect": true
      },
      {
        "text": "To manage DNS records for a domain.",
        "isCorrect": false
      },
      {
        "text": "To encrypt the data within an S3 bucket.",
        "isCorrect": false
      }
    ],
    "explanation": "Bucket policies are resource-based policies written in JSON that you attach to S3 buckets to manage access permissions for those buckets and the objects in them.",
    "awsService": "S3"
  },
  {
    "id": "q48",
    "questionText": "Which Route 53 routing policy is used to route users to the endpoint that provides the best performance, based on their geographic location?",
    "options": [
      {
        "text": "Geoproximity routing policy",
        "isCorrect": false
      },
      {
        "text": "Geolocation routing policy",
        "isCorrect": false
      },
      {
        "text": "Latency routing policy",
        "isCorrect": true
      },
      {
        "text": "Failover routing policy",
        "isCorrect": false
      }
    ],
    "explanation": "Latency routing policy is used when you have resources in multiple AWS Regions and you want to route viewers to the region that provides the lowest latency.",
    "awsService": "Route 53"
  },
  {
    "id": "q49",
    "questionText": "Which AWS service provides a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads?",
    "options": [
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      },
      {
        "text": "AWS Shield",
        "isCorrect": false
      },
      {
        "text": "Amazon GuardDuty",
        "isCorrect": true
      },
      {
        "text": "AWS WAF",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon GuardDuty is an intelligent threat detection service that uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.",
    "awsService": "GuardDuty"
  },
  {
    "id": "q50",
    "questionText": "What is the difference between an EBS-backed and an Instance Store-backed EC2 instance?",
    "options": [
      {
        "text": "EBS-backed instance data persists independently of the instance's life, while Instance Store data is ephemeral and lost if the instance is stopped or terminated.",
        "isCorrect": true
      },
      {
        "text": "EBS-backed instances are free, while Instance Store-backed instances are not.",
        "isCorrect": false
      },
      {
        "text": "EBS-backed instances cannot be stopped, while Instance Store-backed instances can.",
        "isCorrect": false
      },
      {
        "text": "EBS-backed instances have lower performance than Instance Store-backed instances.",
        "isCorrect": false
      }
    ],
    "explanation": "The key difference is data persistence. EBS volumes are persistent network-attached storage, while Instance Store provides temporary, block-level storage located on disks that are physically attached to the host computer.",
    "awsService": "EC2"
  },
  {
    "id": "q51",
    "questionText": "A company needs to ensure that its S3 objects are stored in at least three geographically distant locations. How can this be achieved with a single S3 bucket?",
    "options": [
      {
        "text": "Use S3 Standard storage class.",
        "isCorrect": true
      },
      {
        "text": "Use S3 One Zone-IA storage class.",
        "isCorrect": false
      },
      {
        "text": "Enable Cross-Region Replication to two other regions.",
        "isCorrect": false
      },
      {
        "text": "This is not possible with a single bucket.",
        "isCorrect": false
      }
    ],
    "explanation": "The S3 Standard, Standard-IA, and Intelligent-Tiering storage classes automatically store data across a minimum of three Availability Zones within a single AWS Region, providing high durability and availability.",
    "awsService": "S3"
  },
  {
    "id": "q52",
    "questionText": "Which service can be used to orchestrate a complex workflow of multiple AWS Lambda functions?",
    "options": [
      {
        "text": "Amazon SQS",
        "isCorrect": false
      },
      {
        "text": "Amazon SNS",
        "isCorrect": false
      },
      {
        "text": "AWS Step Functions",
        "isCorrect": true
      },
      {
        "text": "Amazon Kinesis",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Step Functions lets you coordinate multiple AWS services into serverless workflows. You can design and run workflows that stitch together services such as AWS Lambda and Amazon SNS into feature-rich applications.",
    "awsService": "Step Functions"
  },
  {
    "id": "q53",
    "questionText": "What is the primary use case for Amazon RDS Read Replicas?",
    "options": [
      {
        "text": "To improve database availability during a failure.",
        "isCorrect": false
      },
      {
        "text": "To scale read-heavy database workloads.",
        "isCorrect": true
      },
      {
        "text": "To provide a cold backup of the database.",
        "isCorrect": false
      },
      {
        "text": "To encrypt the database data at rest.",
        "isCorrect": false
      }
    ],
    "explanation": "Read Replicas allow you to create one or more read-only copies of your primary database instance. This is useful for offloading read traffic from your primary instance to improve the performance of read-heavy applications.",
    "awsService": "RDS"
  },
  {
    "id": "q54",
    "questionText": "Which AWS service provides managed DDoS protection for applications running on AWS?",
    "options": [
      {
        "text": "AWS WAF",
        "isCorrect": false
      },
      {
        "text": "AWS Shield",
        "isCorrect": true
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      },
      {
        "text": "AWS Firewall Manager",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield Standard is enabled automatically at no extra cost.",
    "awsService": "Shield"
  },
  {
    "id": "q55",
    "questionText": "How can you grant users from an external identity provider (like Active Directory) access to your AWS account?",
    "options": [
      {
        "text": "Create an IAM user for each external user.",
        "isCorrect": false
      },
      {
        "text": "Use IAM Roles with identity federation (SAML 2.0 or OpenID Connect).",
        "isCorrect": true
      },
      {
        "text": "Share the root account credentials.",
        "isCorrect": false
      },
      {
        "text": "Use AWS Organizations.",
        "isCorrect": false
      }
    ],
    "explanation": "Identity federation allows you to manage user identities in an external system and give those users secure access to your AWS resources by assuming an IAM role, without ever creating IAM users for them.",
    "awsService": "IAM"
  },
  {
    "id": "q56",
    "questionText": "Which EC2 placement group strategy is recommended for applications that require low network latency and high network throughput between instances?",
    "options": [
      {
        "text": "Spread Placement Group",
        "isCorrect": false
      },
      {
        "text": "Partition Placement Group",
        "isCorrect": false
      },
      {
        "text": "Cluster Placement Group",
        "isCorrect": true
      },
      {
        "text": "Dispersed Placement Group",
        "isCorrect": false
      }
    ],
    "explanation": "A Cluster placement group is a logical grouping of instances within a single Availability Zone. It is ideal for applications that need low network latency, high network throughput, or both.",
    "awsService": "EC2"
  },
  {
    "id": "q57",
    "questionText": "What is the purpose of AWS Organizations?",
    "options": [
      {
        "text": "To manage infrastructure as code.",
        "isCorrect": false
      },
      {
        "text": "To centrally govern and manage multiple AWS accounts.",
        "isCorrect": true
      },
      {
        "text": "To monitor application performance.",
        "isCorrect": false
      },
      {
        "text": "To provide a private connection to AWS.",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. You can use it to manage billing, control access, compliance, and security, and share resources across your AWS accounts.",
    "awsService": "Organizations"
  },
  {
    "id": "q58",
    "questionText": "Which S3 feature can be used to automatically transition objects to a more cost-effective storage class as they age?",
    "options": [
      {
        "text": "S3 Versioning",
        "isCorrect": false
      },
      {
        "text": "S3 Lifecycle Policies",
        "isCorrect": true
      },
      {
        "text": "S3 Cross-Region Replication",
        "isCorrect": false
      },
      {
        "text": "S3 Event Notifications",
        "isCorrect": false
      }
    ],
    "explanation": "S3 Lifecycle configuration enables you to specify the lifecycle management of objects in a bucket. You can define rules to automatically transition objects to other storage classes or to expire objects after a certain period.",
    "awsService": "S3"
  },
  {
    "id": "q59",
    "questionText": "Which type of Elastic Load Balancer operates at the transport layer (Layer 4) and is capable of handling millions of requests per second with ultra-low latency?",
    "options": [
      {
        "text": "Application Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Classic Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Network Load Balancer",
        "isCorrect": true
      },
      {
        "text": "Gateway Load Balancer",
        "isCorrect": false
      }
    ],
    "explanation": "Network Load Balancer (NLB) is designed for extreme performance. It operates at the connection level (Layer 4), routing connections to targets based on IP protocol data.",
    "awsService": "ELB"
  },
  {
    "id": "q60",
    "questionText": "What is the most secure way to store sensitive configuration data, such as database passwords, for use by your applications running on AWS?",
    "options": [
      {
        "text": "Store them in a text file on an EC2 instance.",
        "isCorrect": false
      },
      {
        "text": "Store them in environment variables.",
        "isCorrect": false
      },
      {
        "text": "Use AWS Secrets Manager or AWS Systems Manager Parameter Store.",
        "isCorrect": true
      },
      {
        "text": "Hardcode them in the application source code.",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Secrets Manager and Parameter Store are dedicated services for securely storing and managing secrets. They provide encryption, access control, and rotation capabilities, which is far more secure than storing secrets in plaintext.",
    "awsService": "Secrets Manager"
  },
  {
    "id": "q61",
    "questionText": "A company wants to analyze data directly in Amazon S3 using standard SQL queries without loading it into a database. Which service should be used?",
    "options": [
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      },
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon Athena",
        "isCorrect": true
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.",
    "awsService": "Athena"
  },
  {
    "id": "q62",
    "questionText": "What is the purpose of a VPC peering connection?",
    "options": [
      {
        "text": "To connect a VPC to the public internet.",
        "isCorrect": false
      },
      {
        "text": "To connect a VPC to an on-premises data center.",
        "isCorrect": false
      },
      {
        "text": "To privately connect two VPCs, enabling them to communicate as if they are on the same network.",
        "isCorrect": true
      },
      {
        "text": "To distribute traffic between two VPCs.",
        "isCorrect": false
      }
    ],
    "explanation": "A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses.",
    "awsService": "VPC"
  },
  {
    "id": "q63",
    "questionText": "Which AWS service provides a managed Kubernetes service?",
    "options": [
      {
        "text": "Amazon ECS",
        "isCorrect": false
      },
      {
        "text": "Amazon EKS",
        "isCorrect": true
      },
      {
        "text": "AWS Fargate",
        "isCorrect": false
      },
      {
        "text": "AWS Lambda",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Elastic Kubernetes Service (EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes.",
    "awsService": "EKS"
  },
  {
    "id": "q64",
    "questionText": "To improve the security of the AWS root user, what is the recommended best practice?",
    "options": [
      {
        "text": "Share the root user credentials with all administrators.",
        "isCorrect": false
      },
      {
        "text": "Enable Multi-Factor Authentication (MFA) and do not use the root user for daily tasks.",
        "isCorrect": true
      },
      {
        "text": "Store the root user access key in the application code.",
        "isCorrect": false
      },
      {
        "text": "Use the root user for all administrative tasks to ensure full permissions.",
        "isCorrect": false
      }
    ],
    "explanation": "The root user has unrestricted access to all resources. The best practice is to lock it down with MFA and create separate IAM users with limited permissions for daily administrative tasks.",
    "awsService": "IAM"
  },
  {
    "id": "q65",
    "questionText": "Which S3 storage class is designed for non-critical, reproducible data that can be lost, at a lower cost than S3 Standard?",
    "options": [
      {
        "text": "S3 Standard-IA",
        "isCorrect": false
      },
      {
        "text": "S3 Intelligent-Tiering",
        "isCorrect": false
      },
      {
        "text": "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
        "isCorrect": true
      },
      {
        "text": "S3 Glacier Flexible Retrieval",
        "isCorrect": false
      }
    ],
    "explanation": "S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs less than S3 Standard-IA.",
    "awsService": "S3"
  },
  {
    "id": "q66",
    "questionText": "What is AWS Fargate?",
    "options": [
      {
        "text": "A service for running serverless functions.",
        "isCorrect": false
      },
      {
        "text": "A serverless compute engine for containers that works with both Amazon ECS and EKS.",
        "isCorrect": true
      },
      {
        "text": "A managed relational database service.",
        "isCorrect": false
      },
      {
        "text": "A service for provisioning infrastructure as code.",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.",
    "awsService": "Fargate"
  },
  {
    "id": "q67",
    "questionText": "A solutions architect needs to ensure that all objects uploaded to an S3 bucket are encrypted. What is the simplest way to enforce this?",
    "options": [
      {
        "text": "Manually encrypt each object before uploading.",
        "isCorrect": false
      },
      {
        "text": "Enable default encryption on the S3 bucket.",
        "isCorrect": true
      },
      {
        "text": "Use a Lambda function to encrypt objects after they are uploaded.",
        "isCorrect": false
      },
      {
        "text": "Use AWS WAF to inspect and encrypt objects.",
        "isCorrect": false
      }
    ],
    "explanation": "You can configure your Amazon S3 bucket to use default encryption. When you enable default encryption, all new objects are encrypted when they are stored in the bucket, using the encryption method you specify (SSE-S3 or SSE-KMS).",
    "awsService": "S3"
  },
  {
    "id": "q68",
    "questionText": "Which AWS service can be used to automate the deployment of applications to EC2 instances?",
    "options": [
      {
        "text": "AWS CodeCommit",
        "isCorrect": false
      },
      {
        "text": "AWS CodeBuild",
        "isCorrect": false
      },
      {
        "text": "AWS CodeDeploy",
        "isCorrect": true
      },
      {
        "text": "AWS CodePipeline",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.",
    "awsService": "CodeDeploy"
  },
  {
    "id": "q69",
    "questionText": "What is the primary benefit of using Amazon Aurora over a standard MySQL RDS instance?",
    "options": [
      {
        "text": "It is a NoSQL database.",
        "isCorrect": false
      },
      {
        "text": "It offers significantly higher performance, availability, and durability with its cloud-native architecture.",
        "isCorrect": true
      },
      {
        "text": "It is free of charge.",
        "isCorrect": false
      },
      {
        "text": "It can only be accessed from within a VPC.",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).",
    "awsService": "Aurora"
  },
  {
    "id": "q70",
    "questionText": "Which AWS service would you use to create a full CI/CD pipeline to build, test, and deploy your application?",
    "options": [
      {
        "text": "AWS CodeDeploy",
        "isCorrect": false
      },
      {
        "text": "AWS CloudFormation",
        "isCorrect": false
      },
      {
        "text": "AWS CodePipeline",
        "isCorrect": true
      },
      {
        "text": "AWS CodeCommit",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. It orchestrates the entire process, from source code (e.g., CodeCommit) to build (e.g., CodeBuild) to deployment (e.g., CodeDeploy).",
    "awsService": "CodePipeline"
  },
  {
    "id": "q71",
    "questionText": "What is the purpose of an Elastic IP address?",
    "options": [
      {
        "text": "To provide a private IP address for an EC2 instance.",
        "isCorrect": false
      },
      {
        "text": "To provide a static, public IPv4 address designed for dynamic cloud computing.",
        "isCorrect": true
      },
      {
        "text": "To automatically scale the number of IP addresses.",
        "isCorrect": false
      },
      {
        "text": "To load balance traffic across multiple IP addresses.",
        "isCorrect": false
      }
    ],
    "explanation": "An Elastic IP address is a static IPv4 address that you can associate with your account. You can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.",
    "awsService": "EC2"
  },
  {
    "id": "q72",
    "questionText": "Which AWS service provides a managed container orchestration service for running Docker containers?",
    "options": [
      {
        "text": "AWS Lambda",
        "isCorrect": false
      },
      {
        "text": "Amazon Elastic Container Service (ECS)",
        "isCorrect": true
      },
      {
        "text": "Amazon EC2",
        "isCorrect": false
      },
      {
        "text": "AWS Batch",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon ECS is a highly scalable, high-performance container orchestration service that supports Docker containers and allows you to easily run and scale containerized applications on AWS.",
    "awsService": "ECS"
  },
  {
    "id": "q73",
    "questionText": "A company needs to store frequently accessed data with microsecond latency for a read-heavy application. Which service would be the best choice?",
    "options": [
      {
        "text": "Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB with DAX",
        "isCorrect": true
      },
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement â€“ from milliseconds to microseconds â€“ even at millions of requests per second.",
    "awsService": "DAX"
  },
  {
    "id": "q74",
    "questionText": "What is the function of a Route Table in a VPC?",
    "options": [
      {
        "text": "To filter traffic entering a subnet.",
        "isCorrect": false
      },
      {
        "text": "To contain a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed.",
        "isCorrect": true
      },
      {
        "text": "To assign IP addresses to EC2 instances.",
        "isCorrect": false
      },
      {
        "text": "To act as a firewall for an EC2 instance.",
        "isCorrect": false
      }
    ],
    "explanation": "Each subnet in your VPC must be associated with a route table. The route table controls the routing for the subnet. A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same route table.",
    "awsService": "VPC"
  },
  {
    "id": "q75",
    "questionText": "Which AWS service allows you to assess, audit, and evaluate the configurations of your AWS resources?",
    "options": [
      {
        "text": "AWS CloudTrail",
        "isCorrect": false
      },
      {
        "text": "Amazon CloudWatch",
        "isCorrect": false
      },
      {
        "text": "AWS Config",
        "isCorrect": true
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.",
    "awsService": "Config"
  },
  {
    "id": "q76",
    "questionText": "Which of the following is a primary benefit of the AWS Cloud's economies of scale?",
    "options": [
      {
        "text": "Increased latency for global users.",
        "isCorrect": false
      },
      {
        "text": "Variable monthly costs that are difficult to predict.",
        "isCorrect": false
      },
      {
        "text": "Lower pay-as-you-go prices as a result of AWS's massive scale.",
        "isCorrect": true
      },
      {
        "text": "The need to invest heavily in upfront hardware.",
        "isCorrect": false
      }
    ],
    "explanation": "Because AWS has hundreds of thousands of customers and aggregates usage, it can achieve higher economies of scale, which translates into lower pay-as-you-go prices for customers.",
    "awsService": "Cloud Economics"
  },
  {
    "id": "q77",
    "questionText": "What is the best way to secure a new AWS account?",
    "options": [
      {
        "text": "Use the root user for all tasks.",
        "isCorrect": false
      },
      {
        "text": "Enable MFA on the root user, create an IAM admin user, and use that for daily tasks.",
        "isCorrect": true
      },
      {
        "text": "Delete the root user account.",
        "isCorrect": false
      },
      {
        "text": "Store the root access keys in a public S3 bucket.",
        "isCorrect": false
      }
    ],
    "explanation": "Following the principle of least privilege, the root user should be secured with MFA and used only for tasks that require it. An IAM user with administrative privileges should be created for day-to-day management.",
    "awsService": "IAM"
  },
  {
    "id": "q78",
    "questionText": "An application requires a storage layer that can be shared across multiple EC2 instances and provides file-level locking. Which service is suitable?",
    "options": [
      {
        "text": "Amazon EBS",
        "isCorrect": false
      },
      {
        "text": "Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Amazon EFS",
        "isCorrect": true
      },
      {
        "text": "Instance Store",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon EFS is a file storage service for use with Amazon EC2. EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently accessible storage for up to thousands of EC2 instances.",
    "awsService": "EFS"
  },
  {
    "id": "q79",
    "questionText": "Which AWS service can be used to provision the same infrastructure across multiple AWS accounts and regions in a standardized and repeatable way?",
    "options": [
      {
        "text": "AWS Elastic Beanstalk",
        "isCorrect": false
      },
      {
        "text": "AWS CloudFormation",
        "isCorrect": true
      },
      {
        "text": "AWS OpsWorks",
        "isCorrect": false
      },
      {
        "text": "Amazon EC2",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. Templates can be reused across accounts and regions.",
    "awsService": "CloudFormation"
  },
  {
    "id": "q80",
    "questionText": "What is the purpose of a 'golden image' in the context of EC2?",
    "options": [
      {
        "text": "A backup of an EBS volume.",
        "isCorrect": false
      },
      {
        "text": "A pre-configured Amazon Machine Image (AMI) that contains all necessary software and configurations.",
        "isCorrect": true
      },
      {
        "text": "A snapshot of a running EC2 instance.",
        "isCorrect": false
      },
      {
        "text": "A template for an AWS CloudFormation stack.",
        "isCorrect": false
      }
    ],
    "explanation": "A 'golden AMI' is a customized AMI that you create, which is pre-loaded with all the necessary software, patches, and configurations. This speeds up instance launch times and ensures consistency.",
    "awsService": "EC2"
  },
  {
    "id": "q81",
    "questionText": "Which design principle supports building a resilient architecture on AWS?",
    "options": [
      {
        "text": "Using a single, large EC2 instance for all components.",
        "isCorrect": false
      },
      {
        "text": "Assuming that infrastructure components will not fail.",
        "isCorrect": false
      },
      {
        "text": "Designing for failure and using multiple Availability Zones.",
        "isCorrect": true
      },
      {
        "text": "Storing all data in a single EBS volume.",
        "isCorrect": false
      }
    ],
    "explanation": "A core principle of designing for the cloud is to assume that failures will happen. By designing your architecture to be fault-tolerant and deploying across multiple AZs, you can build a highly resilient system.",
    "awsService": "Well-Architected Framework"
  },
  {
    "id": "q82",
    "questionText": "Which service is most appropriate for running a self-managed relational database on AWS, where you need full control over the operating system?",
    "options": [
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon Aurora",
        "isCorrect": false
      },
      {
        "text": "Running the database on an Amazon EC2 instance.",
        "isCorrect": true
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      }
    ],
    "explanation": "While RDS and Aurora are managed services, running your database directly on an EC2 instance gives you complete control over the database software, the underlying operating system, and its configuration. This is necessary when a specific configuration or database version not supported by RDS is required.",
    "awsService": "EC2"
  },
  {
    "id": "q83",
    "questionText": "What is the primary benefit of decoupling an application's components using a service like Amazon SQS?",
    "options": [
      {
        "text": "It increases the performance of individual components.",
        "isCorrect": false
      },
      {
        "text": "It allows the application to be deployed in a single Availability Zone.",
        "isCorrect": false
      },
      {
        "text": "It improves resilience by allowing components to fail independently without cascading failures.",
        "isCorrect": true
      },
      {
        "text": "It reduces the cost of data transfer between components.",
        "isCorrect": false
      }
    ],
    "explanation": "Decoupling means that the different components of your application can operate independently. If one component fails, the others can continue to work, and messages can be stored in the SQS queue until the failed component recovers. This increases the overall fault tolerance of the system.",
    "awsService": "SQS"
  },
  {
    "id": "q84",
    "questionText": "Which AWS service can be used to provide free, public SSL/TLS certificates for your domains?",
    "options": [
      {
        "text": "AWS Certificate Manager (ACM)",
        "isCorrect": true
      },
      {
        "text": "AWS Key Management Service (KMS)",
        "isCorrect": false
      },
      {
        "text": "AWS Secrets Manager",
        "isCorrect": false
      },
      {
        "text": "IAM Certificate Store",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. Public certificates from ACM are free.",
    "awsService": "ACM"
  },
  {
    "id": "q85",
    "questionText": "A company wants to analyze the cost of their AWS usage by department. What is the best way to achieve this?",
    "options": [
      {
        "text": "Create a separate AWS account for each department.",
        "isCorrect": false
      },
      {
        "text": "Use cost allocation tags to label resources by department.",
        "isCorrect": true
      },
      {
        "text": "Manually calculate the costs from the monthly bill.",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudWatch to monitor costs.",
        "isCorrect": false
      }
    ],
    "explanation": "Cost allocation tags are labels that you can apply to AWS resources. After you activate cost allocation tags, AWS uses the tags to organize your resource costs on your cost allocation report, making it easier to categorize and track your AWS costs.",
    "awsService": "Cost Management"
  },
  {
    "id": "q86",
    "questionText": "Which of the following is a characteristic of Amazon S3?",
    "options": [
      {
        "text": "It is a block storage service.",
        "isCorrect": false
      },
      {
        "text": "It provides object storage with a flat namespace.",
        "isCorrect": true
      },
      {
        "text": "It can be mounted as a file system on an EC2 instance.",
        "isCorrect": false
      },
      {
        "text": "Data is automatically deleted after 30 days.",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon S3 is an object storage service. Unlike a file system with a hierarchical directory structure, S3 has a flat structure. You create buckets, and the buckets store objects (files).",
    "awsService": "S3"
  },
  {
    "id": "q87",
    "questionText": "What is the purpose of an EC2 instance's 'User Data' field?",
    "options": [
      {
        "text": "To store persistent application data.",
        "isCorrect": false
      },
      {
        "text": "To pass a script or configuration commands to an instance at launch.",
        "isCorrect": true
      },
      {
        "text": "To store the instance's private key.",
        "isCorrect": false
      },
      {
        "text": "To define the instance's IAM role.",
        "isCorrect": false
      }
    ],
    "explanation": "User data can be used to pass a script that will be executed when the instance starts. This is commonly used for bootstrapping, such as installing software, applying patches, or configuring the instance.",
    "awsService": "EC2"
  },
  {
    "id": "q88",
    "questionText": "Which AWS service provides a simple way to set up a new AWS environment with prescribed best practices, often for a specific workload like a WordPress site?",
    "options": [
      {
        "text": "Amazon EC2",
        "isCorrect": false
      },
      {
        "text": "Amazon Lightsail",
        "isCorrect": true
      },
      {
        "text": "AWS Elastic Beanstalk",
        "isCorrect": false
      },
      {
        "text": "AWS Lambda",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server with AWS. Lightsail plans include everything you need to jumpstart your project â€“ a virtual machine, SSD-based storage, data transfer, DNS management, and a static IP address â€“ for a low, predictable price.",
    "awsService": "Lightsail"
  },
  {
    "id": "q89",
    "questionText": "Which AWS service would you use to automate infrastructure provisioning and application deployment with less administrative overhead than CloudFormation?",
    "options": [
      {
        "text": "AWS OpsWorks",
        "isCorrect": false
      },
      {
        "text": "AWS Elastic Beanstalk",
        "isCorrect": true
      },
      {
        "text": "AWS Config",
        "isCorrect": false
      },
      {
        "text": "AWS Service Catalog",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. You simply upload your code, and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring.",
    "awsService": "Elastic Beanstalk"
  },
  {
    "id": "q90",
    "questionText": "What is the main difference between a Security Group and a Network ACL?",
    "options": [
      {
        "text": "Security Groups are stateless; Network ACLs are stateful.",
        "isCorrect": false
      },
      {
        "text": "Security Groups operate at the instance level; Network ACLs operate at the subnet level.",
        "isCorrect": true
      },
      {
        "text": "Security Groups can only have allow rules; Network ACLs can only have deny rules.",
        "isCorrect": false
      },
      {
        "text": "Security Groups are free; Network ACLs have a cost.",
        "isCorrect": false
      }
    ],
    "explanation": "The two key differences are scope and state. Security Groups are stateful firewalls that apply to individual instances. Network ACLs are stateless firewalls that apply to all traffic entering or exiting an entire subnet.",
    "awsService": "VPC"
  },
  {
    "id": "q91",
    "questionText": "Which AWS service provides a petabyte-scale data transport solution with on-board compute capabilities?",
    "options": [
      {
        "text": "AWS Snowball",
        "isCorrect": false
      },
      {
        "text": "AWS Snowball Edge",
        "isCorrect": true
      },
      {
        "text": "AWS Snowmobile",
        "isCorrect": false
      },
      {
        "text": "AWS DataSync",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Snowball Edge is a type of Snowball device with on-board storage and compute power for select AWS capabilities. Snowball Edge can undertake local processing and edge-computing workloads in addition to transferring data between your local environment and the AWS Cloud.",
    "awsService": "Snowball"
  },
  {
    "id": "q92",
    "questionText": "Which Route 53 routing policy would you use to route users to different endpoints based on their geographic location (e.g., country or continent)?",
    "options": [
      {
        "text": "Latency routing policy",
        "isCorrect": false
      },
      {
        "text": "Failover routing policy",
        "isCorrect": false
      },
      {
        "text": "Geolocation routing policy",
        "isCorrect": true
      },
      {
        "text": "Weighted routing policy",
        "isCorrect": false
      }
    ],
    "explanation": "Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region.",
    "awsService": "Route 53"
  },
  {
    "id": "q93",
    "questionText": "What is the concept of 'elasticity' in the AWS Cloud?",
    "options": [
      {
        "text": "The ability to pay for services with a flexible payment plan.",
        "isCorrect": false
      },
      {
        "text": "The ability of the infrastructure to scale resources up and down easily in response to demand.",
        "isCorrect": true
      },
      {
        "text": "The ability to connect physical data centers to the AWS network.",
        "isCorrect": false
      },
      {
        "text": "The ability to withstand a component failure without downtime.",
        "isCorrect": false
      }
    ],
    "explanation": "Elasticity is a fundamental concept of cloud computing. It refers to the ability to acquire resources as you need them and release resources when you no longer need them. In the cloud, you don't have to over-provision for peak load.",
    "awsService": "Cloud Concepts"
  },
  {
    "id": "q94",
    "questionText": "An application needs to publish messages to a large number of different subscribers simultaneously. Which service is best suited for this 'fan-out' scenario?",
    "options": [
      {
        "text": "Amazon SQS",
        "isCorrect": false
      },
      {
        "text": "Amazon SNS",
        "isCorrect": true
      },
      {
        "text": "AWS Lambda",
        "isCorrect": false
      },
      {
        "text": "Amazon Kinesis",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon SNS uses a publish-subscribe model where a single message published to an SNS topic can be delivered to multiple subscriber endpoints, such as SQS queues, Lambda functions, or HTTP endpoints. This is known as a fan-out pattern.",
    "awsService": "SNS"
  },
  {
    "id": "q95",
    "questionText": "Which type of EBS volume is designed for high-throughput, frequently accessed workloads such as big data analytics and data warehousing?",
    "options": [
      {
        "text": "General Purpose SSD (gp2/gp3)",
        "isCorrect": false
      },
      {
        "text": "Provisioned IOPS SSD (io1/io2)",
        "isCorrect": false
      },
      {
        "text": "Throughput Optimized HDD (st1)",
        "isCorrect": true
      },
      {
        "text": "Cold HDD (sc1)",
        "isCorrect": false
      }
    ],
    "explanation": "Throughput Optimized HDD (st1) volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS. This volume type is a good fit for large, sequential workloads such as streaming workloads, big data, and log processing.",
    "awsService": "EBS"
  },
  {
    "id": "q96",
    "questionText": "What is the primary function of AWS Key Management Service (KMS)?",
    "options": [
      {
        "text": "To manage SSL/TLS certificates.",
        "isCorrect": false
      },
      {
        "text": "To create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.",
        "isCorrect": true
      },
      {
        "text": "To store and rotate database credentials.",
        "isCorrect": false
      },
      {
        "text": "To provide DDoS protection.",
        "isCorrect": false
      }
    ],
    "explanation": "AWS KMS makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. It is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2.",
    "awsService": "KMS"
  },
  {
    "id": "q97",
    "questionText": "A company needs to ensure that an EC2 instance can be recovered if it is accidentally terminated. What should be enabled?",
    "options": [
      {
        "text": "Enable detailed monitoring.",
        "isCorrect": false
      },
      {
        "text": "Enable termination protection.",
        "isCorrect": true
      },
      {
        "text": "Attach an Elastic IP address.",
        "isCorrect": false
      },
      {
        "text": "Place the instance in a Spread Placement Group.",
        "isCorrect": false
      }
    ],
    "explanation": "To prevent your instance from being accidentally terminated, you can enable termination protection for that instance. The `DisableApiTermination` attribute controls whether the instance can be terminated using the console, CLI, or API.",
    "awsService": "EC2"
  },
  {
    "id": "q98",
    "questionText": "Which AWS service provides a way to connect your on-premises Active Directory to the AWS cloud?",
    "options": [
      {
        "text": "AWS IAM",
        "isCorrect": false
      },
      {
        "text": "Amazon Cognito",
        "isCorrect": false
      },
      {
        "text": "AWS Directory Service",
        "isCorrect": true
      },
      {
        "text": "AWS Organizations",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS services to use managed Active Directory in the AWS Cloud. You can also use AD Connector to redirect directory requests to your on-premises Active Directory.",
    "awsService": "Directory Service"
  },
  {
    "id": "q99",
    "questionText": "What is the maximum size of a single object that can be uploaded to Amazon S3?",
    "options": [
      {
        "text": "5 GB",
        "isCorrect": false
      },
      {
        "text": "5 TB",
        "isCorrect": true
      },
      {
        "text": "50 TB",
        "isCorrect": false
      },
      {
        "text": "There is no limit.",
        "isCorrect": false
      }
    ],
    "explanation": "The total volume of data and number of objects you can store in Amazon S3 are unlimited. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes (TB).",
    "awsService": "S3"
  },
  {
    "id": "q100",
    "questionText": "Which AWS service can be used to provide a user-friendly alias or CNAME for an Application Load Balancer's DNS name?",
    "options": [
      {
        "text": "Amazon CloudFront",
        "isCorrect": false
      },
      {
        "text": "Amazon Route 53",
        "isCorrect": true
      },
      {
        "text": "AWS Certificate Manager",
        "isCorrect": false
      },
      {
        "text": "Elastic IP Address",
        "isCorrect": false
      }
    ],
    "explanation": "You can create an Alias record in Amazon Route 53 that points to your Application Load Balancer. An Alias record is a Route 53-specific type of record that lets you route traffic to selected AWS resources, such as CloudFront distributions and ELB load balancers.",
    "awsService": "Route 53"
  },
  {
    "id": "q101",
    "questionText": "Which of the following is a responsibility of the customer under the AWS Shared Responsibility Model?",
    "options": [
      {
        "text": "Patching the host operating system of an EC2 instance.",
        "isCorrect": false
      },
      {
        "text": "Managing the physical security of the data centers.",
        "isCorrect": false
      },
      {
        "text": "Configuring security groups and network ACLs.",
        "isCorrect": true
      },
      {
        "text": "Managing the hypervisor for virtualization.",
        "isCorrect": false
      }
    ],
    "explanation": "Under the Shared Responsibility Model, the customer is responsible for security 'in' the cloud. This includes managing the guest operating system (including updates and security patches), any application software or utilities installed by the customer on the instances, and the configuration of the AWS-provided firewall (called a security group) on each instance.",
    "awsService": "Security"
  },
  {
    "id": "q102",
    "questionText": "A company runs a critical application on a Multi-AZ RDS MySQL database. During a planned maintenance window, they need to perform a schema change that will take several hours. What is the BEST approach to minimize downtime while ensuring data consistency?",
    "options": [
      {
        "text": "Create a read replica, promote it to master, and perform the schema change.",
        "isCorrect": false
      },
      {
        "text": "Create a snapshot, restore it to a new instance, perform the schema change, and switch over.",
        "isCorrect": true
      },
      {
        "text": "Enable Multi-AZ failover and perform the schema change on the standby instance.",
        "isCorrect": false
      },
      {
        "text": "Use AWS Database Migration Service to migrate to a new database during the maintenance.",
        "isCorrect": false
      }
    ],
    "explanation": "The blue-green deployment approach using snapshots allows for testing the schema change on a copy of the production data before switching over, minimizing risk and downtime. Multi-AZ standby instances are synchronized replicas and can't have independent schema changes.",
    "awsService": "RDS"
  },
  {
    "id": "q103",
    "questionText": "An organization needs to implement a disaster recovery solution for their three-tier web application. The RTO requirement is 4 hours and RPO is 1 hour. Which AWS disaster recovery strategy would be MOST cost-effective while meeting these requirements?",
    "options": [
      {
        "text": "Backup and Restore",
        "isCorrect": false
      },
      {
        "text": "Pilot Light",
        "isCorrect": true
      },
      {
        "text": "Warm Standby",
        "isCorrect": false
      },
      {
        "text": "Multi-Site Active/Active",
        "isCorrect": false
      }
    ],
    "explanation": "Pilot Light strategy keeps critical core systems running in AWS (like database replication) but other systems are shut down. It can meet 4-hour RTO requirements cost-effectively. Backup/Restore typically takes longer than 4 hours for full application recovery, while Warm Standby and Multi-Site are more expensive.",
    "awsService": "Disaster Recovery"
  },
  {
    "id": "q104",
    "questionText": "A financial services company requires that all API calls to their AWS account be logged and stored for 7 years for compliance. The logs must be tamper-proof and searchable. What combination of services should be used?",
    "options": [
      {
        "text": "CloudTrail + S3 + S3 Glacier",
        "isCorrect": false
      },
      {
        "text": "CloudTrail + S3 + S3 Object Lock + Amazon Athena",
        "isCorrect": true
      },
      {
        "text": "CloudWatch Logs + S3 + AWS Config",
        "isCorrect": false
      },
      {
        "text": "AWS Config + CloudTrail + Amazon ElasticSearch",
        "isCorrect": false
      }
    ],
    "explanation": "CloudTrail logs all API calls, S3 stores the logs, S3 Object Lock ensures tamper-proof storage with WORM compliance, and Athena enables SQL-based searching of the log data. This combination meets all requirements for compliance logging.",
    "awsService": "CloudTrail"
  },
  {
    "id": "q105",
    "questionText": "A gaming company experiences highly variable traffic patterns with spikes during new game releases. Their application uses EC2 instances behind an Application Load Balancer. They want to optimize costs while maintaining performance. What combination of EC2 purchasing options should they use?",
    "options": [
      {
        "text": "100% On-Demand instances with Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "100% Spot instances with multiple instance types",
        "isCorrect": false
      },
      {
        "text": "Reserved Instances for baseline + Spot instances for burst capacity",
        "isCorrect": true
      },
      {
        "text": "Dedicated Hosts for all instances",
        "isCorrect": false
      }
    ],
    "explanation": "A mixed strategy using Reserved Instances for predictable baseline capacity and Spot instances for burst traffic provides the best cost optimization. Reserved Instances offer significant savings for baseline load, while Spot instances provide cost-effective scaling for unpredictable spikes.",
    "awsService": "EC2"
  },
  {
    "id": "q106",
    "questionText": "An e-commerce application stores user session data in a DynamoDB table. During flash sales, the application experiences throttling errors despite provisioned capacity appearing sufficient. What is the MOST likely cause and solution?",
    "options": [
      {
        "text": "Hot partitions due to poor partition key design; implement a more distributed partition key",
        "isCorrect": true
      },
      {
        "text": "Insufficient read capacity; increase the read capacity units",
        "isCorrect": false
      },
      {
        "text": "Network latency; move DynamoDB to a different region",
        "isCorrect": false
      },
      {
        "text": "Application logic errors; implement exponential backoff",
        "isCorrect": false
      }
    ],
    "explanation": "Hot partitions occur when the partition key doesn't distribute traffic evenly across partitions. During flash sales, if session data uses predictable keys (like sequential user IDs), traffic concentrates on specific partitions causing throttling. A well-distributed partition key strategy is essential.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q107",
    "questionText": "A company wants to implement Infrastructure as Code for their AWS resources across multiple accounts and regions. They need to ensure consistent deployments, prevent configuration drift, and enable easy rollbacks. What approach should they use?",
    "options": [
      {
        "text": "AWS CloudFormation with Cross-Stack References",
        "isCorrect": false
      },
      {
        "text": "AWS CloudFormation StackSets with AWS Organizations",
        "isCorrect": true
      },
      {
        "text": "Terraform with state file stored in S3",
        "isCorrect": false
      },
      {
        "text": "AWS CDK with manual deployment scripts",
        "isCorrect": false
      }
    ],
    "explanation": "CloudFormation StackSets allows deployment and management of CloudFormation stacks across multiple accounts and regions from a central location. Combined with AWS Organizations, it provides governance, consistent deployments, and centralized management with easy rollback capabilities.",
    "awsService": "CloudFormation"
  },
  {
    "id": "q108",
    "questionText": "A data analytics team needs to process 50TB of log data daily. The processing jobs are fault-tolerant and can handle interruptions. The data needs to be processed within 6 hours of arrival. What is the MOST cost-effective compute solution?",
    "options": [
      {
        "text": "AWS Lambda with SQS for job queuing",
        "isCorrect": false
      },
      {
        "text": "EC2 Spot Fleet with Auto Scaling",
        "isCorrect": true
      },
      {
        "text": "AWS Batch with On-Demand instances",
        "isCorrect": false
      },
      {
        "text": "Amazon EMR with Reserved Instances",
        "isCorrect": false
      }
    ],
    "explanation": "For large-scale, fault-tolerant batch processing, EC2 Spot Fleet provides the most cost-effective solution with up to 90% savings. The fault-tolerant nature of the workload makes it ideal for Spot instances, and Auto Scaling ensures capacity is available when needed.",
    "awsService": "EC2"
  },
  {
    "id": "q109",
    "questionText": "A company needs to migrate a legacy Oracle database to AWS. The database is 10TB in size, has complex stored procedures, and requires minimal downtime during migration. What migration strategy should be used?",
    "options": [
      {
        "text": "Use AWS Database Migration Service with homogeneous migration to RDS Oracle",
        "isCorrect": true
      },
      {
        "text": "Export/import using Oracle Data Pump to RDS Oracle",
        "isCorrect": false
      },
      {
        "text": "Use AWS Schema Conversion Tool to migrate to Aurora PostgreSQL",
        "isCorrect": false
      },
      {
        "text": "Set up Oracle GoldenGate replication to EC2-hosted Oracle",
        "isCorrect": false
      }
    ],
    "explanation": "For large Oracle databases with minimal downtime requirements, DMS provides continuous replication capabilities with Change Data Capture (CDC). Homogeneous migration to RDS Oracle preserves stored procedures and reduces conversion complexity while minimizing downtime.",
    "awsService": "DMS"
  },
  {
    "id": "q110",
    "questionText": "A SaaS application serves multiple tenants and stores tenant data in separate S3 buckets. Each tenant should only access their own data. What is the MOST scalable way to implement this security requirement?",
    "options": [
      {
        "text": "Create separate IAM users for each tenant with bucket-specific policies",
        "isCorrect": false
      },
      {
        "text": "Use IAM roles with policy variables and STS assume role with conditions",
        "isCorrect": true
      },
      {
        "text": "Implement application-level access control without IAM",
        "isCorrect": false
      },
      {
        "text": "Use S3 bucket policies with explicit deny for other tenants",
        "isCorrect": false
      }
    ],
    "explanation": "IAM roles with policy variables (like ${aws:username}) and STS assume role provide scalable multi-tenant security. The application assumes roles with conditions that dynamically restrict access to tenant-specific resources without creating separate policies for each tenant.",
    "awsService": "IAM"
  },
  {
    "id": "q111",
    "questionText": "A machine learning team trains models using GPU-intensive workloads that run for 2-8 hours unpredictably. The team wants to minimize costs while ensuring jobs complete successfully. What EC2 strategy should they use?",
    "options": [
      {
        "text": "Use Spot instances with checkpointing and multiple AZs",
        "isCorrect": true
      },
      {
        "text": "Use On-Demand instances with Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "Use Reserved Instances for GPU instances",
        "isCorrect": false
      },
      {
        "text": "Use Dedicated Hosts for consistent performance",
        "isCorrect": false
      }
    ],
    "explanation": "For ML training workloads, Spot instances offer significant cost savings (up to 90%). Implementing checkpointing allows jobs to resume after interruptions, and using multiple AZs increases availability. The unpredictable 2-8 hour duration makes Reserved Instances less cost-effective.",
    "awsService": "EC2"
  },
  {
    "id": "q112",
    "questionText": "A company wants to implement a centralized logging solution for applications running across multiple AWS accounts. The logs must be searchable, have configurable retention periods, and support real-time alerting. What architecture should they use?",
    "options": [
      {
        "text": "CloudWatch Logs in each account with cross-account access",
        "isCorrect": false
      },
      {
        "text": "Kinesis Data Streams + Kinesis Data Firehose + OpenSearch + CloudWatch Alarms",
        "isCorrect": true
      },
      {
        "text": "S3 with lifecycle policies and Athena for querying",
        "isCorrect": false
      },
      {
        "text": "ELK stack on EC2 instances with cross-account VPC peering",
        "isCorrect": false
      }
    ],
    "explanation": "Kinesis Data Streams collects logs from multiple accounts, Kinesis Data Firehose provides reliable delivery to OpenSearch for indexing and searching, with configurable buffering and compression. CloudWatch Alarms enable real-time alerting based on log patterns and metrics.",
    "awsService": "Kinesis"
  },
  {
    "id": "q113",
    "questionText": "An organization wants to implement a DevOps pipeline that automatically deploys applications to multiple environments (dev, staging, prod) with appropriate approval gates. What AWS services combination provides the best solution?",
    "options": [
      {
        "text": "CodeCommit + CodeBuild + CodeDeploy + Lambda for approvals",
        "isCorrect": false
      },
      {
        "text": "CodePipeline + CodeBuild + CodeDeploy + Manual approval actions",
        "isCorrect": true
      },
      {
        "text": "Jenkins on EC2 + CodeDeploy + SNS notifications",
        "isCorrect": false
      },
      {
        "text": "GitHub Actions + CodeBuild + CloudFormation",
        "isCorrect": false
      }
    ],
    "explanation": "CodePipeline orchestrates the entire pipeline with stages for each environment, CodeBuild handles application building/testing, CodeDeploy manages deployments, and manual approval actions provide human gates between environments. This is the native AWS solution for CI/CD with approvals.",
    "awsService": "CodePipeline"
  },
  {
    "id": "q114",
    "questionText": "A real-time analytics application processes millions of events per second and needs to maintain exactly-once processing guarantees. The application must scale automatically and handle stream processing with low latency. What architecture should be used?",
    "options": [
      {
        "text": "Kinesis Data Streams + Lambda with DLQ",
        "isCorrect": false
      },
      {
        "text": "SQS FIFO + Lambda with reserved concurrency",
        "isCorrect": false
      },
      {
        "text": "Kinesis Data Streams + Kinesis Analytics + Kinesis Data Firehose",
        "isCorrect": true
      },
      {
        "text": "Amazon MQ + EC2 with Auto Scaling",
        "isCorrect": false
      }
    ],
    "explanation": "Kinesis Data Streams provides high-throughput event ingestion with ordered processing per shard. Kinesis Analytics enables real-time stream processing with exactly-once semantics using Apache Flink. Kinesis Data Firehose provides reliable delivery to various destinations with automatic scaling.",
    "awsService": "Kinesis"
  },
  {
    "id": "q115",
    "questionText": "A financial application requires encryption at rest and in transit for all data. The application also needs to implement key rotation every 90 days and maintain an audit trail of all key usage. What key management strategy should be implemented?",
    "options": [
      {
        "text": "AWS KMS with automatic key rotation and CloudTrail logging",
        "isCorrect": true
      },
      {
        "text": "Client-side encryption with manually managed keys in S3",
        "isCorrect": false
      },
      {
        "text": "AWS CloudHSM with custom key rotation scripts",
        "isCorrect": false
      },
      {
        "text": "Application-level encryption with database-stored keys",
        "isCorrect": false
      }
    ],
    "explanation": "AWS KMS provides FIPS 140-2 Level 2 validated HSMs, automatic key rotation capabilities, and integrates with CloudTrail for comprehensive audit logging. It supports both server-side encryption and client-side encryption, making it ideal for financial compliance requirements.",
    "awsService": "KMS"
  },
  {
    "id": "q116",
    "questionText": "A company wants to implement a microservices architecture with service discovery, load balancing, and health checks. The services need to scale independently and communicate securely. What AWS solution provides the best approach?",
    "options": [
      {
        "text": "ECS with Application Load Balancer and Route 53",
        "isCorrect": false
      },
      {
        "text": "EKS with AWS Load Balancer Controller and AWS App Mesh",
        "isCorrect": true
      },
      {
        "text": "Lambda functions with API Gateway",
        "isCorrect": false
      },
      {
        "text": "EC2 instances with Consul and HAProxy",
        "isCorrect": false
      }
    ],
    "explanation": "EKS provides a managed Kubernetes platform for microservices, AWS Load Balancer Controller enables advanced load balancing features, and AWS App Mesh provides service mesh capabilities including service discovery, traffic management, and observability for secure service-to-service communication.",
    "awsService": "EKS"
  },
  {
    "id": "q117",
    "questionText": "A data science team needs to perform complex analytics on datasets ranging from 100GB to 10TB. The queries are ad-hoc and unpredictable. They want a serverless solution that can scale automatically and only charge for actual usage. What service should they use?",
    "options": [
      {
        "text": "Amazon Redshift with Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "Amazon Athena with data partitioning",
        "isCorrect": true
      },
      {
        "text": "Amazon EMR with Spot instances",
        "isCorrect": false
      },
      {
        "text": "Amazon RDS with read replicas",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Athena is serverless and perfect for ad-hoc analytics on large datasets stored in S3. It automatically scales and you only pay for queries run. Proper data partitioning in S3 significantly improves query performance and reduces costs for large datasets.",
    "awsService": "Athena"
  },
  {
    "id": "q118",
    "questionText": "An application needs to process video files uploaded to S3. Processing takes 10-30 minutes per file and should start immediately after upload. The processing is CPU-intensive and requires specific software libraries. What architecture provides the best solution?",
    "options": [
      {
        "text": "S3 Event Notification + Lambda + EC2 instances",
        "isCorrect": false
      },
      {
        "text": "S3 Event Notification + SQS + ECS with Fargate",
        "isCorrect": true
      },
      {
        "text": "S3 Event Notification + Step Functions + Lambda",
        "isCorrect": false
      },
      {
        "text": "CloudWatch Events + Lambda + Batch",
        "isCorrect": false
      }
    ],
    "explanation": "S3 Event Notifications trigger immediately on upload, SQS provides reliable queuing for processing requests, and ECS with Fargate allows containerized processing with custom software libraries. This architecture scales automatically and handles long-running CPU-intensive tasks efficiently.",
    "awsService": "ECS"
  },
  {
    "id": "q119",
    "questionText": "A company needs to implement a solution that monitors AWS resources for compliance violations and automatically remediate common issues. The solution should support custom rules and provide detailed reporting. What services should be used?",
    "options": [
      {
        "text": "AWS Config + AWS Config Rules + Systems Manager Automation",
        "isCorrect": true
      },
      {
        "text": "CloudWatch + CloudWatch Alarms + Lambda",
        "isCorrect": false
      },
      {
        "text": "AWS Security Hub + GuardDuty + Inspector",
        "isCorrect": false
      },
      {
        "text": "CloudTrail + CloudWatch Logs + SNS",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Config continuously monitors resource configurations, Config Rules evaluate compliance against desired configurations (including custom rules), and Systems Manager Automation can automatically remediate violations. This combination provides comprehensive compliance monitoring and automated remediation.",
    "awsService": "Config"
  },
  {
    "id": "q120",
    "questionText": "A gaming company wants to implement a global leaderboard system that can handle millions of concurrent users with sub-millisecond latency. The system needs to support real-time updates and maintain consistency. What database solution should they use?",
    "options": [
      {
        "text": "DynamoDB with Global Tables and DAX",
        "isCorrect": true
      },
      {
        "text": "Aurora Global Database with read replicas",
        "isCorrect": false
      },
      {
        "text": "ElastiCache for Redis with cluster mode",
        "isCorrect": false
      },
      {
        "text": "DocumentDB with cross-region replication",
        "isCorrect": false
      }
    ],
    "explanation": "DynamoDB with Global Tables provides multi-region, multi-master replication for global scale, while DAX (DynamoDB Accelerator) provides microsecond latency for reads. This combination handles millions of concurrent users with the required sub-millisecond performance for gaming applications.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q121",
    "questionText": "A company wants to optimize their AWS costs across multiple accounts. They need detailed cost analysis, budget alerts, and recommendations for cost optimization. What combination of AWS services should they implement?",
    "options": [
      {
        "text": "Cost Explorer + Budgets + Trusted Advisor",
        "isCorrect": false
      },
      {
        "text": "Cost Explorer + Budgets + Compute Optimizer + Trusted Advisor",
        "isCorrect": true
      },
      {
        "text": "CloudWatch + CloudTrail + Cost Allocation Tags",
        "isCorrect": false
      },
      {
        "text": "AWS Organizations + Service Control Policies + CloudFormation",
        "isCorrect": false
      }
    ],
    "explanation": "Cost Explorer provides detailed cost analysis and trends, Budgets enables proactive cost management with alerts, Compute Optimizer provides ML-powered recommendations for right-sizing, and Trusted Advisor offers additional cost optimization recommendations. This comprehensive combination covers all aspects of cost optimization.",
    "awsService": "Cost Management"
  },
  {
    "id": "q122",
    "questionText": "An IoT application receives sensor data from millions of devices every minute. The data needs to be processed in real-time for anomaly detection and stored for batch analytics. What architecture should be implemented?",
    "options": [
      {
        "text": "API Gateway + Lambda + DynamoDB + S3",
        "isCorrect": false
      },
      {
        "text": "IoT Core + Kinesis Data Streams + Lambda + S3 + Kinesis Analytics",
        "isCorrect": true
      },
      {
        "text": "IoT Core + SQS + EC2 + RDS + RedShift",
        "isCorrect": false
      },
      {
        "text": "Direct Connect + Kinesis Data Firehose + ElasticSearch",
        "isCorrect": false
      }
    ],
    "explanation": "IoT Core handles device connectivity and message routing, Kinesis Data Streams provides high-throughput ingestion, Lambda processes data for real-time anomaly detection, S3 stores data for batch analytics, and Kinesis Analytics enables real-time stream processing for immediate insights.",
    "awsService": "IoT Core"
  },
  {
    "id": "q123",
    "questionText": "A company needs to implement a data lake solution that can store structured and unstructured data, provide fine-grained access control, and enable both batch and interactive analytics. What AWS services combination should be used?",
    "options": [
      {
        "text": "S3 + Glue + Athena + QuickSight",
        "isCorrect": false
      },
      {
        "text": "S3 + Lake Formation + Glue + Athena + EMR",
        "isCorrect": true
      },
      {
        "text": "Redshift + Spectrum + S3 + Glue",
        "isCorrect": false
      },
      {
        "text": "DynamoDB + S3 + Lambda + Kinesis Analytics",
        "isCorrect": false
      }
    ],
    "explanation": "S3 provides scalable storage for the data lake, Lake Formation simplifies setup and provides fine-grained access control with column-level security, Glue handles ETL and data catalog, Athena enables interactive SQL queries, and EMR provides big data processing capabilities for batch analytics.",
    "awsService": "Lake Formation"
  },
  {
    "id": "q124",
    "questionText": "A startup wants to implement a cost-effective backup strategy for their production environment that includes EC2 instances, EBS volumes, and RDS databases. They need point-in-time recovery and want to automate the backup process. What solution should they use?",
    "options": [
      {
        "text": "AWS Backup with backup plans and lifecycle policies",
        "isCorrect": true
      },
      {
        "text": "Lambda functions with boto3 to create manual snapshots",
        "isCorrect": false
      },
      {
        "text": "EBS snapshots + RDS automated backups + AMI creation scripts",
        "isCorrect": false
      },
      {
        "text": "AWS DataSync + S3 with versioning enabled",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Backup provides a centralized backup solution across multiple AWS services, with automated backup plans, lifecycle policies for cost optimization, point-in-time recovery, and compliance reporting. It's the most comprehensive and cost-effective solution for multi-service backup requirements.",
    "awsService": "AWS Backup"
  },
  {
    "id": "q125",
    "questionText": "A financial trading application requires ultra-low latency networking between compute instances for high-frequency trading algorithms. What EC2 features should be configured to minimize network latency?",
    "options": [
      {
        "text": "Placement groups + Enhanced networking + Instance types with high network performance",
        "isCorrect": true
      },
      {
        "text": "Multiple AZs + Load balancing + Auto Scaling groups",
        "isCorrect": false
      },
      {
        "text": "Reserved instances + Dedicated hosts + EBS optimization",
        "isCorrect": false
      },
      {
        "text": "Spot instances + Mixed instance types + Elastic IP addresses",
        "isCorrect": false
      }
    ],
    "explanation": "Cluster placement groups provide 10 Gbps network performance between instances, Enhanced Networking (SR-IOV) reduces network latency and jitter, and selecting instance types specifically designed for high network performance (like C5n, M5n, R5n) minimizes latency for HFT applications.",
    "awsService": "EC2"
  },
  {
    "id": "q126",
    "questionText": "A company wants to implement a serverless web application with user authentication, file uploads, and real-time notifications. The application should scale automatically and minimize operational overhead. What architecture should be used?",
    "options": [
      {
        "text": "Cognito + API Gateway + Lambda + S3 + SNS",
        "isCorrect": false
      },
      {
        "text": "Cognito + API Gateway + Lambda + S3 + WebSocket API + DynamoDB",
        "isCorrect": true
      },
      {
        "text": "ALB + ECS + RDS + ElastiCache + SQS",
        "isCorrect": false
      },
      {
        "text": "CloudFront + S3 + EC2 + RDS + Redis",
        "isCorrect": false
      }
    ],
    "explanation": "Cognito handles user authentication, API Gateway provides REST endpoints, Lambda processes business logic, S3 stores files, WebSocket API enables real-time notifications, and DynamoDB provides scalable NoSQL storage. This fully serverless architecture scales automatically with minimal operational overhead.",
    "awsService": "API Gateway"
  },
  {
    "id": "q127",
    "questionText": "A machine learning company needs to train large models that require hundreds of GPUs working together. The training jobs can take several days to complete and need fault tolerance. What AWS service is BEST suited for this workload?",
    "options": [
      {
        "text": "EC2 P4 instances with EFA networking",
        "isCorrect": false
      },
      {
        "text": "SageMaker Training with distributed training",
        "isCorrect": true
      },
      {
        "text": "AWS Batch with GPU-enabled instances",
        "isCorrect": false
      },
      {
        "text": "EKS with GPU-enabled node groups",
        "isCorrect": false
      }
    ],
    "explanation": "SageMaker Training is specifically designed for machine learning workloads, supports distributed training across multiple GPU instances, provides built-in fault tolerance with checkpointing, and offers optimized ML frameworks. It's the most suitable service for large-scale ML training requirements.",
    "awsService": "SageMaker"
  },
  {
    "id": "q128",
    "questionText": "A company wants to implement a hybrid cloud storage solution where frequently accessed data is stored on-premises and infrequently accessed data is automatically moved to AWS. What AWS service should they use?",
    "options": [
      {
        "text": "AWS Storage Gateway - Volume Gateway",
        "isCorrect": false
      },
      {
        "text": "AWS Storage Gateway - File Gateway",
        "isCorrect": true
      },
      {
        "text": "AWS DataSync with scheduled transfers",
        "isCorrect": false
      },
      {
        "text": "Direct Connect with S3 Transfer Acceleration",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Storage Gateway File Gateway provides a seamless bridge between on-premises environments and S3. It caches frequently accessed data locally while automatically storing all data in S3, enabling hybrid cloud storage with intelligent tiering based on access patterns.",
    "awsService": "Storage Gateway"
  },
  {
    "id": "q129",
    "questionText": "A global e-commerce application experiences seasonal traffic spikes and wants to implement automatic scaling across multiple regions. The application uses containers and needs zero-downtime deployments. What solution provides the best approach?",
    "options": [
      {
        "text": "ECS with Auto Scaling and Application Load Balancer",
        "isCorrect": false
      },
      {
        "text": "EKS with Cluster Autoscaler, HPA, and Blue/Green deployments",
        "isCorrect": true
      },
      {
        "text": "Lambda functions with reserved concurrency",
        "isCorrect": false
      },
      {
        "text": "EC2 Auto Scaling groups with multiple AZs",
        "isCorrect": false
      }
    ],
    "explanation": "EKS provides managed Kubernetes with global reach, Cluster Autoscaler handles node scaling, Horizontal Pod Autoscaler (HPA) scales application pods based on metrics, and Blue/Green deployments enable zero-downtime updates. This combination provides enterprise-grade container orchestration with automatic scaling.",
    "awsService": "EKS"
  },
  {
    "id": "q130",
    "questionText": "A healthcare organization needs to process and analyze medical images (DICOM files) with machine learning for diagnostic assistance. The solution must be HIPAA compliant and provide audit trails. What AWS architecture should be implemented?",
    "options": [
      {
        "text": "S3 + SageMaker + CloudTrail + KMS",
        "isCorrect": true
      },
      {
        "text": "EFS + EC2 + Lambda + CloudWatch",
        "isCorrect": false
      },
      {
        "text": "Glacier + Batch + GuardDuty + Config",
        "isCorrect": false
      },
      {
        "text": "WorkDocs + Comprehend Medical + WAF",
        "isCorrect": false
      }
    ],
    "explanation": "S3 provides HIPAA-eligible storage for DICOM files with server-side encryption, SageMaker offers HIPAA-eligible ML services for image analysis, CloudTrail provides audit trails for compliance, and KMS manages encryption keys. This combination meets healthcare compliance requirements while enabling ML capabilities.",
    "awsService": "SageMaker"
  },
  {
    "id": "q131",
    "questionText": "A company wants to implement a CDN solution that can handle both static and dynamic content, provide real-time analytics, and integrate with their existing AWS infrastructure. What solution should they choose?",
    "options": [
      {
        "text": "CloudFront with Lambda@Edge and Real-time Logs",
        "isCorrect": true
      },
      {
        "text": "Application Load Balancer with Global Accelerator",
        "isCorrect": false
      },
      {
        "text": "Route 53 with health checks and failover routing",
        "isCorrect": false
      },
      {
        "text": "API Gateway with caching enabled and CloudWatch",
        "isCorrect": false
      }
    ],
    "explanation": "CloudFront provides global CDN capabilities for both static and dynamic content, Lambda@Edge enables dynamic content processing at edge locations, and Real-time Logs provide immediate access to detailed analytics data. This solution offers the most comprehensive CDN features with AWS integration.",
    "awsService": "CloudFront"
  },
  {
    "id": "q132",
    "questionText": "A DevOps team wants to implement infrastructure testing and validation before deploying changes to production. They need to test both infrastructure configuration and application behavior. What approach should they use?",
    "options": [
      {
        "text": "CloudFormation with drift detection and AWS Config rules",
        "isCorrect": false
      },
      {
        "text": "AWS CodePipeline with AWS CodeBuild for testing and AWS CodeDeploy with blue/green deployment",
        "isCorrect": true
      },
      {
        "text": "Terraform with terraform validate and manual testing",
        "isCorrect": false
      },
      {
        "text": "AWS Systems Manager with patch management and compliance scanning",
        "isCorrect": false
      }
    ],
    "explanation": "CodePipeline orchestrates the testing workflow, CodeBuild runs infrastructure validation tests and application tests in isolated environments, and CodeDeploy with blue/green deployment allows safe production deployment with easy rollback capabilities. This provides comprehensive testing and validation.",
    "awsService": "CodePipeline"
  },
  {
    "id": "q133",
    "questionText": "A financial services company needs to implement data masking and tokenization for sensitive customer data across their applications. The solution must support different masking techniques and maintain referential integrity. What AWS service should they use?",
    "options": [
      {
        "text": "AWS Secrets Manager with automatic rotation",
        "isCorrect": false
      },
      {
        "text": "AWS Glue with custom ETL scripts for data transformation",
        "isCorrect": true
      },
      {
        "text": "AWS KMS with envelope encryption",
        "isCorrect": false
      },
      {
        "text": "Amazon Macie for data discovery and classification",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Glue provides powerful ETL capabilities with custom Python/Scala scripts that can implement various data masking and tokenization techniques while maintaining referential integrity. It can process data at scale and integrate with multiple data sources and targets.",
    "awsService": "Glue"
  },
  {
    "id": "q134",
    "questionText": "A company operates a multi-tenant SaaS application where each tenant's data must be isolated and encrypted with separate keys. They want to implement a centralized key management strategy that provides audit trails and key rotation. What solution should they implement?",
    "options": [
      {
        "text": "AWS KMS with customer-managed keys per tenant and CloudTrail logging",
        "isCorrect": true
      },
      {
        "text": "Application-level encryption with keys stored in Parameter Store",
        "isCorrect": false
      },
      {
        "text": "Database-level encryption with separate schemas per tenant",
        "isCorrect": false
      },
      {
        "text": "S3 bucket encryption with default AWS managed keys",
        "isCorrect": false
      }
    ],
    "explanation": "AWS KMS with customer-managed keys provides tenant-specific encryption keys with centralized management, automatic key rotation capabilities, and comprehensive audit trails through CloudTrail. This approach ensures data isolation while maintaining operational efficiency and compliance.",
    "awsService": "KMS"
  },
  {
    "id": "q135",
    "questionText": "A gaming company wants to implement a real-time matchmaking system that can handle millions of concurrent players worldwide. The system needs low latency, automatic scaling, and session state persistence. What architecture should they use?",
    "options": [
      {
        "text": "API Gateway + Lambda + DynamoDB + ElastiCache",
        "isCorrect": false
      },
      {
        "text": "GameLift + DynamoDB + ElastiCache for Redis with Global Datastore",
        "isCorrect": true
      },
      {
        "text": "EC2 with Auto Scaling + Application Load Balancer + RDS",
        "isCorrect": false
      },
      {
        "text": "Fargate + Application Load Balancer + Aurora Global Database",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon GameLift provides dedicated game server hosting with automatic scaling and fleet management, DynamoDB handles player data with global scale, and ElastiCache for Redis with Global Datastore provides ultra-low latency session state persistence across regions for optimal gaming experience.",
    "awsService": "GameLift"
  },
  {
    "id": "q136",
    "questionText": "A company wants to implement a document processing pipeline that can extract text from various file formats (PDF, images, handwritten documents), analyze sentiment, and store structured data. What AWS services should be combined?",
    "options": [
      {
        "text": "Textract + Comprehend + Lambda + DynamoDB",
        "isCorrect": true
      },
      {
        "text": "Rekognition + Transcribe + Step Functions + S3",
        "isCorrect": false
      },
      {
        "text": "SageMaker + Polly + API Gateway + RDS",
        "isCorrect": false
      },
      {
        "text": "Translate + Lex + Kinesis + Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Textract extracts text and data from documents including handwritten text, Amazon Comprehend analyzes sentiment and extracts insights from text, Lambda orchestrates the processing pipeline, and DynamoDB stores the structured results. This combination provides end-to-end document processing capabilities.",
    "awsService": "Textract"
  },
  {
    "id": "q137",
    "questionText": "A media company needs to implement a video streaming platform that can handle live broadcasts, on-demand content, and adaptive bitrate streaming for global audiences. What AWS architecture should they implement?",
    "options": [
      {
        "text": "MediaLive + MediaStore + CloudFront + Elemental MediaConvert",
        "isCorrect": true
      },
      {
        "text": "EC2 with FFmpeg + S3 + CloudFront + Lambda",
        "isCorrect": false
      },
      {
        "text": "Kinesis Video Streams + S3 + API Gateway + Lambda",
        "isCorrect": false
      },
      {
        "text": "ECS with streaming containers + ALB + Route 53",
        "isCorrect": false
      }
    ],
    "explanation": "AWS MediaLive provides live video processing and streaming, MediaStore offers optimized storage for media content, CloudFront delivers global content distribution with low latency, and Elemental MediaConvert handles video processing and adaptive bitrate encoding for various devices and network conditions.",
    "awsService": "MediaLive"
  },
  {
    "id": "q138",
    "questionText": "A company wants to implement predictive maintenance for their IoT devices using machine learning. They need to collect sensor data, train models, and deploy them for real-time inference. What end-to-end solution should they use?",
    "options": [
      {
        "text": "IoT Core + Kinesis + SageMaker + Lambda + IoT Greengrass",
        "isCorrect": true
      },
      {
        "text": "API Gateway + Lambda + DynamoDB + EC2 + CloudWatch",
        "isCorrect": false
      },
      {
        "text": "Kinesis + EMR + S3 + Batch + ECS",
        "isCorrect": false
      },
      {
        "text": "Direct Connect + Glue + Athena + QuickSight",
        "isCorrect": false
      }
    ],
    "explanation": "IoT Core collects sensor data from devices, Kinesis streams data for processing, SageMaker handles model training and deployment, Lambda processes inference requests, and IoT Greengrass enables edge computing for real-time inference on IoT devices, creating a complete predictive maintenance solution.",
    "awsService": "IoT Core"
  },
  {
    "id": "q139",
    "questionText": "A startup wants to implement a cost-effective solution for their development, testing, and staging environments that can be easily provisioned and destroyed. They want infrastructure as code and environment consistency. What approach should they use?",
    "options": [
      {
        "text": "AWS CDK with multiple stacks and environment-specific parameters",
        "isCorrect": true
      },
      {
        "text": "Manual EC2 instance creation with saved AMIs",
        "isCorrect": false
      },
      {
        "text": "Docker containers on single EC2 instance",
        "isCorrect": false
      },
      {
        "text": "AWS Quick Starts with default configurations",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CDK (Cloud Development Kit) enables infrastructure as code using familiar programming languages, supports multiple stacks for different environments, allows environment-specific parameters, and provides easy provisioning/destruction of resources. This approach ensures consistency and cost-effectiveness for development workflows.",
    "awsService": "CDK"
  },
  {
    "id": "q140",
    "questionText": "A company needs to implement a solution for processing millions of small files (1-10KB each) stored in S3. The processing is CPU-intensive and needs to scale based on the number of files in the queue. What architecture provides the best performance and cost optimization?",
    "options": [
      {
        "text": "S3 Event Notifications + Lambda with high memory configuration",
        "isCorrect": false
      },
      {
        "text": "S3 Event Notifications + SQS + EC2 Auto Scaling with Spot instances",
        "isCorrect": true
      },
      {
        "text": "S3 Event Notifications + Step Functions + Fargate",
        "isCorrect": false
      },
      {
        "text": "S3 Transfer Acceleration + API Gateway + Lambda",
        "isCorrect": false
      }
    ],
    "explanation": "For millions of small files requiring CPU-intensive processing, S3 Event Notifications trigger processing requests, SQS provides reliable queuing with visibility timeouts, and EC2 Auto Scaling with Spot instances provides cost-effective compute power that scales based on queue depth. This architecture handles high volume efficiently.",
    "awsService": "SQS"
  },
  {
    "id": "q141",
    "questionText": "A financial institution needs to implement automated security scanning and vulnerability management across their AWS infrastructure. The solution should provide continuous monitoring, threat detection, and compliance reporting. What services should be combined?",
    "options": [
      {
        "text": "Security Hub + GuardDuty + Inspector + Config + Systems Manager Patch Manager",
        "isCorrect": true
      },
      {
        "text": "CloudTrail + CloudWatch + WAF + Shield",
        "isCorrect": false
      },
      {
        "text": "Macie + Secrets Manager + Certificate Manager + KMS",
        "isCorrect": false
      },
      {
        "text": "Organizations + Control Tower + Service Control Policies",
        "isCorrect": false
      }
    ],
    "explanation": "Security Hub provides centralized security findings management, GuardDuty offers intelligent threat detection, Inspector performs vulnerability assessments, Config monitors compliance, and Systems Manager Patch Manager automates patch management. This comprehensive combination covers all aspects of security scanning and vulnerability management.",
    "awsService": "Security Hub"
  },
  {
    "id": "q142",
    "questionText": "A company wants to implement a real-time recommendation engine for their e-commerce platform. The system needs to process user behavior events, update recommendations instantly, and serve millions of requests per second. What architecture should they use?",
    "options": [
      {
        "text": "Kinesis Data Streams + Lambda + DynamoDB + API Gateway + CloudFront",
        "isCorrect": false
      },
      {
        "text": "Kinesis Data Streams + Kinesis Analytics + DynamoDB + ElastiCache + Application Load Balancer",
        "isCorrect": true
      },
      {
        "text": "SQS + EC2 + RDS + ElasticSearch + Route 53",
        "isCorrect": false
      },
      {
        "text": "API Gateway + Lambda + S3 + Athena + QuickSight",
        "isCorrect": false
      }
    ],
    "explanation": "Kinesis Data Streams ingests real-time user behavior events, Kinesis Analytics processes streams to generate recommendations, DynamoDB stores user profiles and product data, ElastiCache provides microsecond latency for serving recommendations, and Application Load Balancer distributes traffic across multiple endpoints for high throughput.",
    "awsService": "Kinesis"
  },
  {
    "id": "q143",
    "questionText": "A company operates a global application with users in North America, Europe, and Asia. They want to implement a database solution that provides low-latency reads in all regions while maintaining strong consistency for writes. What database architecture should they use?",
    "options": [
      {
        "text": "RDS with cross-region read replicas",
        "isCorrect": false
      },
      {
        "text": "Aurora Global Database with read replicas in each region",
        "isCorrect": true
      },
      {
        "text": "DynamoDB Global Tables with eventual consistency",
        "isCorrect": false
      },
      {
        "text": "DocumentDB with manual cross-region replication",
        "isCorrect": false
      }
    ],
    "explanation": "Aurora Global Database provides a single database that spans multiple regions with typically less than 1 second replication lag between regions. It offers low-latency local reads in each region while maintaining strong consistency for writes to the primary region, making it ideal for global applications requiring both performance and consistency.",
    "awsService": "Aurora"
  },
  {
    "id": "q144",
    "questionText": "A healthcare organization wants to implement a HIPAA-compliant data analytics platform for processing patient data. The solution needs to support both real-time and batch analytics while maintaining strict access controls. What architecture should they implement?",
    "options": [
      {
        "text": "Kinesis + Lambda + S3 + Athena with encryption and IAM",
        "isCorrect": false
      },
      {
        "text": "Kinesis + S3 + Lake Formation + Glue + EMR with encryption at rest and in transit",
        "isCorrect": true
      },
      {
        "text": "API Gateway + DynamoDB + QuickSight with VPC endpoints",
        "isCorrect": false
      },
      {
        "text": "SQS + EC2 + RDS + Redshift with dedicated tenancy",
        "isCorrect": false
      }
    ],
    "explanation": "This architecture uses HIPAA-eligible services: Kinesis for real-time data ingestion, S3 for encrypted storage, Lake Formation for fine-grained access control and data governance, Glue for ETL processing, and EMR for big data analytics. All services support encryption at rest and in transit, meeting HIPAA compliance requirements.",
    "awsService": "Lake Formation"
  },
  {
    "id": "q145",
    "questionText": "A company wants to implement a multi-account strategy for their organization with centralized governance, billing management, and security policies. They need to automate account creation and apply policies consistently. What AWS solution should they use?",
    "options": [
      {
        "text": "AWS Organizations + Control Tower + Service Control Policies + Account Factory",
        "isCorrect": true
      },
      {
        "text": "IAM + CloudFormation + Config + CloudTrail",
        "isCorrect": false
      },
      {
        "text": "Resource Groups + Systems Manager + AWS Config",
        "isCorrect": false
      },
      {
        "text": "Service Catalog + CloudFormation StackSets + IAM",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Organizations provides multi-account management, Control Tower offers account governance and compliance, Service Control Policies enforce organization-wide policies, and Account Factory automates account provisioning with pre-configured security guardrails. This combination provides comprehensive multi-account governance.",
    "awsService": "Control Tower"
  },
  {
    "id": "q146",
    "questionText": "A company needs to migrate their on-premises Hadoop cluster to AWS while maintaining compatibility with existing Spark and Hive jobs. They want a managed solution that can scale dynamically and integrate with other AWS services. What migration strategy should they use?",
    "options": [
      {
        "text": "Migrate to EC2 instances with manual Hadoop installation",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EMR with existing job configurations and S3 for data storage",
        "isCorrect": true
      },
      {
        "text": "Migrate to EKS with containerized Hadoop components",
        "isCorrect": false
      },
      {
        "text": "Use AWS Glue to replace all Hadoop workloads",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon EMR is a managed Hadoop service that supports existing Spark and Hive jobs with minimal changes. It integrates seamlessly with S3 for data storage, provides automatic scaling, and offers cost optimization features like Spot instances while maintaining compatibility with existing big data workflows.",
    "awsService": "EMR"
  },
  {
    "id": "q147",
    "questionText": "A social media company wants to implement content moderation using machine learning to detect inappropriate images and text in real-time. The solution should handle millions of posts per day and provide confidence scores. What AWS services should be combined?",
    "options": [
      {
        "text": "Rekognition + Comprehend + Step Functions + DynamoDB",
        "isCorrect": true
      },
      {
        "text": "SageMaker + Lambda + S3 + ElasticSearch",
        "isCorrect": false
      },
      {
        "text": "Textract + Translate + API Gateway + RDS",
        "isCorrect": false
      },
      {
        "text": "Lex + Polly + Kinesis + Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Rekognition provides image moderation with confidence scores for inappropriate content detection, Amazon Comprehend analyzes text for sentiment and inappropriate content, Step Functions orchestrates the moderation workflow, and DynamoDB stores moderation results and metadata for high-volume processing.",
    "awsService": "Rekognition"
  },
  {
    "id": "q148",
    "questionText": "A company wants to implement a serverless data processing pipeline that can handle both scheduled batch jobs and event-driven processing. The pipeline needs to support complex workflows with error handling and retry logic. What solution should they use?",
    "options": [
      {
        "text": "Lambda + CloudWatch Events + SQS + DLQ",
        "isCorrect": false
      },
      {
        "text": "Step Functions + Lambda + EventBridge + S3",
        "isCorrect": true
      },
      {
        "text": "Glue + Batch + CloudFormation + SNS",
        "isCorrect": false
      },
      {
        "text": "Kinesis + Lambda + DynamoDB + CloudWatch",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Step Functions provides serverless workflow orchestration with built-in error handling and retry logic, Lambda executes individual processing steps, EventBridge handles both scheduled and event-driven triggers, and S3 stores processed data. This combination offers comprehensive serverless data processing capabilities.",
    "awsService": "Step Functions"
  },
  {
    "id": "q149",
    "questionText": "A company operates a microservices architecture and wants to implement distributed tracing to monitor application performance and troubleshoot issues across service boundaries. What AWS solution should they implement?",
    "options": [
      {
        "text": "CloudWatch + CloudTrail + Config",
        "isCorrect": false
      },
      {
        "text": "X-Ray + CloudWatch + Application Insights",
        "isCorrect": true
      },
      {
        "text": "GuardDuty + Security Hub + Inspector",
        "isCorrect": false
      },
      {
        "text": "Systems Manager + OpsWorks + CloudFormation",
        "isCorrect": false
      }
    ],
    "explanation": "AWS X-Ray provides distributed tracing capabilities to track requests across microservices, CloudWatch collects metrics and logs, and Application Insights automatically discovers application topology and provides intelligent analytics. This combination enables comprehensive monitoring and troubleshooting of distributed applications.",
    "awsService": "X-Ray"
  },
  {
    "id": "q150",
    "questionText": "A company wants to implement a solution for automated infrastructure remediation that can detect configuration drift, security violations, and performance issues, then automatically fix common problems. What combination of services provides the best approach?",
    "options": [
      {
        "text": "Config + CloudWatch + Lambda + Systems Manager Automation",
        "isCorrect": true
      },
      {
        "text": "CloudTrail + SNS + Lambda + EC2",
        "isCorrect": false
      },
      {
        "text": "Security Hub + GuardDuty + WAF + Shield",
        "isCorrect": false
      },
      {
        "text": "Trusted Advisor + Support API + CloudFormation",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Config monitors resource configurations and detects drift, CloudWatch monitors performance metrics and security events, Lambda processes alerts and triggers remediation, and Systems Manager Automation executes pre-defined remediation runbooks. This combination provides comprehensive automated infrastructure remediation capabilities.",
    "awsService": "Systems Manager"
  },
  {
    "id": "q151",
    "questionText": "A company needs to store large amounts of archival data that is rarely accessed. The data must be retained for 7 years for compliance purposes. Which S3 storage class would be most cost-effective?",
    "options": [
      {
        "text": "S3 Standard",
        "isCorrect": false
      },
      {
        "text": "S3 Standard-IA",
        "isCorrect": false
      },
      {
        "text": "S3 Glacier Deep Archive",
        "isCorrect": true
      },
      {
        "text": "S3 One Zone-IA",
        "isCorrect": false
      }
    ],
    "explanation": "S3 Glacier Deep Archive is designed for long-term archival storage with the lowest cost and retrieval times of 12 hours or more, making it perfect for compliance data that's rarely accessed.",
    "awsService": "S3"
  },
  {
    "id": "q152",
    "questionText": "An application requires a database that can handle millions of requests per second with single-digit millisecond latency. Which AWS service would be most appropriate?",
    "options": [
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": true
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      },
      {
        "text": "Amazon Aurora",
        "isCorrect": false
      }
    ],
    "explanation": "DynamoDB is a NoSQL database service that provides fast and predictable performance with seamless scalability, capable of handling millions of requests per second with single-digit millisecond latency.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q153",
    "questionText": "A web application needs to process user uploads in the background. The processing can take anywhere from 1 minute to 15 minutes. Which compute service would be most cost-effective?",
    "options": [
      {
        "text": "Amazon EC2 On-Demand instances",
        "isCorrect": false
      },
      {
        "text": "AWS Lambda",
        "isCorrect": false
      },
      {
        "text": "Amazon ECS with Fargate",
        "isCorrect": true
      },
      {
        "text": "Amazon EC2 Spot instances",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Lambda has a maximum execution time of 15 minutes, making it unsuitable for longer processing. ECS with Fargate is ideal for variable-length background processing as you pay only for the compute time used.",
    "awsService": "ECS"
  },
  {
    "id": "q154",
    "questionText": "A company wants to implement blue-green deployments for their web application. Which AWS service combination would best support this pattern?",
    "options": [
      {
        "text": "Route 53 and Application Load Balancer",
        "isCorrect": true
      },
      {
        "text": "CloudFront and S3",
        "isCorrect": false
      },
      {
        "text": "EC2 Auto Scaling and Network Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Lambda and API Gateway",
        "isCorrect": false
      }
    ],
    "explanation": "Route 53 weighted routing policies combined with Application Load Balancer allow you to gradually shift traffic between blue and green environments, enabling safe blue-green deployments.",
    "awsService": "Route 53"
  },
  {
    "id": "q155",
    "questionText": "An organization needs to ensure all AWS API calls are logged for security auditing. Which service should they enable?",
    "options": [
      {
        "text": "AWS CloudWatch",
        "isCorrect": false
      },
      {
        "text": "AWS CloudTrail",
        "isCorrect": true
      },
      {
        "text": "AWS Config",
        "isCorrect": false
      },
      {
        "text": "AWS X-Ray",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing by logging all AWS API calls made in your account.",
    "awsService": "CloudTrail"
  },
  {
    "id": "q156",
    "questionText": "A startup needs a managed database service that can automatically scale read capacity based on demand without downtime. Which service should they choose?",
    "options": [
      {
        "text": "Amazon RDS MySQL",
        "isCorrect": false
      },
      {
        "text": "Amazon Aurora Serverless",
        "isCorrect": true
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Aurora Serverless automatically starts, scales compute capacity up or down based on application needs, and shuts down when not in use, providing automatic scaling without downtime.",
    "awsService": "Aurora"
  },
  {
    "id": "q157",
    "questionText": "A company wants to run containers without managing the underlying infrastructure. Which AWS service provides the most serverless container experience?",
    "options": [
      {
        "text": "Amazon ECS with EC2",
        "isCorrect": false
      },
      {
        "text": "Amazon EKS",
        "isCorrect": false
      },
      {
        "text": "AWS Fargate",
        "isCorrect": true
      },
      {
        "text": "AWS Batch",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Fargate is a serverless compute engine for containers that allows you to run containers without having to manage servers or clusters of Amazon EC2 instances.",
    "awsService": "Fargate"
  },
  {
    "id": "q158",
    "questionText": "An e-commerce application experiences high traffic during sales events. The application uses a relational database. How can you improve read performance during these events?",
    "options": [
      {
        "text": "Increase the instance size",
        "isCorrect": false
      },
      {
        "text": "Add read replicas",
        "isCorrect": true
      },
      {
        "text": "Enable Multi-AZ deployment",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Read replicas allow you to scale read capacity by creating read-only copies of your database that can handle read queries, reducing load on the primary database during high-traffic events.",
    "awsService": "RDS"
  },
  {
    "id": "q159",
    "questionText": "A media company needs to distribute video content globally with low latency. Which AWS service should they use?",
    "options": [
      {
        "text": "Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Amazon CloudFront",
        "isCorrect": true
      },
      {
        "text": "AWS Global Accelerator",
        "isCorrect": false
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon CloudFront is a content delivery network (CDN) service that delivers content to users globally with low latency by caching content at edge locations worldwide.",
    "awsService": "CloudFront"
  },
  {
    "id": "q160",
    "questionText": "A financial services company needs to encrypt sensitive data in their S3 bucket using their own encryption keys. Which encryption option should they use?",
    "options": [
      {
        "text": "SSE-S3",
        "isCorrect": false
      },
      {
        "text": "SSE-KMS",
        "isCorrect": false
      },
      {
        "text": "SSE-C",
        "isCorrect": true
      },
      {
        "text": "Client-side encryption",
        "isCorrect": false
      }
    ],
    "explanation": "SSE-C (Server-Side Encryption with Customer-Provided Keys) allows you to provide your own encryption keys while Amazon S3 handles the encryption/decryption process.",
    "awsService": "S3"
  },
  {
    "id": "q161",
    "questionText": "A development team needs to test their application against a copy of production data. What is the most cost-effective way to create a test database?",
    "options": [
      {
        "text": "Create a new RDS instance and import data",
        "isCorrect": false
      },
      {
        "text": "Create a snapshot and restore to a smaller instance",
        "isCorrect": true
      },
      {
        "text": "Use RDS read replica",
        "isCorrect": false
      },
      {
        "text": "Export data to S3 and create new database",
        "isCorrect": false
      }
    ],
    "explanation": "Creating a snapshot of the production database and restoring it to a smaller, less expensive instance type provides an exact copy of production data while minimizing costs for testing.",
    "awsService": "RDS"
  },
  {
    "id": "q162",
    "questionText": "An application needs to process messages asynchronously with the ability to handle message failures and retry logic. Which service should be used?",
    "options": [
      {
        "text": "Amazon SNS",
        "isCorrect": false
      },
      {
        "text": "Amazon SQS",
        "isCorrect": true
      },
      {
        "text": "Amazon Kinesis",
        "isCorrect": false
      },
      {
        "text": "AWS Step Functions",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon SQS (Simple Queue Service) provides reliable message queuing with built-in retry logic, dead letter queues for handling failures, and asynchronous message processing capabilities.",
    "awsService": "SQS"
  },
  {
    "id": "q163",
    "questionText": "A company wants to monitor application performance and identify bottlenecks in their microservices architecture. Which service provides distributed tracing?",
    "options": [
      {
        "text": "Amazon CloudWatch",
        "isCorrect": false
      },
      {
        "text": "AWS CloudTrail",
        "isCorrect": false
      },
      {
        "text": "AWS X-Ray",
        "isCorrect": true
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      }
    ],
    "explanation": "AWS X-Ray helps developers analyze and debug distributed applications by providing distributed tracing capabilities to identify performance bottlenecks and errors across microservices.",
    "awsService": "X-Ray"
  },
  {
    "id": "q164",
    "questionText": "A company needs to ensure their EC2 instances are patched and compliant with security standards. Which service can automate this process?",
    "options": [
      {
        "text": "AWS Config",
        "isCorrect": false
      },
      {
        "text": "AWS Systems Manager",
        "isCorrect": true
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      },
      {
        "text": "AWS Security Hub",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Systems Manager provides patch management capabilities that can automatically patch EC2 instances and maintain compliance with security standards through its Patch Manager feature.",
    "awsService": "Systems Manager"
  },
  {
    "id": "q165",
    "questionText": "An IoT application generates millions of events per second that need to be processed in real-time. Which service is best suited for this use case?",
    "options": [
      {
        "text": "Amazon SQS",
        "isCorrect": false
      },
      {
        "text": "Amazon SNS",
        "isCorrect": false
      },
      {
        "text": "Amazon Kinesis Data Streams",
        "isCorrect": true
      },
      {
        "text": "AWS Lambda",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Kinesis Data Streams is designed for real-time data streaming and can handle millions of events per second, making it ideal for high-volume IoT data ingestion and processing.",
    "awsService": "Kinesis"
  },
  {
    "id": "q166",
    "questionText": "A web application running on EC2 needs to access S3 buckets securely without storing credentials in the application code. What is the best approach?",
    "options": [
      {
        "text": "Store credentials in environment variables",
        "isCorrect": false
      },
      {
        "text": "Use IAM roles for EC2",
        "isCorrect": true
      },
      {
        "text": "Use AWS Secrets Manager",
        "isCorrect": false
      },
      {
        "text": "Hard-code credentials in the application",
        "isCorrect": false
      }
    ],
    "explanation": "IAM roles for EC2 provide temporary credentials that are automatically rotated, eliminating the need to store long-term credentials in your application or EC2 instance.",
    "awsService": "IAM"
  },
  {
    "id": "q167",
    "questionText": "A company wants to implement disaster recovery with an RTO of 1 hour and RPO of 1 hour. Which DR strategy would be most appropriate?",
    "options": [
      {
        "text": "Backup and Restore",
        "isCorrect": false
      },
      {
        "text": "Pilot Light",
        "isCorrect": false
      },
      {
        "text": "Warm Standby",
        "isCorrect": true
      },
      {
        "text": "Multi-Site Active/Active",
        "isCorrect": false
      }
    ],
    "explanation": "Warm Standby maintains a scaled-down version of your production environment that can be quickly scaled up during a disaster, meeting the 1-hour RTO/RPO requirements cost-effectively.",
    "awsService": "General"
  },
  {
    "id": "q168",
    "questionText": "An application needs to store session data that expires after 30 minutes. Which service would be most appropriate?",
    "options": [
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": true
      },
      {
        "text": "Amazon S3",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon ElastiCache (Redis or Memcached) is perfect for session storage as it provides sub-millisecond latency, automatic expiration (TTL), and is designed for temporary data caching.",
    "awsService": "ElastiCache"
  },
  {
    "id": "q169",
    "questionText": "A company needs to run analytics on their S3 data without moving it to another service. Which AWS service allows querying S3 data directly?",
    "options": [
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      },
      {
        "text": "Amazon Athena",
        "isCorrect": true
      },
      {
        "text": "Amazon EMR",
        "isCorrect": false
      },
      {
        "text": "AWS Glue",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL without needing to move or transform the data.",
    "awsService": "Athena"
  },
  {
    "id": "q170",
    "questionText": "A mobile application needs to authenticate users and provide temporary AWS credentials. Which service should be used?",
    "options": [
      {
        "text": "Amazon Cognito",
        "isCorrect": true
      },
      {
        "text": "AWS IAM",
        "isCorrect": false
      },
      {
        "text": "AWS STS",
        "isCorrect": false
      },
      {
        "text": "AWS Directory Service",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Cognito provides authentication, authorization, and user management for web and mobile apps, and can provide temporary AWS credentials to authenticated users through identity pools.",
    "awsService": "Cognito"
  },
  {
    "id": "q171",
    "questionText": "A company has a web application that stores user session data in memory on EC2 instances. The application needs to be highly available across multiple AZs. What is the best solution for session storage?",
    "options": [
      {
        "text": "Store sessions in local EC2 instance memory",
        "isCorrect": false
      },
      {
        "text": "Use Amazon ElastiCache for Redis",
        "isCorrect": true
      },
      {
        "text": "Store sessions in Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Use Amazon RDS for session storage",
        "isCorrect": false
      }
    ],
    "explanation": "ElastiCache for Redis provides high availability, automatic failover, and sub-millisecond latency perfect for session storage across multiple AZs.",
    "awsService": "ElastiCache"
  },
  {
    "id": "q172",
    "questionText": "A startup needs to deploy a simple web application quickly without managing servers. The application receives sporadic traffic. Which service combination is most cost-effective?",
    "options": [
      {
        "text": "EC2 + Application Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Lambda + API Gateway",
        "isCorrect": true
      },
      {
        "text": "ECS Fargate + ALB",
        "isCorrect": false
      },
      {
        "text": "Elastic Beanstalk",
        "isCorrect": false
      }
    ],
    "explanation": "Lambda + API Gateway is serverless, automatically scales to zero when not in use, and charges only for actual requests - perfect for sporadic traffic patterns.",
    "awsService": "Lambda"
  },
  {
    "id": "q173",
    "questionText": "A company wants to analyze clickstream data in real-time to detect fraud. The data comes in at 10,000 events per second. Which architecture is most suitable?",
    "options": [
      {
        "text": "S3 + Athena",
        "isCorrect": false
      },
      {
        "text": "Kinesis Data Streams + Lambda",
        "isCorrect": true
      },
      {
        "text": "SQS + EC2",
        "isCorrect": false
      },
      {
        "text": "DynamoDB + CloudWatch",
        "isCorrect": false
      }
    ],
    "explanation": "Kinesis Data Streams can handle high-throughput real-time data ingestion, and Lambda can process events immediately for real-time fraud detection.",
    "awsService": "Kinesis"
  },
  {
    "id": "q174",
    "questionText": "An application needs to store configuration data that is accessed frequently but changes rarely. The data must be encrypted and versioned. Which service is most appropriate?",
    "options": [
      {
        "text": "Amazon S3",
        "isCorrect": false
      },
      {
        "text": "AWS Systems Manager Parameter Store",
        "isCorrect": true
      },
      {
        "text": "AWS Secrets Manager",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      }
    ],
    "explanation": "Parameter Store provides secure storage for configuration data with built-in versioning, encryption via KMS, and high availability with low latency access.",
    "awsService": "Systems Manager"
  },
  {
    "id": "q175",
    "questionText": "A company needs to ensure their Lambda functions can access RDS databases in a private subnet. What is required?",
    "options": [
      {
        "text": "Configure Lambda with VPC settings",
        "isCorrect": true
      },
      {
        "text": "Make RDS publicly accessible",
        "isCorrect": false
      },
      {
        "text": "Use Lambda@Edge",
        "isCorrect": false
      },
      {
        "text": "Create a VPC endpoint",
        "isCorrect": false
      }
    ],
    "explanation": "Lambda functions must be configured with VPC settings (subnet and security group) to access resources in private subnets like RDS databases.",
    "awsService": "Lambda"
  },
  {
    "id": "q176",
    "questionText": "A media company needs to transcode video files uploaded to S3. The processing takes 30-60 minutes per file. Which compute option is most cost-effective?",
    "options": [
      {
        "text": "AWS Lambda",
        "isCorrect": false
      },
      {
        "text": "EC2 Spot Instances",
        "isCorrect": true
      },
      {
        "text": "EC2 On-Demand Instances",
        "isCorrect": false
      },
      {
        "text": "AWS Batch on Fargate",
        "isCorrect": false
      }
    ],
    "explanation": "EC2 Spot Instances can provide up to 90% cost savings for batch processing workloads that can tolerate interruptions, perfect for video transcoding jobs.",
    "awsService": "EC2"
  },
  {
    "id": "q177",
    "questionText": "A company wants to implement auto-scaling for their containerized application based on custom metrics like queue length. Which service should they use?",
    "options": [
      {
        "text": "EC2 Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "ECS Service Auto Scaling",
        "isCorrect": true
      },
      {
        "text": "Lambda Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "Application Auto Scaling",
        "isCorrect": false
      }
    ],
    "explanation": "ECS Service Auto Scaling can scale containers based on CloudWatch metrics including custom metrics like queue length, providing precise scaling control.",
    "awsService": "ECS"
  },
  {
    "id": "q178",
    "questionText": "An e-commerce site experiences high read traffic during flash sales. The database is MySQL. How can you improve read performance without changing the application code?",
    "options": [
      {
        "text": "Enable Multi-AZ deployment",
        "isCorrect": false
      },
      {
        "text": "Add an ElastiCache layer",
        "isCorrect": true
      },
      {
        "text": "Increase the instance size",
        "isCorrect": false
      },
      {
        "text": "Enable automated backups",
        "isCorrect": false
      }
    ],
    "explanation": "ElastiCache can be used as a read-through cache in front of MySQL, dramatically improving read performance without requiring application code changes.",
    "awsService": "ElastiCache"
  },
  {
    "id": "q179",
    "questionText": "A financial services company needs to ensure all data in transit between their application and S3 is encrypted. What should they implement?",
    "options": [
      {
        "text": "Server-side encryption with S3-managed keys",
        "isCorrect": false
      },
      {
        "text": "HTTPS/TLS for all S3 API calls",
        "isCorrect": true
      },
      {
        "text": "Client-side encryption",
        "isCorrect": false
      },
      {
        "text": "S3 bucket policy for encryption",
        "isCorrect": false
      }
    ],
    "explanation": "HTTPS/TLS encrypts data in transit between the client and S3. This is the standard method for ensuring data is encrypted while being transmitted.",
    "awsService": "S3"
  },
  {
    "id": "q180",
    "questionText": "A company wants to migrate their on-premises Active Directory to AWS while maintaining existing user authentication. Which service should they use?",
    "options": [
      {
        "text": "Amazon Cognito",
        "isCorrect": false
      },
      {
        "text": "AWS Directory Service",
        "isCorrect": true
      },
      {
        "text": "AWS IAM",
        "isCorrect": false
      },
      {
        "text": "AWS SSO",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Directory Service provides managed Microsoft Active Directory in the AWS cloud, allowing seamless migration and integration with existing AD infrastructure.",
    "awsService": "Directory Service"
  },
  {
    "id": "q181",
    "questionText": "An application needs to send email notifications to thousands of users. The emails should be delivered reliably with bounce and complaint tracking. Which service is most appropriate?",
    "options": [
      {
        "text": "Amazon SNS",
        "isCorrect": false
      },
      {
        "text": "Amazon SES",
        "isCorrect": true
      },
      {
        "text": "Amazon SQS",
        "isCorrect": false
      },
      {
        "text": "AWS Lambda",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon SES (Simple Email Service) is designed for bulk email sending with features like bounce and complaint tracking, delivery analytics, and high deliverability.",
    "awsService": "SES"
  },
  {
    "id": "q182",
    "questionText": "A company needs to store structured data with complex queries involving multiple tables and transactions. Which database service is most suitable?",
    "options": [
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      },
      {
        "text": "Amazon RDS",
        "isCorrect": true
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon RDS provides fully managed relational databases that support complex queries, joins across multiple tables, and ACID transactions - perfect for structured data with complex relationships.",
    "awsService": "RDS"
  },
  {
    "id": "q183",
    "questionText": "A mobile gaming company needs to handle sudden spikes in traffic during game launches. Their API can scale from 100 to 10,000 requests per second. Which architecture is most cost-effective?",
    "options": [
      {
        "text": "EC2 instances with Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "API Gateway + Lambda",
        "isCorrect": true
      },
      {
        "text": "ECS with Fargate",
        "isCorrect": false
      },
      {
        "text": "Elastic Load Balancer + EC2",
        "isCorrect": false
      }
    ],
    "explanation": "API Gateway + Lambda provides automatic scaling from zero to thousands of concurrent executions without pre-provisioning, making it perfect for unpredictable traffic spikes.",
    "awsService": "API Gateway"
  },
  {
    "id": "q184",
    "questionText": "A company wants to analyze customer behavior data to recommend products. They need to process terabytes of historical data. Which service is most appropriate?",
    "options": [
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": true
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Redshift is a data warehouse service optimized for analyzing large datasets (terabytes to petabytes) and complex analytical queries perfect for recommendation engines.",
    "awsService": "Redshift"
  },
  {
    "id": "q185",
    "questionText": "An application needs to process images uploaded by users to extract text. The processing should be automatic and scalable. Which service combination is best?",
    "options": [
      {
        "text": "S3 + Lambda + Textract",
        "isCorrect": true
      },
      {
        "text": "EC2 + OpenCV",
        "isCorrect": false
      },
      {
        "text": "ECS + Custom OCR",
        "isCorrect": false
      },
      {
        "text": "SageMaker + Custom Model",
        "isCorrect": false
      }
    ],
    "explanation": "S3 can trigger Lambda functions automatically when images are uploaded, and Lambda can call Amazon Textract to extract text from images - providing a fully managed, scalable solution.",
    "awsService": "Textract"
  },
  {
    "id": "q186",
    "questionText": "A startup wants to implement CI/CD for their web application. They need automated testing and deployment to multiple environments. Which AWS service provides this capability?",
    "options": [
      {
        "text": "AWS CodeBuild only",
        "isCorrect": false
      },
      {
        "text": "AWS CodePipeline",
        "isCorrect": true
      },
      {
        "text": "AWS CodeDeploy only",
        "isCorrect": false
      },
      {
        "text": "AWS CodeCommit only",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CodePipeline orchestrates the entire CI/CD workflow, integrating with CodeBuild for testing and CodeDeploy for deployment across multiple environments.",
    "awsService": "CodePipeline"
  },
  {
    "id": "q187",
    "questionText": "A company needs to ensure their EC2 instances are compliant with security standards and automatically remediate violations. Which service should they use?",
    "options": [
      {
        "text": "AWS CloudTrail",
        "isCorrect": false
      },
      {
        "text": "AWS Config",
        "isCorrect": true
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      },
      {
        "text": "AWS Security Hub",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Config can monitor resource configurations against compliance rules and trigger automatic remediation actions when violations are detected.",
    "awsService": "Config"
  },
  {
    "id": "q188",
    "questionText": "A data science team needs to train machine learning models using GPU instances. The training jobs run for several hours and can be interrupted. Which compute option is most cost-effective?",
    "options": [
      {
        "text": "EC2 On-Demand GPU instances",
        "isCorrect": false
      },
      {
        "text": "EC2 Spot GPU instances",
        "isCorrect": true
      },
      {
        "text": "SageMaker training jobs",
        "isCorrect": false
      },
      {
        "text": "Lambda with high memory",
        "isCorrect": false
      }
    ],
    "explanation": "EC2 Spot instances can provide up to 90% cost savings for interruptible workloads. GPU training jobs that can handle interruptions benefit significantly from Spot pricing.",
    "awsService": "EC2"
  },
  {
    "id": "q189",
    "questionText": "A company needs to connect their on-premises data center to AWS with a dedicated, high-bandwidth connection for hybrid cloud architecture. Which service should they use?",
    "options": [
      {
        "text": "VPN Gateway",
        "isCorrect": false
      },
      {
        "text": "AWS Direct Connect",
        "isCorrect": true
      },
      {
        "text": "Internet Gateway",
        "isCorrect": false
      },
      {
        "text": "NAT Gateway",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Direct Connect provides a dedicated network connection from on-premises to AWS, offering higher bandwidth, lower latency, and more consistent network experience than VPN.",
    "awsService": "Direct Connect"
  },
  {
    "id": "q190",
    "questionText": "An application generates log files that need to be analyzed for security threats. The analysis should happen in real-time. Which architecture is most suitable?",
    "options": [
      {
        "text": "S3 + Athena",
        "isCorrect": false
      },
      {
        "text": "CloudWatch Logs + Lambda",
        "isCorrect": true
      },
      {
        "text": "ELK Stack on EC2",
        "isCorrect": false
      },
      {
        "text": "Kinesis Data Firehose + S3",
        "isCorrect": false
      }
    ],
    "explanation": "CloudWatch Logs can stream log data to Lambda in real-time, allowing immediate analysis and response to security threats as they appear in the logs.",
    "awsService": "CloudWatch"
  },
  {
    "id": "q191",
    "questionText": "A company wants to implement blue-green deployments for their containerized application running on ECS. What is the best approach?",
    "options": [
      {
        "text": "Use ECS rolling updates",
        "isCorrect": false
      },
      {
        "text": "Use CodeDeploy with ECS",
        "isCorrect": true
      },
      {
        "text": "Manually switch load balancer targets",
        "isCorrect": false
      },
      {
        "text": "Use Auto Scaling groups",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CodeDeploy supports blue-green deployments for ECS services, automatically managing the deployment process and traffic shifting between environments.",
    "awsService": "CodeDeploy"
  },
  {
    "id": "q192",
    "questionText": "A gaming company needs to store player profiles with flexible schema and fast read/write performance for millions of users. Which database is most suitable?",
    "options": [
      {
        "text": "Amazon RDS MySQL",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": true
      },
      {
        "text": "Amazon Aurora",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "DynamoDB provides single-digit millisecond performance at any scale, flexible NoSQL schema perfect for varying player profile structures, and seamless scaling for millions of users.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q193",
    "questionText": "A company needs to ensure their S3 data is replicated to another region for disaster recovery. What should they implement?",
    "options": [
      {
        "text": "S3 Versioning",
        "isCorrect": false
      },
      {
        "text": "S3 Cross-Region Replication",
        "isCorrect": true
      },
      {
        "text": "S3 Transfer Acceleration",
        "isCorrect": false
      },
      {
        "text": "S3 Lifecycle policies",
        "isCorrect": false
      }
    ],
    "explanation": "S3 Cross-Region Replication automatically replicates objects to a bucket in another AWS region, providing geographic redundancy for disaster recovery.",
    "awsService": "S3"
  },
  {
    "id": "q194",
    "questionText": "An IoT application collects sensor data from thousands of devices. The data needs to be processed and stored for analytics. Which architecture handles this best?",
    "options": [
      {
        "text": "API Gateway + Lambda + RDS",
        "isCorrect": false
      },
      {
        "text": "IoT Core + Kinesis + S3",
        "isCorrect": true
      },
      {
        "text": "ALB + EC2 + DynamoDB",
        "isCorrect": false
      },
      {
        "text": "CloudFront + S3 + Athena",
        "isCorrect": false
      }
    ],
    "explanation": "IoT Core securely connects devices, Kinesis handles high-throughput data streaming from thousands of devices, and S3 provides cost-effective storage for analytics.",
    "awsService": "IoT Core"
  },
  {
    "id": "q195",
    "questionText": "A company wants to automate the deployment of infrastructure using code. Which AWS service provides this capability?",
    "options": [
      {
        "text": "AWS Systems Manager",
        "isCorrect": false
      },
      {
        "text": "AWS CloudFormation",
        "isCorrect": true
      },
      {
        "text": "AWS Config",
        "isCorrect": false
      },
      {
        "text": "AWS OpsWorks",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CloudFormation allows you to define infrastructure as code using JSON or YAML templates, enabling automated, repeatable, and version-controlled infrastructure deployments.",
    "awsService": "CloudFormation"
  },
  {
    "id": "q196",
    "questionText": "A web application experiences slow database queries during peak hours. The database CPU and memory utilization are normal. What is the most likely cause and solution?",
    "options": [
      {
        "text": "Increase instance size",
        "isCorrect": false
      },
      {
        "text": "Optimize database queries and add indexes",
        "isCorrect": true
      },
      {
        "text": "Enable Multi-AZ deployment",
        "isCorrect": false
      },
      {
        "text": "Add read replicas",
        "isCorrect": false
      }
    ],
    "explanation": "If CPU and memory are normal but queries are slow, the issue is likely inefficient queries or missing indexes. Query optimization and proper indexing will improve performance.",
    "awsService": "RDS"
  },
  {
    "id": "q197",
    "questionText": "A company needs to process batch jobs that can run for up to 2 hours and require specific GPU configurations. Which service is most appropriate?",
    "options": [
      {
        "text": "AWS Lambda",
        "isCorrect": false
      },
      {
        "text": "AWS Batch",
        "isCorrect": true
      },
      {
        "text": "Amazon ECS",
        "isCorrect": false
      },
      {
        "text": "EC2 Spot Instances",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Batch is specifically designed for batch computing workloads, automatically managing the underlying compute resources and job queues, with support for GPU instances.",
    "awsService": "Batch"
  },
  {
    "id": "q198",
    "questionText": "A media streaming company needs to deliver content globally with low latency and DDoS protection. Which service combination provides this?",
    "options": [
      {
        "text": "S3 + Route 53",
        "isCorrect": false
      },
      {
        "text": "CloudFront + AWS Shield",
        "isCorrect": true
      },
      {
        "text": "ELB + WAF",
        "isCorrect": false
      },
      {
        "text": "Global Accelerator + GuardDuty",
        "isCorrect": false
      }
    ],
    "explanation": "CloudFront provides global content delivery with low latency through edge locations, while AWS Shield provides DDoS protection for the CloudFront distribution.",
    "awsService": "CloudFront"
  },
  {
    "id": "q199",
    "questionText": "A company wants to analyze customer reviews to determine sentiment. They have thousands of reviews to process. Which AI service should they use?",
    "options": [
      {
        "text": "Amazon Textract",
        "isCorrect": false
      },
      {
        "text": "Amazon Comprehend",
        "isCorrect": true
      },
      {
        "text": "Amazon Rekognition",
        "isCorrect": false
      },
      {
        "text": "Amazon Translate",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Comprehend is a natural language processing service that can analyze text for sentiment, entities, key phrases, and topics - perfect for review analysis.",
    "awsService": "Comprehend"
  },
  {
    "id": "q200",
    "questionText": "A development team needs to securely store database passwords and API keys that their applications can retrieve at runtime. Which service is most appropriate?",
    "options": [
      {
        "text": "AWS Systems Manager Parameter Store",
        "isCorrect": false
      },
      {
        "text": "AWS Secrets Manager",
        "isCorrect": true
      },
      {
        "text": "AWS KMS",
        "isCorrect": false
      },
      {
        "text": "S3 with encryption",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Secrets Manager is specifically designed for storing and retrieving sensitive information like database credentials and API keys, with automatic rotation capabilities.",
    "awsService": "Secrets Manager"
  },
  {
    "id": "q201",
    "questionText": "A company runs a critical application that requires 99.99% availability. The RDS database must be highly available across multiple regions. What is the best architecture?",
    "options": [
      {
        "text": "RDS Multi-AZ in one region",
        "isCorrect": false
      },
      {
        "text": "RDS with read replicas in multiple regions",
        "isCorrect": true
      },
      {
        "text": "RDS with automated backups",
        "isCorrect": false
      },
      {
        "text": "Single RDS instance with EBS snapshots",
        "isCorrect": false
      }
    ],
    "explanation": "RDS read replicas in multiple regions provide cross-region disaster recovery and can be promoted to master in case of regional failure, achieving the highest availability.",
    "awsService": "RDS"
  },
  {
    "id": "q202",
    "questionText": "A startup needs to build a REST API that can handle unpredictable traffic spikes with minimal operational overhead. Which solution is most cost-effective?",
    "options": [
      {
        "text": "EC2 Auto Scaling + ALB",
        "isCorrect": false
      },
      {
        "text": "API Gateway + Lambda",
        "isCorrect": true
      },
      {
        "text": "ECS Fargate + ALB",
        "isCorrect": false
      },
      {
        "text": "Elastic Beanstalk",
        "isCorrect": false
      }
    ],
    "explanation": "API Gateway + Lambda provides serverless REST API with automatic scaling, pay-per-request pricing, and zero operational overhead - perfect for startups with unpredictable traffic.",
    "awsService": "API Gateway"
  },
  {
    "id": "q203",
    "questionText": "A data analytics company processes large CSV files (100GB each) stored in S3. The processing takes 2-3 hours per file. What is the most cost-effective compute option?",
    "options": [
      {
        "text": "Lambda functions",
        "isCorrect": false
      },
      {
        "text": "EC2 Spot Instances",
        "isCorrect": true
      },
      {
        "text": "EC2 Reserved Instances",
        "isCorrect": false
      },
      {
        "text": "Fargate",
        "isCorrect": false
      }
    ],
    "explanation": "EC2 Spot Instances provide up to 90% cost savings for batch processing workloads that can tolerate interruptions, perfect for long-running data processing jobs.",
    "awsService": "EC2"
  },
  {
    "id": "q204",
    "questionText": "A mobile app needs to sync user data across devices in real-time. The data should be available offline and sync when connectivity is restored. Which service is most appropriate?",
    "options": [
      {
        "text": "DynamoDB with DynamoDB Streams",
        "isCorrect": false
      },
      {
        "text": "AppSync with offline capabilities",
        "isCorrect": true
      },
      {
        "text": "Cognito Sync",
        "isCorrect": false
      },
      {
        "text": "S3 with Transfer Acceleration",
        "isCorrect": false
      }
    ],
    "explanation": "AWS AppSync provides real-time data synchronization, offline capabilities, and automatic conflict resolution for mobile and web applications.",
    "awsService": "AppSync"
  },
  {
    "id": "q205",
    "questionText": "A company needs to ensure their EC2 instances are patched monthly with zero downtime. What is the best approach?",
    "options": [
      {
        "text": "Manual patching during maintenance windows",
        "isCorrect": false
      },
      {
        "text": "Systems Manager Patch Manager with blue-green deployment",
        "isCorrect": true
      },
      {
        "text": "Auto Scaling with instance refresh",
        "isCorrect": false
      },
      {
        "text": "Lambda function to restart instances",
        "isCorrect": false
      }
    ],
    "explanation": "Systems Manager Patch Manager can automate patching, and when combined with blue-green deployment strategies, can achieve zero-downtime patching.",
    "awsService": "Systems Manager"
  },
  {
    "id": "q206",
    "questionText": "An e-commerce website experiences 10x traffic increase during Black Friday. The application uses session-based authentication. How should sessions be managed?",
    "options": [
      {
        "text": "Store sessions in EC2 local storage",
        "isCorrect": false
      },
      {
        "text": "Use ElastiCache for session storage",
        "isCorrect": true
      },
      {
        "text": "Store sessions in RDS",
        "isCorrect": false
      },
      {
        "text": "Use EBS volumes for sessions",
        "isCorrect": false
      }
    ],
    "explanation": "ElastiCache provides high-performance, distributed session storage that can scale with traffic and provides sub-millisecond latency for session retrieval.",
    "awsService": "ElastiCache"
  },
  {
    "id": "q207",
    "questionText": "A financial institution needs to encrypt all data at rest and in transit with customer-managed encryption keys. Which combination satisfies this requirement?",
    "options": [
      {
        "text": "KMS with AWS-managed keys + TLS",
        "isCorrect": false
      },
      {
        "text": "KMS with customer-managed CMKs + TLS",
        "isCorrect": true
      },
      {
        "text": "S3 server-side encryption + HTTPS",
        "isCorrect": false
      },
      {
        "text": "CloudHSM + VPN",
        "isCorrect": false
      }
    ],
    "explanation": "KMS with customer-managed CMKs provides full control over encryption keys for data at rest, while TLS ensures data in transit encryption.",
    "awsService": "KMS"
  },
  {
    "id": "q208",
    "questionText": "A gaming company needs to store player session data that expires after 24 hours. The data must be highly available and have sub-millisecond latency. Which service is best?",
    "options": [
      {
        "text": "DynamoDB with TTL",
        "isCorrect": true
      },
      {
        "text": "RDS with scheduled cleanup",
        "isCorrect": false
      },
      {
        "text": "ElastiCache",
        "isCorrect": false
      },
      {
        "text": "S3 with lifecycle policies",
        "isCorrect": false
      }
    ],
    "explanation": "DynamoDB provides single-digit millisecond latency, built-in TTL (Time To Live) for automatic expiration, and high availability across multiple AZs.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q209",
    "questionText": "A company wants to run Docker containers without managing the underlying infrastructure. They need persistent storage for databases. Which service combination is appropriate?",
    "options": [
      {
        "text": "ECS on EC2 with EBS",
        "isCorrect": false
      },
      {
        "text": "Fargate with EFS",
        "isCorrect": true
      },
      {
        "text": "Lambda with S3",
        "isCorrect": false
      },
      {
        "text": "EKS with instance store",
        "isCorrect": false
      }
    ],
    "explanation": "Fargate provides serverless container hosting, and EFS provides persistent, shared file storage that can be mounted by Fargate containers.",
    "awsService": "Fargate"
  },
  {
    "id": "q210",
    "questionText": "A media company needs to transcode video files uploaded to S3. The transcoding process is triggered by file uploads and takes 10-45 minutes. What architecture is most suitable?",
    "options": [
      {
        "text": "S3 â†’ Lambda â†’ MediaConvert",
        "isCorrect": false
      },
      {
        "text": "S3 â†’ SQS â†’ EC2 â†’ MediaConvert",
        "isCorrect": true
      },
      {
        "text": "S3 â†’ SNS â†’ Lambda â†’ MediaConvert",
        "isCorrect": false
      },
      {
        "text": "S3 â†’ Step Functions â†’ MediaConvert",
        "isCorrect": false
      }
    ],
    "explanation": "Lambda has a 15-minute execution limit. S3 can trigger SQS messages, which EC2 instances can process to handle long-running MediaConvert jobs.",
    "awsService": "MediaConvert"
  },
  {
    "id": "q211",
    "questionText": "A company needs to implement disaster recovery with RPO of 1 hour and RTO of 4 hours. Their current infrastructure is entirely on-premises. What AWS strategy is most cost-effective?",
    "options": [
      {
        "text": "Multi-site active/active",
        "isCorrect": false
      },
      {
        "text": "Warm standby in AWS",
        "isCorrect": false
      },
      {
        "text": "Pilot light in AWS",
        "isCorrect": true
      },
      {
        "text": "Backup and restore in AWS",
        "isCorrect": false
      }
    ],
    "explanation": "Pilot light maintains minimal infrastructure in AWS (databases, key services) that can be quickly scaled up to meet 4-hour RTO while minimizing costs.",
    "awsService": "Disaster Recovery"
  },
  {
    "id": "q212",
    "questionText": "An application needs to process messages in strict order (FIFO) with exactly-once processing. Which service should be used?",
    "options": [
      {
        "text": "Standard SQS queue",
        "isCorrect": false
      },
      {
        "text": "SQS FIFO queue",
        "isCorrect": true
      },
      {
        "text": "SNS topic",
        "isCorrect": false
      },
      {
        "text": "Kinesis Data Streams",
        "isCorrect": false
      }
    ],
    "explanation": "SQS FIFO queues guarantee message ordering and exactly-once processing, perfect for applications requiring strict message sequencing.",
    "awsService": "SQS"
  },
  {
    "id": "q213",
    "questionText": "A machine learning team needs to train models using large datasets stored in S3. Training jobs require GPU instances and can run for several days. What is the most cost-effective solution?",
    "options": [
      {
        "text": "SageMaker training jobs",
        "isCorrect": false
      },
      {
        "text": "EC2 Spot instances with persistent storage",
        "isCorrect": true
      },
      {
        "text": "EC2 On-Demand GPU instances",
        "isCorrect": false
      },
      {
        "text": "Lambda with high memory allocation",
        "isCorrect": false
      }
    ],
    "explanation": "EC2 Spot instances provide significant cost savings for long-running, interruptible workloads like ML training, with EBS for persistent storage.",
    "awsService": "EC2"
  },
  {
    "id": "q214",
    "questionText": "A company wants to analyze clickstream data in real-time to personalize user experience. The data volume is 50,000 events per second. Which architecture is most suitable?",
    "options": [
      {
        "text": "Kinesis Data Streams â†’ Lambda â†’ DynamoDB",
        "isCorrect": true
      },
      {
        "text": "SQS â†’ EC2 â†’ RDS",
        "isCorrect": false
      },
      {
        "text": "S3 â†’ Athena â†’ QuickSight",
        "isCorrect": false
      },
      {
        "text": "SNS â†’ Lambda â†’ ElastiCache",
        "isCorrect": false
      }
    ],
    "explanation": "Kinesis Data Streams handles high-throughput real-time data, Lambda processes events, and DynamoDB provides fast storage for personalization data.",
    "awsService": "Kinesis"
  },
  {
    "id": "q215",
    "questionText": "A SaaS company needs to provide each customer with isolated data storage while sharing the same application code. What is the best approach?",
    "options": [
      {
        "text": "Separate RDS instance per customer",
        "isCorrect": false
      },
      {
        "text": "Multi-tenant DynamoDB with partition keys",
        "isCorrect": true
      },
      {
        "text": "Separate S3 bucket per customer",
        "isCorrect": false
      },
      {
        "text": "Shared RDS with row-level security",
        "isCorrect": false
      }
    ],
    "explanation": "DynamoDB with customer-specific partition keys provides logical data isolation in a multi-tenant architecture while sharing infrastructure costs.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q216",
    "questionText": "A company needs to ensure their web application can handle a sudden increase from 1,000 to 100,000 concurrent users. The application is stateless. What architecture provides this scalability?",
    "options": [
      {
        "text": "Single large EC2 instance",
        "isCorrect": false
      },
      {
        "text": "Auto Scaling Group with ALB",
        "isCorrect": true
      },
      {
        "text": "Reserved EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Elastic Beanstalk single instance",
        "isCorrect": false
      }
    ],
    "explanation": "Auto Scaling Groups automatically add/remove instances based on demand, while ALB distributes traffic across instances, providing the needed scalability.",
    "awsService": "Auto Scaling"
  },
  {
    "id": "q217",
    "questionText": "A financial services company needs to ensure all API calls are logged and monitored for compliance. Which combination of services provides comprehensive auditing?",
    "options": [
      {
        "text": "CloudWatch Logs only",
        "isCorrect": false
      },
      {
        "text": "CloudTrail + CloudWatch + Config",
        "isCorrect": true
      },
      {
        "text": "VPC Flow Logs only",
        "isCorrect": false
      },
      {
        "text": "GuardDuty only",
        "isCorrect": false
      }
    ],
    "explanation": "CloudTrail logs API calls, CloudWatch provides monitoring and alerting, and Config tracks resource configuration changes for comprehensive compliance auditing.",
    "awsService": "CloudTrail"
  },
  {
    "id": "q218",
    "questionText": "An IoT application collects telemetry data from vehicles. The data must be processed within 1 second of arrival to detect emergencies. Which service combination is most appropriate?",
    "options": [
      {
        "text": "IoT Core â†’ Kinesis Data Streams â†’ Lambda",
        "isCorrect": true
      },
      {
        "text": "IoT Core â†’ S3 â†’ Athena",
        "isCorrect": false
      },
      {
        "text": "IoT Core â†’ SQS â†’ EC2",
        "isCorrect": false
      },
      {
        "text": "IoT Core â†’ DynamoDB â†’ Lambda",
        "isCorrect": false
      }
    ],
    "explanation": "IoT Core ingests data, Kinesis Data Streams provides real-time processing, and Lambda can process events within seconds for emergency detection.",
    "awsService": "IoT Core"
  },
  {
    "id": "q219",
    "questionText": "A company wants to migrate their on-premises PostgreSQL database to AWS with minimal downtime. The database is 2TB and actively used. What is the best migration approach?",
    "options": [
      {
        "text": "Export/import using pg_dump",
        "isCorrect": false
      },
      {
        "text": "AWS Database Migration Service (DMS)",
        "isCorrect": true
      },
      {
        "text": "Manual replication setup",
        "isCorrect": false
      },
      {
        "text": "Stop application and copy data",
        "isCorrect": false
      }
    ],
    "explanation": "AWS DMS provides continuous data replication during migration, allowing minimal downtime by keeping source and target databases in sync during the migration process.",
    "awsService": "DMS"
  },
  {
    "id": "q220",
    "questionText": "A mobile app backend needs to send push notifications to millions of devices across iOS and Android platforms. Which service should be used?",
    "options": [
      {
        "text": "SES for email notifications",
        "isCorrect": false
      },
      {
        "text": "SNS for mobile push notifications",
        "isCorrect": true
      },
      {
        "text": "SQS for message queuing",
        "isCorrect": false
      },
      {
        "text": "Lambda for custom notifications",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon SNS supports mobile push notifications for iOS, Android, and other platforms, handling the complexity of different notification services.",
    "awsService": "SNS"
  },
  {
    "id": "q221",
    "questionText": "A data warehouse needs to store 100TB of historical sales data for analytics. Queries are run monthly for reporting. Which storage solution is most cost-effective?",
    "options": [
      {
        "text": "RDS with large instances",
        "isCorrect": false
      },
      {
        "text": "Redshift with Reserved Instances",
        "isCorrect": true
      },
      {
        "text": "DynamoDB with provisioned capacity",
        "isCorrect": false
      },
      {
        "text": "S3 with frequent access tier",
        "isCorrect": false
      }
    ],
    "explanation": "Redshift is designed for data warehousing and analytics on large datasets. Reserved Instances provide significant cost savings for predictable workloads like monthly reporting.",
    "awsService": "Redshift"
  },
  {
    "id": "q222",
    "questionText": "A company runs microservices that need to communicate asynchronously. Each service should be able to publish and subscribe to multiple message types. What is the best messaging pattern?",
    "options": [
      {
        "text": "Point-to-point with SQS",
        "isCorrect": false
      },
      {
        "text": "Publish-subscribe with SNS and SQS",
        "isCorrect": true
      },
      {
        "text": "Direct service-to-service calls",
        "isCorrect": false
      },
      {
        "text": "Database-based messaging",
        "isCorrect": false
      }
    ],
    "explanation": "SNS provides publish-subscribe messaging where services can publish to topics, and multiple SQS queues can subscribe to receive relevant messages for each microservice.",
    "awsService": "SNS"
  },
  {
    "id": "q223",
    "questionText": "A web application needs to store user-uploaded images with automatic image resizing and thumbnail generation. What architecture is most efficient?",
    "options": [
      {
        "text": "Upload to EC2, process images, store in S3",
        "isCorrect": false
      },
      {
        "text": "S3 â†’ Lambda â†’ S3 for processed images",
        "isCorrect": true
      },
      {
        "text": "CloudFront â†’ Lambda@Edge for processing",
        "isCorrect": false
      },
      {
        "text": "API Gateway â†’ EC2 â†’ S3",
        "isCorrect": false
      }
    ],
    "explanation": "S3 can trigger Lambda functions automatically when images are uploaded, Lambda can process/resize images, and store results back to S3 - providing a serverless, scalable solution.",
    "awsService": "Lambda"
  },
  {
    "id": "q224",
    "questionText": "A company needs to ensure their application deployment can be rolled back quickly if issues are detected. They use containerized applications. What deployment strategy is best?",
    "options": [
      {
        "text": "Rolling deployment",
        "isCorrect": false
      },
      {
        "text": "Blue-green deployment",
        "isCorrect": true
      },
      {
        "text": "In-place deployment",
        "isCorrect": false
      },
      {
        "text": "Canary deployment",
        "isCorrect": false
      }
    ],
    "explanation": "Blue-green deployment maintains two identical environments, allowing instant rollback by switching traffic back to the previous (blue) environment if issues occur.",
    "awsService": "CodeDeploy"
  },
  {
    "id": "q225",
    "questionText": "A gaming company needs to store player profiles that can be accessed globally with single-digit millisecond latency. The data is frequently updated. Which solution is most appropriate?",
    "options": [
      {
        "text": "RDS with read replicas in multiple regions",
        "isCorrect": false
      },
      {
        "text": "DynamoDB Global Tables",
        "isCorrect": true
      },
      {
        "text": "ElastiCache clusters in multiple regions",
        "isCorrect": false
      },
      {
        "text": "S3 with CloudFront",
        "isCorrect": false
      }
    ],
    "explanation": "DynamoDB Global Tables provide single-digit millisecond latency globally with multi-master replication, perfect for frequently updated gaming data.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q226",
    "questionText": "A company wants to analyze their AWS costs and identify opportunities for optimization. Which service provides detailed cost analysis and recommendations?",
    "options": [
      {
        "text": "CloudWatch Billing Alerts",
        "isCorrect": false
      },
      {
        "text": "AWS Cost Explorer with recommendations",
        "isCorrect": true
      },
      {
        "text": "AWS Budgets",
        "isCorrect": false
      },
      {
        "text": "Trusted Advisor",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Cost Explorer provides detailed cost analysis, usage patterns, and cost optimization recommendations to help reduce AWS spending.",
    "awsService": "Cost Management"
  },
  {
    "id": "q227",
    "questionText": "A healthcare application needs to ensure HIPAA compliance for storing patient data. Which combination provides the necessary security controls?",
    "options": [
      {
        "text": "S3 with default encryption",
        "isCorrect": false
      },
      {
        "text": "S3 with KMS encryption + CloudTrail + VPC",
        "isCorrect": true
      },
      {
        "text": "RDS with password authentication",
        "isCorrect": false
      },
      {
        "text": "EC2 with local storage",
        "isCorrect": false
      }
    ],
    "explanation": "HIPAA compliance requires encryption (KMS), audit logging (CloudTrail), and network isolation (VPC) to protect sensitive healthcare data.",
    "awsService": "Security"
  },
  {
    "id": "q228",
    "questionText": "A company needs to process large amounts of streaming data and run complex analytics in real-time. The data arrives at 100,000 records per second. Which service is most suitable?",
    "options": [
      {
        "text": "Kinesis Data Streams with Lambda",
        "isCorrect": false
      },
      {
        "text": "Kinesis Data Analytics",
        "isCorrect": true
      },
      {
        "text": "SQS with EC2 processing",
        "isCorrect": false
      },
      {
        "text": "DynamoDB Streams",
        "isCorrect": false
      }
    ],
    "explanation": "Kinesis Data Analytics is specifically designed for real-time analytics on streaming data, using SQL queries to process high-throughput data streams.",
    "awsService": "Kinesis"
  },
  {
    "id": "q229",
    "questionText": "A development team needs to automate infrastructure provisioning across development, staging, and production environments. What is the best approach?",
    "options": [
      {
        "text": "Manual resource creation",
        "isCorrect": false
      },
      {
        "text": "CloudFormation templates",
        "isCorrect": true
      },
      {
        "text": "AWS CLI scripts",
        "isCorrect": false
      },
      {
        "text": "Terraform with local state",
        "isCorrect": false
      }
    ],
    "explanation": "CloudFormation provides Infrastructure as Code with templates that can be version-controlled and deployed consistently across multiple environments.",
    "awsService": "CloudFormation"
  },
  {
    "id": "q230",
    "questionText": "A company wants to implement a data lake for storing structured and unstructured data from multiple sources. Which combination of services is most appropriate?",
    "options": [
      {
        "text": "RDS + DynamoDB + RedShift",
        "isCorrect": false
      },
      {
        "text": "S3 + Glue + Athena + QuickSight",
        "isCorrect": true
      },
      {
        "text": "EC2 + Hadoop + Spark",
        "isCorrect": false
      },
      {
        "text": "Lambda + DynamoDB + CloudWatch",
        "isCorrect": false
      }
    ],
    "explanation": "S3 provides scalable storage for all data types, Glue catalogs and transforms data, Athena queries data, and QuickSight provides visualization - forming a complete data lake solution.",
    "awsService": "S3"
  },
  {
    "id": "q231",
    "questionText": "A mobile application needs to authenticate users through social media accounts (Google, Facebook) and provide temporary AWS credentials. Which service should be used?",
    "options": [
      {
        "text": "IAM users with access keys",
        "isCorrect": false
      },
      {
        "text": "Cognito Identity Pools",
        "isCorrect": true
      },
      {
        "text": "STS assume role directly",
        "isCorrect": false
      },
      {
        "text": "Custom OAuth implementation",
        "isCorrect": false
      }
    ],
    "explanation": "Cognito Identity Pools enable authentication through social identity providers and provide temporary AWS credentials to authenticated users through STS.",
    "awsService": "Cognito"
  },
  {
    "id": "q232",
    "questionText": "A company runs a legacy application that requires a specific version of Java. They want to migrate to AWS without modifying the application. Which service is most suitable?",
    "options": [
      {
        "text": "Lambda functions",
        "isCorrect": false
      },
      {
        "text": "Elastic Beanstalk",
        "isCorrect": true
      },
      {
        "text": "ECS with Fargate",
        "isCorrect": false
      },
      {
        "text": "API Gateway",
        "isCorrect": false
      }
    ],
    "explanation": "Elastic Beanstalk supports multiple Java versions and platforms, allowing easy deployment of legacy applications without code changes while handling infrastructure management.",
    "awsService": "Elastic Beanstalk"
  },
  {
    "id": "q233",
    "questionText": "A company needs to ensure their critical application can survive the failure of an entire AWS region. What architecture should they implement?",
    "options": [
      {
        "text": "Multi-AZ deployment in one region",
        "isCorrect": false
      },
      {
        "text": "Multi-region active-passive setup",
        "isCorrect": true
      },
      {
        "text": "Single region with backups",
        "isCorrect": false
      },
      {
        "text": "Auto Scaling across AZs",
        "isCorrect": false
      }
    ],
    "explanation": "Multi-region active-passive setup ensures application availability even if an entire region fails, with the passive region ready to take over operations.",
    "awsService": "Global Infrastructure"
  },
  {
    "id": "q234",
    "questionText": "A data science team needs to collaborate on Jupyter notebooks with shared datasets and computing resources. Which service provides this capability?",
    "options": [
      {
        "text": "EC2 with Jupyter installed",
        "isCorrect": false
      },
      {
        "text": "SageMaker Studio",
        "isCorrect": true
      },
      {
        "text": "Lambda with custom runtime",
        "isCorrect": false
      },
      {
        "text": "EMR with Zeppelin",
        "isCorrect": false
      }
    ],
    "explanation": "SageMaker Studio provides a collaborative environment for data scientists with shared Jupyter notebooks, datasets, and managed computing resources.",
    "awsService": "SageMaker"
  },
  {
    "id": "q235",
    "questionText": "A company needs to securely transfer 50TB of data from their on-premises data center to S3. The internet connection is slow and unreliable. What is the best solution?",
    "options": [
      {
        "text": "Direct upload over internet",
        "isCorrect": false
      },
      {
        "text": "AWS Snowball Edge",
        "isCorrect": true
      },
      {
        "text": "AWS DataSync over internet",
        "isCorrect": false
      },
      {
        "text": "S3 Transfer Acceleration",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Snowball Edge is designed for transferring large amounts of data (up to 100TB) when internet connectivity is limited or unreliable.",
    "awsService": "Snowball"
  },
  {
    "id": "q236",
    "questionText": "A company wants to implement automated security scanning of their EC2 instances for vulnerabilities and compliance violations. Which service should they use?",
    "options": [
      {
        "text": "AWS Config",
        "isCorrect": false
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": true
      },
      {
        "text": "AWS CloudTrail",
        "isCorrect": false
      },
      {
        "text": "AWS Security Hub",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Inspector automatically assesses EC2 instances for security vulnerabilities and provides detailed findings with remediation guidance.",
    "awsService": "Inspector"
  },
  {
    "id": "q237",
    "questionText": "A real-time gaming application needs to maintain WebSocket connections for thousands of concurrent players. Which service combination is most appropriate?",
    "options": [
      {
        "text": "CloudFront + S3",
        "isCorrect": false
      },
      {
        "text": "API Gateway WebSocket + Lambda",
        "isCorrect": true
      },
      {
        "text": "ELB + EC2",
        "isCorrect": false
      },
      {
        "text": "Route 53 + ALB",
        "isCorrect": false
      }
    ],
    "explanation": "API Gateway WebSocket APIs can handle persistent connections and route messages to Lambda functions for real-time, serverless gaming applications.",
    "awsService": "API Gateway"
  },
  {
    "id": "q238",
    "questionText": "A company needs to ensure their application logs are retained for 7 years for compliance. The logs are rarely accessed after 30 days. What is the most cost-effective storage strategy?",
    "options": [
      {
        "text": "CloudWatch Logs with default retention",
        "isCorrect": false
      },
      {
        "text": "S3 with lifecycle policies to Glacier Deep Archive",
        "isCorrect": true
      },
      {
        "text": "EBS snapshots",
        "isCorrect": false
      },
      {
        "text": "RDS with automated backups",
        "isCorrect": false
      }
    ],
    "explanation": "S3 lifecycle policies can automatically transition logs to Glacier Deep Archive after 30 days, providing the lowest cost for long-term retention while maintaining compliance.",
    "awsService": "S3"
  },
  {
    "id": "q239",
    "questionText": "A company wants to implement end-to-end encryption for their web application traffic. Which combination provides complete encryption coverage?",
    "options": [
      {
        "text": "CloudFront + ALB with HTTP",
        "isCorrect": false
      },
      {
        "text": "CloudFront HTTPS + ALB HTTPS + backend HTTPS",
        "isCorrect": true
      },
      {
        "text": "Route 53 + CloudFront HTTP",
        "isCorrect": false
      },
      {
        "text": "WAF + ALB HTTP",
        "isCorrect": false
      }
    ],
    "explanation": "End-to-end encryption requires HTTPS/TLS at every layer: CloudFront to users, ALB to CloudFront, and backend services to ALB.",
    "awsService": "CloudFront"
  },
  {
    "id": "q240",
    "questionText": "A machine learning application needs to process images uploaded by users to detect objects. The processing should be automatic and cost-effective. Which service combination is best?",
    "options": [
      {
        "text": "S3 + Lambda + custom ML model",
        "isCorrect": false
      },
      {
        "text": "S3 + Lambda + Amazon Rekognition",
        "isCorrect": true
      },
      {
        "text": "EC2 + OpenCV + custom processing",
        "isCorrect": false
      },
      {
        "text": "SageMaker + custom endpoint",
        "isCorrect": false
      }
    ],
    "explanation": "S3 triggers Lambda automatically on upload, and Amazon Rekognition provides pre-trained object detection capabilities without the need to build custom ML models.",
    "awsService": "Rekognition"
  },
  {
    "id": "q241",
    "questionText": "A startup needs to deploy a web application with a database. They want minimal operational overhead and automatic scaling. Which combination is most suitable?",
    "options": [
      {
        "text": "EC2 + RDS MySQL",
        "isCorrect": false
      },
      {
        "text": "Amplify + DynamoDB",
        "isCorrect": true
      },
      {
        "text": "ECS + Aurora",
        "isCorrect": false
      },
      {
        "text": "Lambda + RDS",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Amplify provides hosting with automatic scaling and CI/CD, while DynamoDB offers serverless database with automatic scaling and minimal operational overhead.",
    "awsService": "Amplify"
  },
  {
    "id": "q242",
    "questionText": "A company runs batch processing jobs that analyze large datasets. The jobs can be interrupted and restarted. Which compute option provides the best cost optimization?",
    "options": [
      {
        "text": "EC2 Reserved Instances",
        "isCorrect": false
      },
      {
        "text": "EC2 Spot Instances",
        "isCorrect": true
      },
      {
        "text": "EC2 On-Demand Instances",
        "isCorrect": false
      },
      {
        "text": "Fargate Spot",
        "isCorrect": false
      }
    ],
    "explanation": "EC2 Spot Instances provide up to 90% cost savings compared to On-Demand pricing and are perfect for fault-tolerant batch processing jobs that can handle interruptions.",
    "awsService": "EC2"
  },
  {
    "id": "q243",
    "questionText": "A global company needs to serve static content to users worldwide with the lowest possible latency. Which solution is most effective?",
    "options": [
      {
        "text": "S3 with regional replication",
        "isCorrect": false
      },
      {
        "text": "CloudFront with global edge locations",
        "isCorrect": true
      },
      {
        "text": "Multiple ALBs in different regions",
        "isCorrect": false
      },
      {
        "text": "Route 53 with latency-based routing",
        "isCorrect": false
      }
    ],
    "explanation": "CloudFront's global network of edge locations caches content close to users worldwide, providing the lowest latency for static content delivery.",
    "awsService": "CloudFront"
  },
  {
    "id": "q244",
    "questionText": "A company needs to ensure their multi-tier application can handle the failure of any single component. What architecture principle should they follow?",
    "options": [
      {
        "text": "Single point of failure design",
        "isCorrect": false
      },
      {
        "text": "Loosely coupled architecture",
        "isCorrect": true
      },
      {
        "text": "Monolithic application design",
        "isCorrect": false
      },
      {
        "text": "Shared database architecture",
        "isCorrect": false
      }
    ],
    "explanation": "Loosely coupled architecture ensures that components can fail independently without affecting the entire system, using services like SQS, SNS, and Load Balancers.",
    "awsService": "Well-Architected Framework"
  },
  {
    "id": "q245",
    "questionText": "A development team needs to test their application with a copy of production data monthly. What is the most cost-effective approach for the test database?",
    "options": [
      {
        "text": "Maintain a separate production-sized RDS instance",
        "isCorrect": false
      },
      {
        "text": "Create RDS snapshots and restore when needed",
        "isCorrect": true
      },
      {
        "text": "Use DynamoDB with on-demand capacity",
        "isCorrect": false
      },
      {
        "text": "Replicate data to a read replica",
        "isCorrect": false
      }
    ],
    "explanation": "RDS snapshots are cost-effective for periodic testing - you only pay for storage until you restore a snapshot to a running instance for testing.",
    "awsService": "RDS"
  },
  {
    "id": "q246",
    "questionText": "A company wants to monitor their application performance and automatically scale based on custom business metrics like order processing rate. Which service combination enables this?",
    "options": [
      {
        "text": "CloudWatch default metrics + Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "CloudWatch custom metrics + Auto Scaling",
        "isCorrect": true
      },
      {
        "text": "X-Ray + Lambda",
        "isCorrect": false
      },
      {
        "text": "Config + Systems Manager",
        "isCorrect": false
      }
    ],
    "explanation": "CloudWatch custom metrics allow you to send business-specific metrics, and Auto Scaling can use these custom metrics to scale resources based on business requirements.",
    "awsService": "CloudWatch"
  },
  {
    "id": "q247",
    "questionText": "A financial services company needs to ensure all administrative actions in their AWS account are logged and tamper-proof. Which service configuration provides this?",
    "options": [
      {
        "text": "CloudWatch Logs with encryption",
        "isCorrect": false
      },
      {
        "text": "CloudTrail with log file validation and S3 MFA delete",
        "isCorrect": true
      },
      {
        "text": "Config with S3 storage",
        "isCorrect": false
      },
      {
        "text": "VPC Flow Logs",
        "isCorrect": false
      }
    ],
    "explanation": "CloudTrail with log file validation ensures logs haven't been tampered with, and S3 MFA delete prevents unauthorized deletion of log files.",
    "awsService": "CloudTrail"
  },
  {
    "id": "q248",
    "questionText": "A company runs a containerized application that needs to scale rapidly based on incoming requests. They want to minimize cold start times. Which service is most appropriate?",
    "options": [
      {
        "text": "Lambda with container images",
        "isCorrect": false
      },
      {
        "text": "ECS with Fargate Spot",
        "isCorrect": false
      },
      {
        "text": "ECS with Fargate and Application Auto Scaling",
        "isCorrect": true
      },
      {
        "text": "EKS with HPA",
        "isCorrect": false
      }
    ],
    "explanation": "ECS with Fargate provides fast container startup times without managing infrastructure, and Application Auto Scaling can quickly scale based on metrics to minimize response times.",
    "awsService": "ECS"
  },
  {
    "id": "q249",
    "questionText": "A data analytics company needs to query petabytes of data stored in S3 using SQL without moving the data. Which service should they use?",
    "options": [
      {
        "text": "Amazon Redshift Spectrum",
        "isCorrect": true
      },
      {
        "text": "Amazon Athena",
        "isCorrect": false
      },
      {
        "text": "Amazon EMR",
        "isCorrect": false
      },
      {
        "text": "AWS Glue",
        "isCorrect": false
      }
    ],
    "explanation": "While Athena can query S3 data, Redshift Spectrum is specifically designed for querying petabyte-scale data in S3 and can handle much larger datasets more efficiently.",
    "awsService": "Redshift"
  },
  {
    "id": "q250",
    "questionText": "A mobile gaming company needs to implement leaderboards that update in real-time for millions of players globally. Which architecture provides the best performance?",
    "options": [
      {
        "text": "RDS with read replicas",
        "isCorrect": false
      },
      {
        "text": "DynamoDB Global Tables with GSI",
        "isCorrect": true
      },
      {
        "text": "ElastiCache clusters in multiple regions",
        "isCorrect": false
      },
      {
        "text": "S3 with CloudFront",
        "isCorrect": false
      }
    ],
    "explanation": "DynamoDB Global Tables provide multi-region, multi-master replication with Global Secondary Indexes (GSI) perfect for real-time leaderboards accessed globally.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q251",
    "questionText": "A company has a CRM application hosted on an Auto Scaling group of On-Demand EC2 instances with different instance types and sizes. The application is extensively used during office hours from 9 AM to 5 PM. Users are complaining that the performance of the application is slow during the start of the day but then works normally after a couple of hours.\n\n**Question:**\nWhich of the following is the MOST operationally efficient solution?",
    "options": [
      {
        "text": "Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the CPU utilization",
        "isCorrect": false
      },
      {
        "text": "Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the Memory utilization",
        "isCorrect": false
      },
      {
        "text": "Configure a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day",
        "isCorrect": true
      },
      {
        "text": "Configure a Predictive scaling policy for the Auto Scaling group to automatically adjust the number of Amazon EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Since the application has predictable usage patterns (9 AM to 5 PM), scheduled scaling is the most operationally efficient solution. It proactively scales the infrastructure before the demand increases, preventing performance issues.",
    "awsService": "EC2"
  },
  {
    "id": "q252",
    "questionText": "A financial application consists of an Auto Scaling group of Amazon EC2 instances, an Application Load Balancer, and a MySQL RDS instance in Multi-AZ deployment. To protect customers' confidential data, the RDS database must only be accessible using authentication tokens specific to the profile credentials of EC2 instances.\n\n**Question:**\nWhich action should be taken?",
    "options": [
      {
        "text": "Enable the IAM DB Authentication",
        "isCorrect": true
      },
      {
        "text": "Configure SSL in your application to encrypt the database connection to RDS",
        "isCorrect": false
      },
      {
        "text": "Create an IAM Role and assign it to your EC2 instances which will grant exclusive access to your RDS instance",
        "isCorrect": false
      },
      {
        "text": "Use a combination of IAM and STS to enforce restricted access to your RDS instance using a temporary authentication token",
        "isCorrect": false
      }
    ],
    "explanation": "IAM DB Authentication allows you to authenticate to your RDS MySQL and PostgreSQL instances using AWS IAM users and roles, eliminating the need to store database credentials in the application code.",
    "awsService": "RDS"
  },
  {
    "id": "q253",
    "questionText": "A company needs to implement a fanout messaging pattern where a single message should be delivered to multiple SQS queues based on message attributes. What is the most efficient solution?",
    "options": [
      {
        "text": "Use multiple SQS queues with a Lambda function to route messages",
        "isCorrect": false
      },
      {
        "text": "Create an SNS topic with message filtering policy and configure multiple SQS queues to subscribe to the topic",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EventBridge with custom event patterns",
        "isCorrect": false
      },
      {
        "text": "Implement application-level message routing using SQS batch operations",
        "isCorrect": false
      }
    ],
    "explanation": "SNS with message filtering policies allows you to implement fanout messaging efficiently. Each SQS queue can subscribe to the SNS topic with specific filter policies to receive only relevant messages.",
    "awsService": "SNS"
  },
  {
    "id": "q254",
    "questionText": "An application requires a database with read replication latency of less than 1 second across multiple regions. Which database solution should be used?",
    "options": [
      {
        "text": "Amazon RDS with Multi-AZ deployment",
        "isCorrect": false
      },
      {
        "text": "Amazon Aurora with cross-region replicas",
        "isCorrect": true
      },
      {
        "text": "Amazon DynamoDB with Global Tables",
        "isCorrect": false
      },
      {
        "text": "Amazon RDS with Read Replicas in multiple regions",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Aurora with cross-region replicas provides sub-second replication latency across regions, making it ideal for applications requiring fast read access globally.",
    "awsService": "Aurora"
  },
  {
    "id": "q255",
    "questionText": "A company wants to improve application performance by reducing response times from milliseconds to microseconds for a DynamoDB table. What solution should be implemented?",
    "options": [
      {
        "text": "Enable DynamoDB Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB Accelerator (DAX)",
        "isCorrect": true
      },
      {
        "text": "Implement Amazon ElastiCache for Redis",
        "isCorrect": false
      },
      {
        "text": "Use DynamoDB Global Secondary Indexes",
        "isCorrect": false
      }
    ],
    "explanation": "DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to 10x performance improvement, reducing response times from milliseconds to microseconds.",
    "awsService": "DynamoDB"
  },
  {
    "id": "q256",
    "questionText": "A web application uses a Network Load Balancer for thousands of game servers around the world that communicate using UDP protocol. The application needs to route traffic based on the location of users. What routing policy should be used with Route 53?",
    "options": [
      {
        "text": "Latency-based routing",
        "isCorrect": false
      },
      {
        "text": "Geolocation routing policy",
        "isCorrect": true
      },
      {
        "text": "Weighted routing policy",
        "isCorrect": false
      },
      {
        "text": "Failover routing policy",
        "isCorrect": false
      }
    ],
    "explanation": "Geolocation routing policy allows you to route traffic based on the geographic location of users, which is perfect for directing users to the nearest game servers.",
    "awsService": "Route53"
  },
  {
    "id": "q257",
    "questionText": "A company needs to upload a 1 TB file to an S3 bucket efficiently. What is the recommended approach?",
    "options": [
      {
        "text": "Use the standard S3 PUT operation",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 multipart upload API",
        "isCorrect": true
      },
      {
        "text": "Use AWS DataSync",
        "isCorrect": false
      },
      {
        "text": "Use AWS Storage Gateway",
        "isCorrect": false
      }
    ],
    "explanation": "S3 multipart upload API allows you to upload large objects in parts, enabling parallel uploads, improved throughput, and the ability to resume uploads if any part fails.",
    "awsService": "S3"
  },
  {
    "id": "q258",
    "questionText": "An application needs to retrieve a subset of data from a large CSV file stored in an S3 bucket without downloading the entire file. Which feature should be used?",
    "options": [
      {
        "text": "S3 Transfer Acceleration",
        "isCorrect": false
      },
      {
        "text": "S3 Select operation",
        "isCorrect": true
      },
      {
        "text": "S3 Intelligent Tiering",
        "isCorrect": false
      },
      {
        "text": "S3 Cross-Region Replication",
        "isCorrect": false
      }
    ],
    "explanation": "S3 Select allows you to retrieve only a subset of data from an object by using simple SQL expressions, reducing the amount of data transferred and improving performance.",
    "awsService": "S3"
  },
  {
    "id": "q259",
    "questionText": "A company needs to monitor the memory and disk space utilization of EC2 instances. The default CloudWatch metrics don't include this information. What should be done?",
    "options": [
      {
        "text": "Enable detailed monitoring in CloudWatch",
        "isCorrect": false
      },
      {
        "text": "Install Amazon CloudWatch agent on the instances",
        "isCorrect": true
      },
      {
        "text": "Use AWS Systems Manager",
        "isCorrect": false
      },
      {
        "text": "Configure AWS Config rules",
        "isCorrect": false
      }
    ],
    "explanation": "The CloudWatch agent must be installed on EC2 instances to collect memory, disk, and other system-level metrics that are not available by default.",
    "awsService": "CloudWatch"
  },
  {
    "id": "q260",
    "questionText": "A company wants to set up a DNS failover to a static website hosted in S3. Which Route 53 routing policy should be used?",
    "options": [
      {
        "text": "Weighted routing",
        "isCorrect": false
      },
      {
        "text": "Latency-based routing",
        "isCorrect": false
      },
      {
        "text": "Failover routing to a static S3 website or CloudFront distribution",
        "isCorrect": true
      },
      {
        "text": "Geolocation routing",
        "isCorrect": false
      }
    ],
    "explanation": "Failover routing policy allows you to configure active-passive failover, where Route 53 fails over to a static S3 website or CloudFront distribution when the primary resource becomes unavailable.",
    "awsService": "Route53"
  },
  {
    "id": "q261",
    "questionText": "A company needs to implement automated backup for all EBS volumes with a 90-day retention policy. What is the most efficient solution?",
    "options": [
      {
        "text": "Create manual EBS snapshots using AWS CLI",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Data Lifecycle Manager to automate EBS snapshots",
        "isCorrect": true
      },
      {
        "text": "Use AWS Backup service",
        "isCorrect": false
      },
      {
        "text": "Create Lambda functions to automate snapshot creation",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Data Lifecycle Manager provides a simple way to automate the creation, retention, and deletion of EBS snapshots and AMIs, making it the most efficient solution for automated backups.",
    "awsService": "EBS"
  },
  {
    "id": "q262",
    "questionText": "An application needs to monitor the SwapUtilization metric of EC2 instances. This metric is not available by default in CloudWatch. What should be implemented?",
    "options": [
      {
        "text": "Enable enhanced monitoring",
        "isCorrect": false
      },
      {
        "text": "Install the CloudWatch agent and configure custom metrics",
        "isCorrect": true
      },
      {
        "text": "Use AWS Systems Manager Session Manager",
        "isCorrect": false
      },
      {
        "text": "Configure VPC Flow Logs",
        "isCorrect": false
      }
    ],
    "explanation": "The CloudWatch agent can collect system-level metrics including SwapUtilization, which are not available through the default CloudWatch metrics.",
    "awsService": "CloudWatch"
  },
  {
    "id": "q263",
    "questionText": "A company needs a fully managed ETL (extract, transform, and load) service to process data from various sources. Which AWS service should be used?",
    "options": [
      {
        "text": "Amazon EMR",
        "isCorrect": false
      },
      {
        "text": "AWS Glue",
        "isCorrect": true
      },
      {
        "text": "Amazon Kinesis Data Firehose",
        "isCorrect": false
      },
      {
        "text": "AWS DataSync",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Glue is a fully managed ETL service that makes it easy to prepare and load data for analytics. It provides automatic code generation and serverless execution.",
    "awsService": "Glue"
  },
  {
    "id": "q264",
    "questionText": "A company needs a fully managed, petabyte-scale data warehouse service for analytics workloads. Which service should be used?",
    "options": [
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": true
      },
      {
        "text": "Amazon Aurora",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Redshift is a fully managed, petabyte-scale data warehouse service designed for analytics workloads with high performance and cost efficiency.",
    "awsService": "Redshift"
  },
  {
    "id": "q265",
    "questionText": "A company wants to encrypt EBS volumes that were restored from unencrypted snapshots. What is the correct approach?",
    "options": [
      {
        "text": "Enable encryption directly on the existing volume",
        "isCorrect": false
      },
      {
        "text": "Copy the snapshot and enable encryption with a new CMK while creating the EBS volume",
        "isCorrect": true
      },
      {
        "text": "Use AWS KMS to encrypt the snapshot",
        "isCorrect": false
      },
      {
        "text": "Create a new encrypted volume and copy data manually",
        "isCorrect": false
      }
    ],
    "explanation": "To encrypt a volume from an unencrypted snapshot, you must copy the snapshot with encryption enabled using a Customer Master Key (CMK), then create the volume from the encrypted snapshot copy.",
    "awsService": "EBS"
  },
  {
    "id": "q266",
    "questionText": "A web application needs to limit the maximum number of requests from a single IP address to prevent abuse. Which AWS service should be implemented?",
    "options": [
      {
        "text": "Amazon CloudFront",
        "isCorrect": false
      },
      {
        "text": "AWS WAF with rate-based rules",
        "isCorrect": true
      },
      {
        "text": "Application Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Amazon API Gateway",
        "isCorrect": false
      }
    ],
    "explanation": "AWS WAF allows you to create rate-based rules that can limit the number of requests from individual IP addresses, providing protection against abuse and DDoS attacks.",
    "awsService": "WAF"
  },
  {
    "id": "q267",
    "questionText": "A company needs to ensure that all uploaded objects in an S3 bucket grant the bucket owner full access. What should be configured?",
    "options": [
      {
        "text": "Enable S3 Block Public Access",
        "isCorrect": false
      },
      {
        "text": "Create a bucket policy that requires users to set the object's ACL to bucket-owner-full-control",
        "isCorrect": true
      },
      {
        "text": "Enable S3 Object Lock",
        "isCorrect": false
      },
      {
        "text": "Configure default encryption",
        "isCorrect": false
      }
    ],
    "explanation": "A bucket policy can enforce that all uploaded objects must include the bucket-owner-full-control ACL, ensuring the bucket owner always has full access to uploaded objects.",
    "awsService": "S3"
  },
  {
    "id": "q268",
    "questionText": "A company wants to protect objects in an S3 bucket from accidental deletion or overwrite. What should be implemented?",
    "options": [
      {
        "text": "Enable S3 Cross-Region Replication",
        "isCorrect": false
      },
      {
        "text": "Enable versioning and MFA delete",
        "isCorrect": true
      },
      {
        "text": "Configure S3 Lifecycle policies",
        "isCorrect": false
      },
      {
        "text": "Enable S3 Transfer Acceleration",
        "isCorrect": false
      }
    ],
    "explanation": "S3 versioning keeps multiple versions of objects, and MFA delete provides an additional layer of security by requiring multi-factor authentication to permanently delete object versions.",
    "awsService": "S3"
  },
  {
    "id": "q269",
    "questionText": "A company needs to access resources on both on-premises and AWS using existing Active Directory credentials. What should be implemented?",
    "options": [
      {
        "text": "AWS IAM with LDAP",
        "isCorrect": false
      },
      {
        "text": "Set up SAML 2.0-Based Federation using Microsoft Active Directory Federation Service",
        "isCorrect": true
      },
      {
        "text": "AWS Single Sign-On",
        "isCorrect": false
      },
      {
        "text": "AWS Cognito User Pools",
        "isCorrect": false
      }
    ],
    "explanation": "SAML 2.0-based federation with Microsoft Active Directory Federation Service allows users to access AWS resources using their existing on-premises Active Directory credentials.",
    "awsService": "IAM"
  },
  {
    "id": "q270",
    "questionText": "A web application needs to serve SSL traffic over the same IP address for multiple domains. What should be configured?",
    "options": [
      {
        "text": "Use multiple Elastic IP addresses",
        "isCorrect": false
      },
      {
        "text": "Use AWS Certificate Manager with CloudFront and enable Server Name Indication (SNI)",
        "isCorrect": true
      },
      {
        "text": "Configure multiple Application Load Balancers",
        "isCorrect": false
      },
      {
        "text": "Use Network Load Balancer with multiple certificates",
        "isCorrect": false
      }
    ],
    "explanation": "Server Name Indication (SNI) allows multiple SSL certificates to be served from the same IP address, enabling CloudFront to serve multiple domains with SSL encryption.",
    "awsService": "CloudFront"
  },
  {
    "id": "q271",
    "questionText": "A company needs to control access to several S3 buckets using a VPC endpoint and allow access only to trusted buckets. What should be implemented?",
    "options": [
      {
        "text": "Configure S3 bucket policies",
        "isCorrect": false
      },
      {
        "text": "Create an endpoint policy for trusted S3 buckets",
        "isCorrect": true
      },
      {
        "text": "Use IAM policies",
        "isCorrect": false
      },
      {
        "text": "Configure VPC NACLs",
        "isCorrect": false
      }
    ],
    "explanation": "VPC endpoint policies allow you to control which S3 buckets can be accessed through the VPC endpoint, providing an additional layer of access control for trusted buckets.",
    "awsService": "VPC"
  },
  {
    "id": "q272",
    "questionText": "A company needs to track all configuration changes made to AWS services for compliance purposes. What should be implemented?",
    "options": [
      {
        "text": "Enable AWS CloudTrail",
        "isCorrect": false
      },
      {
        "text": "Set up AWS Config rules to identify compliant and non-compliant services",
        "isCorrect": true
      },
      {
        "text": "Use Amazon CloudWatch Events",
        "isCorrect": false
      },
      {
        "text": "Configure VPC Flow Logs",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Config continuously monitors and records configuration changes to AWS resources and can evaluate compliance against configuration best practices and organizational policies.",
    "awsService": "Config"
  },
  {
    "id": "q273",
    "questionText": "An application needs short-lived access tokens for temporary access to AWS resources. Which service should be used?",
    "options": [
      {
        "text": "AWS IAM Users",
        "isCorrect": false
      },
      {
        "text": "AWS Security Token Service (STS)",
        "isCorrect": true
      },
      {
        "text": "AWS Cognito",
        "isCorrect": false
      },
      {
        "text": "AWS Secrets Manager",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Security Token Service (STS) provides short-lived, temporary security credentials that act as temporary access tokens for accessing AWS resources.",
    "awsService": "STS"
  },
  {
    "id": "q274",
    "questionText": "A company needs to encrypt and automatically rotate database credentials, API keys, and other secrets. Which service should be used?",
    "options": [
      {
        "text": "AWS Systems Manager Parameter Store",
        "isCorrect": false
      },
      {
        "text": "AWS Secrets Manager with automatic rotation enabled",
        "isCorrect": true
      },
      {
        "text": "AWS KMS",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 with encryption",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Secrets Manager automatically encrypts secrets and can automatically rotate credentials for supported databases and services, reducing the risk of credential exposure.",
    "awsService": "SecretsManager"
  },
  {
    "id": "q275",
    "questionText": "A company wants to implement a cost-effective solution for over-provisioned EC2 resources. What scaling configuration should be used?",
    "options": [
      {
        "text": "Configure simple scaling policies",
        "isCorrect": false
      },
      {
        "text": "Configure target tracking scaling in Auto Scaling Groups",
        "isCorrect": true
      },
      {
        "text": "Use manual scaling",
        "isCorrect": false
      },
      {
        "text": "Configure step scaling policies",
        "isCorrect": false
      }
    ],
    "explanation": "Target tracking scaling automatically adjusts the number of instances to maintain a specified metric target, helping to eliminate over-provisioning and reduce costs.",
    "awsService": "AutoScaling"
  },
  {
    "id": "q276",
    "questionText": "A company has application data stored in tape backup solutions that must be preserved for 10 years. What is the most cost-effective storage solution?",
    "options": [
      {
        "text": "Amazon S3 Standard",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Glacier",
        "isCorrect": false
      },
      {
        "text": "Use AWS Storage Gateway to backup data directly to Amazon S3 Glacier Deep Archive",
        "isCorrect": true
      },
      {
        "text": "Amazon EBS volumes",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Storage Gateway can connect to existing tape infrastructure and automatically archive data to S3 Glacier Deep Archive, which is the most cost-effective option for long-term archival.",
    "awsService": "StorageGateway"
  },
  {
    "id": "q277",
    "questionText": "A company needs to accelerate the transfer of historical records from on-premises to AWS over the Internet in a cost-effective manner. What should be used?",
    "options": [
      {
        "text": "AWS Direct Connect",
        "isCorrect": false
      },
      {
        "text": "AWS DataSync with Amazon S3 Glacier Deep Archive as destination",
        "isCorrect": true
      },
      {
        "text": "AWS Storage Gateway",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Transfer Acceleration",
        "isCorrect": false
      }
    ],
    "explanation": "AWS DataSync can efficiently transfer large amounts of data to AWS, and using S3 Glacier Deep Archive as the destination provides the most cost-effective long-term storage.",
    "awsService": "DataSync"
  },
  {
    "id": "q278",
    "questionText": "A company needs to globally deliver static content and media files to customers with low latency. What is the most efficient solution?",
    "options": [
      {
        "text": "Use multiple S3 buckets in different regions",
        "isCorrect": false
      },
      {
        "text": "Store files in Amazon S3 and create a CloudFront distribution with S3 as the origin",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EFS with regional endpoints",
        "isCorrect": false
      },
      {
        "text": "Deploy EC2 instances in multiple regions",
        "isCorrect": false
      }
    ],
    "explanation": "CloudFront with S3 as the origin provides global content delivery through edge locations, ensuring low latency access to static content worldwide.",
    "awsService": "CloudFront"
  },
  {
    "id": "q279",
    "questionText": "An application must be hosted on two EC2 instances and run continuously for three years with stable and predictable CPU utilization. What is the most cost-effective option?",
    "options": [
      {
        "text": "On-Demand Instances",
        "isCorrect": false
      },
      {
        "text": "Spot Instances",
        "isCorrect": false
      },
      {
        "text": "Reserved Instances",
        "isCorrect": true
      },
      {
        "text": "Dedicated Hosts",
        "isCorrect": false
      }
    ],
    "explanation": "Reserved Instances provide significant cost savings (up to 75%) for applications with predictable usage patterns over a 1 or 3-year term commitment.",
    "awsService": "EC2"
  },
  {
    "id": "q280",
    "questionText": "A company needs to implement a cost-effective solution for S3 objects that are accessed less frequently over time. What should be configured?",
    "options": [
      {
        "text": "Enable S3 Cross-Region Replication",
        "isCorrect": false
      },
      {
        "text": "Create an Amazon S3 lifecycle policy to move objects to S3 Standard-IA",
        "isCorrect": true
      },
      {
        "text": "Enable S3 Transfer Acceleration",
        "isCorrect": false
      },
      {
        "text": "Configure S3 Intelligent Tiering",
        "isCorrect": false
      }
    ],
    "explanation": "S3 lifecycle policies can automatically transition objects to more cost-effective storage classes like Standard-IA for infrequently accessed data, reducing storage costs.",
    "awsService": "S3"
  },
  {
    "id": "q281",
    "questionText": "A company wants to minimize data transfer costs between two EC2 instances. What is the best approach?",
    "options": [
      {
        "text": "Deploy instances in different Availability Zones",
        "isCorrect": false
      },
      {
        "text": "Deploy the EC2 instances in the same Region and Availability Zone",
        "isCorrect": true
      },
      {
        "text": "Use different instance families",
        "isCorrect": false
      },
      {
        "text": "Enable Enhanced Networking",
        "isCorrect": false
      }
    ],
    "explanation": "Data transfer between EC2 instances in the same Availability Zone is free, making it the most cost-effective approach for minimizing data transfer costs.",
    "awsService": "EC2"
  },
  {
    "id": "q282",
    "questionText": "A company needs to import SSL/TLS certificates for their application. What are the available options?",
    "options": [
      {
        "text": "Only AWS Certificate Manager",
        "isCorrect": false
      },
      {
        "text": "Only AWS IAM",
        "isCorrect": false
      },
      {
        "text": "Import the certificate into AWS Certificate Manager or upload it to AWS IAM",
        "isCorrect": true
      },
      {
        "text": "Only Amazon S3",
        "isCorrect": false
      }
    ],
    "explanation": "SSL/TLS certificates can be imported into AWS Certificate Manager for use with AWS services, or uploaded to AWS IAM for use with EC2 instances and other resources.",
    "awsService": "ACM"
  },
  {
    "id": "q283",
    "questionText": "A company has deployed a parallel file system for frequently accessed 'hot' data in a high-performance computing environment. Which service should be used?",
    "options": [
      {
        "text": "Amazon EFS",
        "isCorrect": false
      },
      {
        "text": "Amazon FSx for Lustre",
        "isCorrect": true
      },
      {
        "text": "Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Amazon EBS",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon FSx for Lustre is designed for high-performance computing workloads that require fast access to shared data, providing sub-millisecond latencies and high throughput.",
    "awsService": "FSx"
  },
  {
    "id": "q284",
    "questionText": "A company needs synchronous data replication across Availability Zones with automatic failover for their RDS database. What should be implemented?",
    "options": [
      {
        "text": "Read Replicas",
        "isCorrect": false
      },
      {
        "text": "Enable Multi-AZ deployment in Amazon RDS",
        "isCorrect": true
      },
      {
        "text": "Cross-Region backups",
        "isCorrect": false
      },
      {
        "text": "Database snapshots",
        "isCorrect": false
      }
    ],
    "explanation": "RDS Multi-AZ deployment provides synchronous data replication to a standby instance in a different AZ with automatic failover capabilities for high availability.",
    "awsService": "RDS"
  },
  {
    "id": "q285",
    "questionText": "A company needs storage for 'cold' (infrequently accessed) data with the lowest cost and can tolerate longer retrieval times. What storage class should be used?",
    "options": [
      {
        "text": "Amazon S3 Standard",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Standard-IA",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Glacier Deep Archive",
        "isCorrect": true
      },
      {
        "text": "Amazon S3 One Zone-IA",
        "isCorrect": false
      }
    ],
    "explanation": "S3 Glacier Deep Archive provides the lowest cost storage for long-term retention and digital preservation of data that is accessed once or twice per year.",
    "awsService": "S3"
  },
  {
    "id": "q286",
    "questionText": "A company needs to set up a relational database with a disaster recovery plan that has an RPO of 1 second and RTO of less than 1 minute. What should be used?",
    "options": [
      {
        "text": "Amazon RDS with Multi-AZ",
        "isCorrect": false
      },
      {
        "text": "Amazon Aurora Global Database",
        "isCorrect": true
      },
      {
        "text": "Amazon RDS with Read Replicas",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB Global Tables",
        "isCorrect": false
      }
    ],
    "explanation": "Aurora Global Database provides cross-region replication with typical lag of less than 1 second and recovery time objective (RTO) of less than 1 minute for disaster recovery.",
    "awsService": "Aurora"
  },
  {
    "id": "q287",
    "questionText": "A company wants to monitor database metrics and send email notifications when specific thresholds are breached. What should be implemented?",
    "options": [
      {
        "text": "Use only CloudWatch alarms",
        "isCorrect": false
      },
      {
        "text": "Create an SNS topic and add the topic to CloudWatch alarms",
        "isCorrect": true
      },
      {
        "text": "Use AWS Config rules",
        "isCorrect": false
      },
      {
        "text": "Configure VPC Flow Logs",
        "isCorrect": false
      }
    ],
    "explanation": "CloudWatch alarms can monitor database metrics and trigger notifications through SNS topics, which can then send email notifications to subscribers when thresholds are breached.",
    "awsService": "CloudWatch"
  },
  {
    "id": "q288",
    "questionText": "A company needs to implement a 90-day backup retention policy for Amazon Aurora. What service should be used?",
    "options": [
      {
        "text": "Aurora automated backups",
        "isCorrect": false
      },
      {
        "text": "AWS Backup",
        "isCorrect": true
      },
      {
        "text": "Manual snapshots",
        "isCorrect": false
      },
      {
        "text": "Cross-Region replication",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Backup provides centralized backup management and can extend backup retention beyond the default Aurora backup retention period, supporting up to 35 days for automated backups but longer for managed backups.",
    "awsService": "Backup"
  },
  {
    "id": "q289",
    "questionText": "An application needs to retrieve the instance ID, public keys, and public IP address of an EC2 instance from within the instance. What should be used?",
    "options": [
      {
        "text": "AWS CLI",
        "isCorrect": false
      },
      {
        "text": "Access the EC2 instance metadata service at http://169.254.169.254/latest/meta-data/",
        "isCorrect": true
      },
      {
        "text": "AWS Systems Manager",
        "isCorrect": false
      },
      {
        "text": "Amazon CloudWatch",
        "isCorrect": false
      }
    ],
    "explanation": "The EC2 instance metadata service provides information about the instance accessible via the local IP address 169.254.169.254, including instance ID, public keys, and network information.",
    "awsService": "EC2"
  },
  {
    "id": "q290",
    "questionText": "A company needs to use the AWS Well-Architected Framework to review their applications and follow architectural best practices. What type of AWS resource would this involve?",
    "options": [
      {
        "text": "AWS Config Rules",
        "isCorrect": false
      },
      {
        "text": "AWS Well-Architected Tool for workload reviews",
        "isCorrect": true
      },
      {
        "text": "AWS Systems Manager",
        "isCorrect": false
      },
      {
        "text": "AWS CloudFormation",
        "isCorrect": false
      }
    ],
    "explanation": "The AWS Well-Architected Tool helps you review your architectures against AWS best practices and provides guidance for improvement across the five pillars of the Well-Architected Framework.",
    "awsService": "WellArchitectedTool"
  },
  {
    "id": "q291",
    "questionText": "A company needs to implement attribute-based access control (ABAC) for fine-grained permissions based on user attributes. What IAM feature should be used?",
    "options": [
      {
        "text": "IAM Groups",
        "isCorrect": false
      },
      {
        "text": "IAM Policies with condition keys and tags",
        "isCorrect": true
      },
      {
        "text": "IAM Users",
        "isCorrect": false
      },
      {
        "text": "IAM Roles only",
        "isCorrect": false
      }
    ],
    "explanation": "ABAC in AWS uses IAM policies with condition keys and tags to define permissions based on attributes, allowing for more flexible and scalable access control compared to traditional role-based access control.",
    "awsService": "IAM"
  },
  {
    "id": "q292",
    "questionText": "A serverless application needs to automatically adjust capacity based on incoming request patterns. Which compute option provides this capability?",
    "options": [
      {
        "text": "Amazon EC2 with Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "AWS Lambda with automatic scaling",
        "isCorrect": true
      },
      {
        "text": "Amazon ECS with Service Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "AWS Batch",
        "isCorrect": false
      }
    ],
    "explanation": "AWS Lambda automatically scales execution environment capacity based on incoming requests, providing true serverless computing without the need to manage infrastructure or capacity planning.",
    "awsService": "Lambda"
  },
  {
    "id": "q293",
    "questionText": "A company wants to implement CI/CD best practices for software development. Which AWS services should be considered for continuous integration?",
    "options": [
      {
        "text": "Amazon S3 and CloudFront",
        "isCorrect": false
      },
      {
        "text": "AWS CodeBuild and AWS CodePipeline",
        "isCorrect": true
      },
      {
        "text": "Amazon EC2 and ELB",
        "isCorrect": false
      },
      {
        "text": "AWS Lambda and API Gateway",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CodeBuild provides managed build service for compiling source code and running tests, while CodePipeline orchestrates the continuous integration and deployment workflow.",
    "awsService": "CodeBuild"
  },
  {
    "id": "q294",
    "questionText": "A company needs to ensure data encryption in transit and at rest for their S3 bucket. What should be configured?",
    "options": [
      {
        "text": "Only SSL/TLS for data in transit",
        "isCorrect": false
      },
      {
        "text": "Only server-side encryption for data at rest",
        "isCorrect": false
      },
      {
        "text": "Enable Amazon S3 Server-Side Encryption and use SSL/TLS for data in transit",
        "isCorrect": true
      },
      {
        "text": "Use VPC endpoints only",
        "isCorrect": false
      }
    ],
    "explanation": "Complete data protection requires both server-side encryption (SSE) for data at rest in S3 and SSL/TLS encryption for data in transit during API calls and data transfers.",
    "awsService": "S3"
  },
  {
    "id": "q295",
    "questionText": "A high-performance computing application requires low-latency networking between EC2 instances for parallel processing workloads. What should be implemented?",
    "options": [
      {
        "text": "Enhanced Networking with SR-IOV",
        "isCorrect": false
      },
      {
        "text": "Elastic Fabric Adapter (EFA)",
        "isCorrect": true
      },
      {
        "text": "Multiple Elastic Network Interfaces",
        "isCorrect": false
      },
      {
        "text": "VPC Peering",
        "isCorrect": false
      }
    ],
    "explanation": "Elastic Fabric Adapter (EFA) provides high-performance networking for HPC and machine learning applications, enabling low-latency communication with OS-bypass capabilities.",
    "awsService": "EFA"
  },
  {
    "id": "q296",
    "questionText": "A company needs to implement automated scaling for their containerized applications running on Amazon ECS. What scaling capability should be configured?",
    "options": [
      {
        "text": "EC2 Auto Scaling only",
        "isCorrect": false
      },
      {
        "text": "ECS Service Auto Scaling with target tracking",
        "isCorrect": true
      },
      {
        "text": "Manual scaling only",
        "isCorrect": false
      },
      {
        "text": "Lambda scaling",
        "isCorrect": false
      }
    ],
    "explanation": "ECS Service Auto Scaling automatically adjusts the number of tasks in an ECS service based on CloudWatch metrics, providing efficient resource utilization for containerized applications.",
    "awsService": "ECS"
  },
  {
    "id": "q297",
    "questionText": "A company wants to enable cross-account access to their S3 bucket while maintaining security. What is the recommended approach?",
    "options": [
      {
        "text": "Make the bucket public",
        "isCorrect": false
      },
      {
        "text": "Use cross-account IAM roles with bucket policies",
        "isCorrect": true
      },
      {
        "text": "Share access keys",
        "isCorrect": false
      },
      {
        "text": "Use VPC peering only",
        "isCorrect": false
      }
    ],
    "explanation": "Cross-account IAM roles combined with bucket policies provide secure, auditable access to S3 resources across AWS accounts without sharing long-term credentials.",
    "awsService": "S3"
  },
  {
    "id": "q298",
    "questionText": "A company needs to implement Blue/Green deployments for their web application to minimize downtime during updates. Which AWS service facilitates this deployment strategy?",
    "options": [
      {
        "text": "AWS CloudFormation only",
        "isCorrect": false
      },
      {
        "text": "AWS CodeDeploy with Blue/Green deployment configuration",
        "isCorrect": true
      },
      {
        "text": "Amazon EC2 Auto Scaling only",
        "isCorrect": false
      },
      {
        "text": "AWS Lambda versions only",
        "isCorrect": false
      }
    ],
    "explanation": "AWS CodeDeploy supports Blue/Green deployments, allowing you to deploy application updates with minimal downtime by gradually shifting traffic from the current version to the new version.",
    "awsService": "CodeDeploy"
  },
  {
    "id": "q299",
    "questionText": "A company needs to analyze streaming data in real-time for fraud detection. Which AWS service is most appropriate for this use case?",
    "options": [
      {
        "text": "Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Amazon Kinesis Data Analytics",
        "isCorrect": true
      },
      {
        "text": "Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Amazon Kinesis Data Analytics enables real-time analysis of streaming data using SQL queries, making it ideal for fraud detection and real-time analytics use cases.",
    "awsService": "Kinesis"
  },
  {
    "id": "q300",
    "questionText": "A company wants to migrate their on-premises VMware environment to AWS while maintaining compatibility. Which service should they use?",
    "options": [
      {
        "text": "Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "AWS Server Migration Service",
        "isCorrect": false
      },
      {
        "text": "VMware Cloud on AWS",
        "isCorrect": true
      },
      {
        "text": "AWS Application Migration Service",
        "isCorrect": false
      }
    ],
    "explanation": "VMware Cloud on AWS allows organizations to run VMware vSphere workloads natively on AWS infrastructure, providing seamless migration and hybrid cloud capabilities with full VMware compatibility.",
    "awsService": "VMwareCloud"
  },
  {
    "id": "q301",
    "questionText": "A retail company has developed a REST API with the following architecture:\n\n**Current Setup:**\nâ€¢ Deployed in Auto Scaling group behind Application Load Balancer\nâ€¢ User data stored in Amazon DynamoDB\nâ€¢ Static content (images) served via Amazon S3\n\n**Performance Analysis:**\nâ€¢ 90% of read requests target commonly accessed data across all users\n\n**Question:**\nAs a Solutions Architect, which solution would you suggest as the MOST efficient way to improve application performance?",
    "options": [
      {
        "text": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3",
        "isCorrect": true
      },
      {
        "text": "Enable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Enable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nEnable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and Amazon CloudFront for Amazon S3\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvementâ€”from milliseconds to microsecondsâ€”even at millions of requests per second.\n\nAmazon DynamoDB Accelerator (DAX) is tightly integrated with Amazon DynamoDBâ€”you simply provision a DAX cluster, use the DAX client SDK to point your existing Amazon DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with Amazon DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache Amazon DynamoDB reads.\n\nAmazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from S3 directly to your users.\n\nWhen a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file theyâ€™ve requested isnâ€™t yet cached, CloudFront retrieves it from your origin â€“ for example, the Amazon S3 bucket where youâ€™ve stored your content.\n\nSo, you can use Amazon CloudFront to improve application performance to serve static content from Amazon S3.\n\nIncorrect options:\n\nEnable ElastiCache Redis for DynamoDB and Amazon CloudFront for Amazon S3\n\nAmazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\n\nAmazon ElastiCache for Redis Overview:\n\nvia - https://aws.amazon.com/elasticache/redis/\n\nAlthough you can integrate Redis with DynamoDB, it's much more involved than using DAX which is a much better fit.\n\nEnable Amazon DynamoDB Accelerator (DAX) for Amazon DynamoDB and ElastiCache Memcached for Amazon S3\n\nEnable ElastiCache Redis for DynamoDB and ElastiCache Memcached for Amazon S3\n\nAmazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.\n\nAmazon ElastiCache Memcached cannot be used as a cache to serve static content from Amazon S3, so both these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/dax/\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\n\nhttps://aws.amazon.com/elasticache/redis/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q302",
    "questionText": "An e-commerce company is looking for a solution with high availability, as it plans to migrate its flagship application to a fleet of Amazon Elastic Compute Cloud (Amazon EC2) instances. The solution should allow for content-based routing as part of the architecture.\n\nAs a Solutions Architect, which of the following will you suggest for the company?",
    "options": [
      {
        "text": "Use an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance",
        "isCorrect": true
      },
      {
        "text": "Use a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance",
        "isCorrect": false
      },
      {
        "text": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance",
        "isCorrect": false
      },
      {
        "text": "Use an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse an Application Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure Auto Scaling group to mask any failure of an instance\n\nThe Application Load Balancer (ALB) is best suited for load balancing HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), the Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request.\n\nThis is the correct option since the question has a specific requirement for content-based routing which can be configured via the Application Load Balancer. Different Availability Zones (AZs) provide high availability to the overall architecture and Auto Scaling group will help mask any instance failures.\n\nMore info on Application Load Balancer:\n\nvia - https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/\n\nIncorrect options:\n\nUse a Network Load Balancer for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Private IP address to mask any failure of an instance - Network Load Balancer cannot facilitate content-based routing so this option is incorrect.\n\nUse an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure an elastic IP address (EIP) to mask any failure of an instance\n\nUse an Auto Scaling group for distributing traffic to the Amazon EC2 instances spread across different Availability Zones (AZs). Configure a Public IP address to mask any failure of an instance\n\nBoth these options are incorrect as you cannot use the Auto Scaling group to distribute traffic to the Amazon EC2 instances.\n\nAn elastic IP address (EIP) is a static, public, IPv4 address allocated to your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Elastic IPs do not change and remain allocated to your account until you delete them.\n\nMore info on Elastic Load Balancing (ELB):\n\nvia - https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf\n\nYou can span your Auto Scaling group across multiple Availability Zones (AZs) within an AWS Region and then attaching a load balancer to distribute incoming traffic across those zones.\n\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/\n\nhttps://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q303",
    "questionText": "A healthcare company uses its on-premises infrastructure to run legacy applications that require specialized customizations to the underlying Oracle database as well as its host operating system (OS). The company also wants to improve the availability of the Oracle database layer. The company has hired you as an AWS Certified Solutions Architect â€“ Associate to build a solution on AWS that meets these requirements while minimizing the underlying infrastructure maintenance effort.\n\nWhich of the following options represents the best solution for this use case?",
    "options": [
      {
        "text": "Deploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "isCorrect": false
      },
      {
        "text": "Leverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "isCorrect": true
      },
      {
        "text": "Leverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "isCorrect": false
      },
      {
        "text": "Leverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nLeverage multi-AZ configuration of Amazon RDS Custom for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nAmazon RDS is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while managing time-consuming database administration tasks. Amazon RDS can automatically back up your database and keep your database software up to date with the latest version. However, RDS does not allow you to access the host OS of the database.\n\nFor the given use-case, you need to use Amazon RDS Custom for Oracle as it allows you to access and customize your database server host and operating system, for example by applying special patches and changing the database software settings to support third-party applications that require privileged access. Amazon RDS Custom for Oracle facilitates these functionalities with minimum infrastructure maintenance effort. You need to set up the RDS Custom for Oracle in multi-AZ configuration for high availability.\n\nAmazon RDS Custom for Oracle:\n\nvia - https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/\n\nIncorrect options:\n\nLeverage multi-AZ configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nLeverage cross AZ read-replica configuration of Amazon RDS for Oracle that allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system\n\nAmazon RDS for Oracle does not allow you to access and customize your database server host and operating system. Therefore, both these options are incorrect.\n\nDeploy the Oracle database layer on multiple Amazon EC2 instances spread across two Availability Zones (AZs). This deployment configuration guarantees high availability and also allows the Database Administrator (DBA) to access and customize the database environment and the underlying operating system - The use case requires that the best solution should involve minimum infrastructure maintenance effort. When you use Amazon EC2 instances to host the databases, you need to manage the server health, server maintenance, server patching, and database maintenance tasks yourself. In addition, you will also need to manage the multi-AZ configuration by deploying Amazon EC2 instances across two Availability Zones (AZs), perhaps by using an Auto Scaling group. These steps entail significant maintenance effort. Hence this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/\n\nhttps://aws.amazon.com/rds/faqs/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q304",
    "questionText": "An e-commerce company manages a digital catalog of consumer products submitted by third-party sellers. Each product submission includes a description stored as a text file in an Amazon S3 bucket. These descriptions may include ingredient information for consumable products like snacks, supplements, or beverages. The company wants to build a fully automated solution that extracts ingredient names from the uploaded product descriptions and uses those names to query an Amazon DynamoDB table, which returns precomputed health and safety scores for each ingredient. Non-food items and invalid submissions can be ignored without affecting application logic. The company has no in-house machine learning (ML) experts and is looking for the most cost-effective solution with minimal operational overhead.\n\nWhich solution meets these requirements MOST cost-effectively?",
    "options": [
      {
        "text": "Configure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores",
        "isCorrect": true
      },
      {
        "text": "Use Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly",
        "isCorrect": false
      },
      {
        "text": "Create a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure S3 Event Notifications to trigger an AWS Lambda function whenever a new product description is uploaded. Inside the function, use Amazon Comprehend's custom entity recognition feature to extract ingredient names. Store these names in the DynamoDB table and let the front-end application query for health scores\n\nThis option uses Amazon Comprehend's custom entity recognition feature to detect and extract ingredients from unstructured text. Since Comprehend is fully managed and doesn't require ML expertise, it aligns with the companyâ€™s constraints. It integrates easily into a serverless pipeline using S3 Event Notifications and AWS Lambda. The ingredient names can then be used as lookup keys in a DynamoDB table to retrieve nutrition or safety scores. This architecture is event-driven, scalable, cost-efficient, and requires minimal operational effort.\n\n\nvia - https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html\n\nIncorrect options:\n\nUse Amazon SageMaker with a custom-trained NLP model to identify ingredients from the uploaded descriptions. Use Amazon EventBridge to invoke a Lambda function that forwards the document content to a SageMaker endpoint and stores the results in DynamoDB. Fine-tune the model using labeled ingredient datasets from open-source repositories and retrain it monthly - While SageMaker is a powerful and flexible ML service, it requires ML expertise to train, deploy, and manage models. Fine-tuning and maintaining a custom NLP model, especially for entity extraction, involves substantial effort and cost. It also contradicts the companyâ€™s requirement to avoid ML development overhead. This solution is over-engineered for the use case and not the most cost-effective.\n\nCreate a workflow where Amazon Transcribe is used to convert synthetic audio versions (created from text of the product descriptions) back into text. Analyze the transcripts manually or using simple keyword matching within a Lambda function. Use Amazon SNS to notify the content moderation team for each processed file - This approach is convoluted and inefficient. Generating synthetic audio from text, transcribing it back, and relying on manual or keyword-based analysis introduces redundant processing steps and delays. Additionally, using Amazon SNS to notify human reviewers increases operational burden and violates the requirement for a fully automated solution.\n\nUse Amazon Lookout for Vision to scan the uploaded text files in the S3 bucket and extract entities. Invoke this workflow using an S3-triggered Lambda function. Parse the output and use Amazon API Gateway to push updates to the frontend in real time - Amazon Lookout for Vision is designed for image-based anomaly detection, not text processing. It cannot extract entities or analyze structured/unstructured text documents, making it completely unsuitable for this use case. Using API Gateway for pushing updates is unrelated to the core requirement of ingredient extraction and further increases complexity.\n\nReferences:\n\nhttps://docs.aws.amazon.com/comprehend/latest/dg/what-is.html\n\nhttps://docs.aws.amazon.com/comprehend/latest/dg/custom-entity-recognition.html\n\nhttps://aws.amazon.com/blogs/machine-learning/build-a-custom-entity-recognizer-using-amazon-comprehend/\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html\n\nhttps://docs.aws.amazon.com/transcribe/latest/dg/what-is.html\n\nhttps://docs.aws.amazon.com/lookout-for-vision/latest/developer-guide/what-is.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q305",
    "questionText": "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. The company is looking for AWS services that can be used for buffering or throttling to handle such traffic variations.\n\nWhich of the following services can be used to support this requirement?",
    "options": [
      {
        "text": "Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda",
        "isCorrect": false
      },
      {
        "text": "Amazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
        "isCorrect": true
      },
      {
        "text": "Amazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis",
        "isCorrect": false
      },
      {
        "text": "Elastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThrottling is the process of limiting the number of requests an authorized program can submit to a given operation in a given amount of time.\n\nAmazon API Gateway, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis\n\nTo prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API using the token bucket algorithm, where a token counts for a request. Specifically, API Gateway sets a limit on a steady-state rate and a burst of request submissions against all APIs in your account. In the token bucket algorithm, the burst is the maximum bucket size.\n\nAmazon Simple Queue Service (Amazon SQS) - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers buffer capabilities to smooth out temporary volume spikes without losing messages or increasing latency.\n\nAmazon Kinesis - Amazon Kinesis is a fully managed, scalable service that can ingest, buffer, and process streaming data in real-time.\n\nIncorrect options:\n\nAmazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS) and AWS Lambda - Amazon SQS has the ability to buffer its messages. Amazon Simple Notification Service (SNS) cannot buffer messages and is generally used with SQS to provide the buffering facility. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect.\n\nAmazon Gateway Endpoints, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis - A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. This cannot help in throttling or buffering of requests. Amazon SQS and Kinesis can buffer incoming data. Since Gateway Endpoint is an incorrect service for throttling or buffering, this option is incorrect.\n\nElastic Load Balancer, Amazon Simple Queue Service (Amazon SQS), AWS Lambda - Elastic Load Balancer cannot throttle requests. Amazon SQS can be used to buffer messages. When requests come in faster than your Lambda function can scale, or when your function is at maximum concurrency, additional requests fail as the Lambda throttles those requests with error code 429 status code. So, this combination of services is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html\n\nhttps://aws.amazon.com/sqs/features/",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q306",
    "questionText": "A media company runs a photo-sharing web application that is accessed across three different countries. The application is deployed on several Amazon Elastic Compute Cloud (Amazon EC2) instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company.\n\nWhich configuration should be used to meet this changed requirement?",
    "options": [
      {
        "text": "Configure the security group on the Application Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)",
        "isCorrect": true
      },
      {
        "text": "Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)",
        "isCorrect": false
      },
      {
        "text": "Configure the security group for the Amazon EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAWS Web Application Firewall (AWS WAF) is a web application firewall service that lets you monitor web requests and protect your web applications from malicious requests. Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. You can also use AWS WAF preconfigured protections to block common attacks like SQL injection or cross-site scripting.\n\nConfigure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)\n\nYou can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). Geographic (Geo) Match Conditions in AWS WAF allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access.\n\nGeo match conditions are important for many customers. For example, legal and licensing requirements restrict some customers from delivering their applications outside certain countries. These customers can configure a whitelist that allows only viewers in those countries. Other customers need to prevent the downloading of their encrypted software by users in certain countries. These customers can configure a blacklist so that end-users from those countries are blocked from downloading their software.\n\nIncorrect options:\n\nUse Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC) - Geo Restriction feature of Amazon CloudFront helps in restricting traffic based on the user's geographic location. But, CloudFront works from edge locations and doesn't belong to a VPC. Hence, this option itself is incorrect and given only as a distractor.\n\nConfigure the security group on the Application Load Balancer\n\nConfigure the security group for the Amazon EC2 instances\n\nSecurity Groups cannot restrict access based on the user's geographic location.\n\nReferences:\n\nhttps://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/\n\nhttps://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\n\nhttps://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q307",
    "questionText": "The flagship application for a gaming company connects to an Amazon Aurora database and the entire technology stack is currently deployed in the United States. Now, the company has plans to expand to Europe and Asia for its operations. It needs the games table to be accessible globally but needs the users and games_played tables to be regional only.\n\nHow would you implement this with minimal application refactoring?",
    "options": [
      {
        "text": "Use an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables",
        "isCorrect": true
      },
      {
        "text": "Use an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "isCorrect": false
      },
      {
        "text": "Use a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables",
        "isCorrect": false
      },
      {
        "text": "Use a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse an Amazon Aurora Global Database for the games table and use Amazon Aurora for the users and games_played tables\n\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database.\n\nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.\n\nFor the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (games table) and the other one for the local tables (users and games_played tables).\n\nIncorrect options:\n\nUse an Amazon Aurora Global Database for the games table and use Amazon DynamoDB tables for the users and games_played tables\n\nUse a Amazon DynamoDB global table for the games table and use Amazon Aurora for the users and games_played tables\n\nUse a Amazon DynamoDB global table for the games table and use Amazon DynamoDB tables for the users and games_played tables\n\nHere, we want minimal application refactoring. Amazon DynamoDB and Amazon Aurora have a completely different APIs, due to Amazon Aurora being SQL and Amazon DynamoDB being NoSQL. So all three options are incorrect, as they have Amazon DynamoDB as one of the components.\n\nReference:\n\nhttps://aws.amazon.com/rds/aurora/faqs/",
    "awsService": "DynamoDB",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q308",
    "questionText": "A media agency stores its re-creatable assets on Amazon Simple Storage Service (Amazon S3) buckets. The assets are accessed by a large number of users for the first few days and the frequency of access falls down drastically after a week. Although the assets would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the assets on Amazon S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible.\n\nAs an AWS Certified Solutions Architect â€“ Associate, can you suggest a way to lower the storage costs while fulfilling the business requirements?",
    "options": [
      {
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days",
        "isCorrect": false
      },
      {
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days",
        "isCorrect": false
      },
      {
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days",
        "isCorrect": true
      },
      {
        "text": "Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days\n\nAmazon S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single Availability Zone (AZ) and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed and re-creatable data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. The minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA.\n\nAmazon S3 One Zone-IA offers the same high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. S3 Storage Classes can be configured at the object level, and a single bucket can contain objects stored across Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.\n\nConstraints for Lifecycle storage class transitions:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\n\nSupported Amazon S3 lifecycle transitions:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\n\nIncorrect options:\n\nConfigure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days\n\nConfigure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days\n\nAs mentioned earlier, the minimum storage duration is 30 days before you can transition objects from Amazon S3 Standard to Amazon S3 One Zone-IA or Amazon S3 Standard-IA, so both these options are added as distractors.\n\nConfigure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days - Amazon S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. But, it costs more than Amazon S3 One Zone-IA because of the redundant storage across Availability Zones (AZs). As the data is re-creatable, so you don't need to incur this additional cost.\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q309",
    "questionText": "A biotechnology firm runs genomics data analysis workloads using AWS Lambda functions deployed inside a VPC in their central AWS account. The input data for these workloads consists of large files stored in an Amazon Elastic File System (Amazon EFS) that resides in a separate AWS account managed by a research partner. The firm wants the Lambda function in their account to access the shared EFS storage directly. The access pattern and file volume are expected to grow as additional research datasets are added over time, so the solution must be scalable and cost-efficient, and should require minimal operational overhead.\n\nWhich solution best meets these requirements in the MOST cost-effective way?",
    "options": [
      {
        "text": "Use Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point",
        "isCorrect": true
      },
      {
        "text": "Set up an Amazon S3 bucket in the research partnerâ€™s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts",
        "isCorrect": false
      },
      {
        "text": "Create a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation",
        "isCorrect": false
      },
      {
        "text": "Package the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon EFS resource policies to allow cross-account access to the file system from the central account. Attach the EFS mount target to a shared VPC or peered VPC, and mount the file system in the Lambda function configuration using an EFS access point\n\nThis solution uses native AWS support for cross-account access to EFS via resource policies, combined with EFS access points and VPC-level networking (VPC peering or shared VPC). The Lambda function in the central account can mount the file system directly using the EFS mount target and access point as if it were in the same account. This method supports automatic scaling, secure access, and avoids any need for data duplication or proxy functions. It is the most cost-effective and operationally efficient solution.\n\nIncorrect options:\n\nSet up an Amazon S3 bucket in the research partnerâ€™s account and periodically copy EFS contents into the bucket using scheduled AWS DataSync jobs. Use Amazon S3 Access Points to expose the data to the Lambda function in the central account, allowing access via S3 API calls instead of file system mounts - While this approach enables cross-account access to data using Amazon S3 Access Points, it introduces significant storage duplication and recurring transfer costs. EFS contents must be periodically copied to S3 using AWS DataSync, which adds operational complexity and delay. This solution also does not support real-time or low-latency file system operations, which may be required by the workloads.\n\nCreate a second Lambda function in the research partner's account that mounts the EFS file system locally. Have the main Lambda function in the central account invoke this secondary Lambda via Amazon API Gateway for data access and computation. Use IAM cross-account permissions to allow invocation - This option introduces additional latency and architectural complexity by involving a proxy Lambda function in a separate account, which acts as an intermediary for file access. It breaks the direct file system access model, limits performance, and requires managing a custom API layer. It also increases operational overhead, and any scaling of the secondary function will incur further compute cost and complexity.\n\nPackage the genomic input data as a Lambda layer and publish it in the research partner's account. Share the layer across accounts by modifying its resource policy and attach the layer to the Lambda function in the central account to access the data during execution - Lambda layers are designed for shared code and dependencies, not for large or dynamic datasets like genomic files. There are strict size limits (250 MB unzipped) on Lambda layers, and they are immutable once published. Sharing large or growing data sets via Lambda layers is infeasible and not scalable. This method is both cost-ineffective and technically unsuitable for the given use case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/whatisefs.html\n\nhttps://repost.aws/questions/QUCxMGeJpWRKKuvibwxWfP7Q/cross-account-efs-access-point-mount-on-lambda-with-root-path-as\n\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html\n\nhttps://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/adding-layers.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q310",
    "questionText": "A retail company uses Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon API Gateway, Amazon RDS, Elastic Load Balancer and Amazon CloudFront services. To improve the security of these services, the Risk Advisory group has suggested a feasibility check for using the Amazon GuardDuty service.\n\nWhich of the following would you identify as data sources supported by Amazon GuardDuty?",
    "options": [
      {
        "text": "VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs",
        "isCorrect": false
      },
      {
        "text": "VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "isCorrect": true
      },
      {
        "text": "Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events",
        "isCorrect": false
      },
      {
        "text": "Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nVPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events\n\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in AWS. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.\n\nAmazon GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail events, Amazon VPC Flow Logs, and DNS logs.\n\nWith a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with Amazon EventBridge Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.\n\nHow Amazon GuardDuty works:\n\nvia - https://aws.amazon.com/guardduty/\n\nIncorrect options:\n\nVPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs\n\nElastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events\n\nAmazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/guardduty/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q311",
    "questionText": "A digital wallet company plans to launch a new cloud-based service for processing user cash transfers and peer-to-peer payments. The application will receive transaction requests from mobile clients via a secure endpoint. Each transaction request must go through a lightweight validation step before being forwarded for backend processing, which includes fraud detection, ledger updates, and notifications. The backend workload is compute- and memory-intensive, requires scaling based on volume, and must run for a longer duration than typical short-lived tasks. The engineering team prefers a fully managed solution that minimizes infrastructure maintenance, including provisioning and patching of virtual machines or containers.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "text": "Build a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand",
        "isCorrect": false
      },
      {
        "text": "Configure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks",
        "isCorrect": false
      },
      {
        "text": "Expose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required",
        "isCorrect": true
      },
      {
        "text": "Create an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the companyâ€™s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nExpose an Amazon API Gateway REST API endpoint to receive transaction requests from mobile clients. Integrate the API with AWS Lambda to perform basic validation. For backend processing, deploy the long-running application to Amazon ECS using the Fargate launch type, allowing ECS to manage compute and memory provisioning automatically, with no server management required\n\nThis architecture uses Amazon API Gateway to securely expose a RESTful endpoint, which integrates seamlessly with AWS Lambda for the lightweight validation logic. Once validated, the data is passed to Amazon ECS running on Fargate, a serverless container orchestration service that automatically provisions compute and memory resources based on task definitions. ECS with Fargate eliminates the need for cluster management, OS patching, or infrastructure provisioning. This makes the solution fully managed, highly scalable, and the most operationally efficient for long-running backend tasks.\n\nWhat is serverless development?\n\nvia - https://docs.aws.amazon.com/serverless/latest/devguide/welcome.html\n\nIncorrect options:\n\nBuild a REST API using Amazon API Gateway. Integrate it with an AWS Step Functions state machine for validation. Launch the backend application using Amazon EKS with self-managed nodes, and use Kubernetes Jobs to handle transaction processing workflows. Manually scale the cluster based on demand - Although Step Functions can manage workflows and EKS supports containerized apps, EKS with self-managed nodes introduces infrastructure complexity. You must provision and manage the worker nodes, monitor Kubernetes components, and maintain security patches. This goes against the company's requirement to avoid managing infrastructure. Step Functions is also less ideal for handling synchronous validation of lightweight logic, where Lambda is more cost-effective.\n\nConfigure Amazon SQS to receive encrypted payment notifications from mobile devices. Use Amazon EventBridge rules to extract the payload and perform validation. Route the messages to a backend system hosted on Amazon Lightsail instances with dynamic scaling policies based on memory thresholds and instance health checks - This approach combines messaging and routing but uses Amazon Lightsail, which is designed for simplified hosting of websites and smaller applications. While Lightsail supports scaling policies, it lacks the advanced orchestration, scalability, and managed features needed for enterprise-grade backend processing. Additionally, using EventBridge for validation is not correct â€” itâ€™s a routing service, not a transformation or logic execution engine.\n\nCreate an Amazon API Gateway endpoint to receive transaction requests from mobile devices. Use AWS Lambda to validate the transactions. For backend processing, deploy the application on Amazon EKS Anywhere, running on on-premises servers in the companyâ€™s data center. Use a custom provisioning script to scale Kubernetes worker nodes based on transaction volume - This solution introduces unnecessary infrastructure management, which contradicts the company's requirement to avoid managing infrastructure. Amazon EKS Anywhere is intended for organizations that need to run Kubernetes on-premises, often for latency, compliance, or hybrid cloud scenarios. However, it requires you to manage the entire infrastructure stack, including hardware, networking, OS patches, Kubernetes upgrades, and worker node scaling. That adds significant operational overhead, making it unsuitable for a team prioritizing low-maintenance, cloud-native solutions.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\n\nhttps://docs.aws.amazon.com/serverless/latest/devguide/welcome.html\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-set-up-simple-proxy.html\n\nhttps://aws.amazon.com/eks/eks-anywhere/",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q312",
    "questionText": "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect â€“ Associate, you have been tasked to make the application more resilient to periodic spikes in request rates.\n\nWhich of the following solutions would you recommend for the given use-case? (Select two)",
    "options": [
      {
        "text": "Use Amazon Aurora Replica",
        "isCorrect": true
      },
      {
        "text": "Use AWS Shield",
        "isCorrect": false
      },
      {
        "text": "Use AWS Global Accelerator",
        "isCorrect": false
      },
      {
        "text": "Use AWS Direct Connect",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudFront distribution in front of the Application Load Balancer",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nYou can use Amazon Aurora replicas and Amazon CloudFront distribution to make the application more resilient to spikes in request rates.\n\nUse Amazon Aurora Replica\n\nAmazon Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Amazon Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. Up to 15 Aurora Replicas can be distributed across the Availability Zones (AZs) that a DB cluster spans within an AWS Region.\n\nUse Amazon CloudFront distribution in front of the Application Load Balancer\n\nAmazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. Amazon CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.\n\nAmazon CloudFront offers an origin failover feature to help support your data resiliency needs. Amazon CloudFront is a global service that delivers your content through a worldwide network of data centers called edge locations or points of presence (POPs). If your content is not already cached in an edge location, Amazon CloudFront retrieves it from an origin that you've identified as the source for the definitive version of the content.\n\nIncorrect options:\n\nUse AWS Shield - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency. There are two tiers of AWS Shield - Standard and Advanced. AWS Shield cannot be used to improve application resiliency to handle spikes in traffic.\n\nUse AWS Global Accelerator - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. Amazon Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Since Amazon CloudFront is better for improving application resiliency to handle spikes in traffic, so this option is ruled out.\n\nUse AWS Direct Connect - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. AWS Direct Connect cannot be used to improve application resiliency to handle spikes in traffic.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/disaster-recovery-resiliency.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\nhttps://aws.amazon.com/global-accelerator/faqs/\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/disaster-recovery-resiliency.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q313",
    "questionText": "A government agency is developing a online application to assist users in submitting permit requests through a web-based interface. The system architecture consists of a front-end web application tier and a background processing tier that handles the validation and submission of the forms. The application is expected to see high traffic and it must ensure that every submitted request is processed exactly once, with no loss of data.\n\nWhich design choice best satisfies these requirements?",
    "options": [
      {
        "text": "Implement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier",
        "isCorrect": true
      },
      {
        "text": "Implement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier",
        "isCorrect": false
      },
      {
        "text": "Leverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling",
        "isCorrect": false
      },
      {
        "text": "Leverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nImplement an Amazon SQS FIFO queue to reliably buffer and deliver form submissions from the web application layer to the processing tier\n\nSQS FIFO (First-In-First-Out) queues are designed for exactly-once message processing with order preservation. They prevent duplicate deliveries, making them ideal for transactional workflows such as registration or permit submissions. FIFO queues guarantee that each message is delivered and processed once and only once, which satisfies the applicationâ€™s reliability and data integrity requirements.\n\nIncorrect options:\n\nImplement an Amazon SQS standard queue to reliably buffer and deliver form submissions from the web application layer to the processing tier - The option to leverage an Amazon Simple Queue Service (Amazon SQS) standard queue between the web and processing tiers is incorrect for this use case because SQS standard queues do not guarantee exactly-once message delivery. Instead, they provide at-least-once delivery and best-effort ordering. In the context of form submissions or permit requestsâ€”where each submission must be processed exactly once and in the correct orderâ€”standard queues introduce the risk of duplicate messages or out-of-order processing\n\nLeverage Amazon EventBridge to send events from the web application to the processing tier for asynchronous form handling - Amazon EventBridge is a powerful event bus that enables loosely coupled architectures and event-driven applications. However, it does not guarantee exactly-once delivery. Instead, EventBridge provides at-least-once delivery semantics, which means events can be delivered more than once in certain failure scenarios. So, the key shortcomings for this use case are - no built-in ordering of events and possible duplicate event delivery.\n\nLeverage Amazon API Gateway to pass the form submissions to AWS Lambda for processing in real time - While API Gateway with Lambda is effective for real-time synchronous processing, it does not inherently offer decoupling between tiers or a durable queue. It also lacks native support for guaranteed exactly-once processing across asynchronous workloads.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-fifo-queues.html\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q314",
    "questionText": "A leading social media analytics company is contemplating moving its dockerized application stack into AWS Cloud. The company is not sure about the pricing for using Amazon Elastic Container Service (Amazon ECS) with the EC2 launch type compared to the Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type.\n\nWhich of the following is correct regarding the pricing for these two services?",
    "options": [
      {
        "text": "Amazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests",
        "isCorrect": true
      },
      {
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour",
        "isCorrect": false
      },
      {
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests",
        "isCorrect": false
      },
      {
        "text": "Both Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. Amazon ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests\n\nAmazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. ECS allows you to easily run, scale, and secure Docker container applications on AWS.\n\nAmazon ECS Overview:\n\nvia - https://aws.amazon.com/ecs/\n\nWith the Fargate launch type, you pay for the amount of vCPU and memory resources that your containerized application requests. vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS Task terminates, rounded up to the nearest second.\nWith the EC2 launch type, there is no additional charge for the EC2 launch type. You pay for AWS resources (e.g. EC2 instances or EBS volumes) you create to store and run your application.\n\nIncorrect options:\n\nBoth Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on vCPU and memory resources that the containerized application requests\n\nBoth Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are charged based on Amazon EC2 instances and Amazon EBS Elastic Volumes used\n\nAs mentioned above - with the Fargate launch type, you pay for the amount of vCPU and memory resources. With EC2 launch type, you pay for AWS resources (e.g. EC2 instances or EBS volumes). Hence both these options are incorrect.\n\nBoth Amazon ECS with EC2 launch type and Amazon ECS with Fargate launch type are just charged based on Elastic Container Service used per hour\n\nThis is a made-up option and has been added as a distractor.\n\nReferences:\n\nhttps://aws.amazon.com/ecs/pricing/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q315",
    "questionText": "An audit department generates and accesses the audit reports only twice in a financial year. The department uses AWS Step Functions to orchestrate the report creating process that has failover and retry scenarios built into the solution. The underlying data to create these audit reports is stored on Amazon S3, runs into hundreds of Terabytes and should be available with millisecond latency.\n\nAs an AWS Certified Solutions Architect â€“ Associate, which is the MOST cost-effective storage class that you would recommend to be used for this use-case?",
    "options": [
      {
        "text": "Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering)",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
        "isCorrect": true
      },
      {
        "text": "Amazon S3 Standard",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Glacier Deep Archive",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA)\n\nSince the data is accessed only twice in a financial year but needs rapid access when required, the most cost-effective storage class for this use-case is Amazon S3 Standard-IA. S3 Standard-IA storage class is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. Amazon Standard-IA is designed for 99.9% availability compared to 99.99% availability of Amazon S3 Standard. However, the report creation process has failover and retry scenarios built into the workflow, so in case the data is not available owing to the 99.9% availability of Amazon S3 Standard-IA, the job will be auto re-invoked till data is successfully retrieved. Therefore this is the correct option.\n\nAmazon S3 Storage Classes Overview:\n\nvia - https://aws.amazon.com/s3/storage-classes/\n\nIncorrect options:\n\nAmazon S3 Standard - Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. As described above, Amazon S3 Standard-IA storage is a better fit than Amazon S3 Standard, hence using S3 standard is ruled out for the given use-case.\n\nAmazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) - For a small monthly object monitoring and automation charge, Amazon S3 Intelligent-Tiering monitors access patterns and automatically moves objects that have not been accessed to lower-cost access tiers. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. S3 Standard-IA matches the high durability, high throughput, and low latency of S3 Intelligent-Tiering, with a low per GB storage price and per GB retrieval fee. Moreover, Standard-IA has the same availability as that of Amazon S3 Intelligent-Tiering. So, it's cost-efficient to use S3 Standard-IA instead of S3 Intelligent-Tiering.\n\nAmazon S3 Glacier Deep Archive - Amazon S3 Glacier Deep Archive is a secure, durable, and low-cost storage class for data archiving. Amazon S3 Glacier Deep Archive does not support millisecond latency, so this option is ruled out.\n\nFor more details on the durability, availability, cost and access latency - please review this reference link:\nhttps://aws.amazon.com/s3/storage-classes",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q316",
    "questionText": "A new DevOps engineer has joined a large financial services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management (AWS IAM).\n\nAs an AWS Certified Solutions Architect â€“ Associate, which best practices would you recommend (Select two)?",
    "options": [
      {
        "text": "Create a minimum number of accounts and share these account credentials among employees",
        "isCorrect": false
      },
      {
        "text": "Grant maximum privileges to avoid assigning privileges again",
        "isCorrect": false
      },
      {
        "text": "Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users",
        "isCorrect": true
      },
      {
        "text": "Use user credentials to provide access specific permissions for Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nEnable AWS Multi-Factor Authentication (AWS MFA) for privileged users\n\nAs per the AWS best practices, it is better to enable Multi Factor Authentication (MFA) for privileged users via an MFA-enabled mobile device or hardware MFA token.\n\nConfigure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions\n\nAWS recommends to turn on AWS CloudTrail to log all IAM actions for monitoring and audit purposes.\n\nIncorrect options:\n\nCreate a minimum number of accounts and share these account credentials among employees - AWS recommends that user account credentials should not be shared between users. So, this option is incorrect.\n\nGrant maximum privileges to avoid assigning privileges again - AWS recommends granting the least privileges required to complete a certain job and avoid giving excessive privileges which can be misused. So, this option is incorrect.\n\nUse user credentials to provide access specific permissions for Amazon EC2 instances - It is highly recommended to use roles to grant access permissions for EC2 instances working on different AWS services. So, this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/iam/\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\n\nhttps://aws.amazon.com/cloudtrail/faqs/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q317",
    "questionText": "An organization wants to delegate access to a set of users from the development environment so that they can access some resources in the production environment which is managed under another AWS account.\n\nAs a solutions architect, which of the following steps would you recommend?",
    "options": [
      {
        "text": "Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment",
        "isCorrect": false
      },
      {
        "text": "Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment",
        "isCorrect": true
      },
      {
        "text": "It is not possible to access cross-account resources",
        "isCorrect": false
      },
      {
        "text": "Both IAM roles and IAM users can be used interchangeably for cross-account access",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment\n\nIAM roles allow you to delegate access to users or services that normally don't have access to your organization's AWS resources. IAM users or AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls. Consequently, you don't have to share long-term credentials for access to a resource. Using IAM roles, it is possible to access cross-account resources.\n\nIncorrect options:\n\nCreate new IAM user credentials for the production environment and share these credentials with the set of users from the development environment - There is no need to create new IAM user credentials for the production environment, as you can use IAM roles to access cross-account resources.\n\nIt is not possible to access cross-account resources - You can use IAM roles to access cross-account resources.\n\nBoth IAM roles and IAM users can be used interchangeably for cross-account access - IAM roles and IAM users are separate IAM entities and should not be mixed. Only IAM roles can be used to access cross-account resources.\n\nReference:\n\nhttps://aws.amazon.com/iam/features/manage-roles/",
    "awsService": "IAM",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q318",
    "questionText": "A news network uses Amazon Simple Storage Service (Amazon S3) to aggregate the raw video footage from its reporting teams across the US. The news network has recently expanded into new geographies in Europe and Asia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination Amazon S3 bucket.\n\nWhich of the following are the MOST cost-effective options to improve the file upload speed into Amazon S3 (Select two)",
    "options": [
      {
        "text": "Create multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Create multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket",
        "isCorrect": true
      },
      {
        "text": "Use multipart uploads for faster file uploads into the destination Amazon S3 bucket",
        "isCorrect": true
      },
      {
        "text": "Use AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nUse Amazon S3 Transfer Acceleration (Amazon S3TA) to enable faster file uploads into the destination S3 bucket\n\nAmazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Amazon S3TA takes advantage of Amazon CloudFrontâ€™s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\n\nUse multipart uploads for faster file uploads into the destination Amazon S3 bucket\n\nMultipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput, therefore it facilitates faster file uploads.\n\nIncorrect options:\n\nCreate multiple AWS Direct Connect connections between the AWS Cloud and branch offices in Europe and Asia. Use the direct connect connections for faster file uploads into Amazon S3 - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.\nDirect connect takes significant time (several months) to be provisioned and is an overkill for the given use-case.\n\nCreate multiple AWS Site-to-Site VPN connections between the AWS Cloud and branch offices in Europe and Asia. Use these VPN connections for faster file uploads into Amazon S3 - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet.\nVPN Connections are a good solution if you have low to modest bandwidth requirements and can tolerate the inherent variability in Internet-based connectivity. Site-to-site VPN will not help in accelerating the file transfer speeds into S3 for the given use-case.\n\nUse AWS Global Accelerator for faster file uploads into the destination Amazon S3 bucket - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. AWS Global Accelerator will not help in accelerating the file transfer speeds into S3 for the given use-case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q319",
    "questionText": "The IT department at a consulting firm is conducting a training workshop for new developers. As part of an evaluation exercise on Amazon S3, the new developers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3.\n\nCan you spot the INVALID lifecycle transitions from the options below? (Select two)",
    "options": [
      {
        "text": "Amazon S3 Intelligent-Tiering => Amazon S3 Standard",
        "isCorrect": true
      },
      {
        "text": "Amazon S3 One Zone-IA => Amazon S3 Standard-IA",
        "isCorrect": true
      },
      {
        "text": "Amazon S3 Standard => Amazon S3 Intelligent-Tiering",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Standard-IA => Amazon S3 One Zone-IA",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nAs the question wants to know about the INVALID lifecycle transitions, the following options are the correct answers -\n\nAmazon S3 Intelligent-Tiering => Amazon S3 Standard\n\nAmazon S3 One Zone-IA => Amazon S3 Standard-IA\n\nFollowing are the unsupported life cycle transitions for S3 storage classes -\nAny storage class to the Amazon S3 Standard storage class.\nAny storage class to the Reduced Redundancy storage class.\nThe Amazon S3 Intelligent-Tiering storage class to the Amazon S3 Standard-IA storage class.\nThe Amazon S3 One Zone-IA storage class to the Amazon S3 Standard-IA or Amazon S3 Intelligent-Tiering storage classes.\n\nIncorrect options:\n\nAmazon S3 Standard => Amazon S3 Intelligent-Tiering\n\nAmazon S3 Standard-IA => Amazon S3 Intelligent-Tiering\n\nAmazon S3 Standard-IA => Amazon S3 One Zone-IA\n\nHere are the supported life cycle transitions for S3 storage classes -\nThe S3 Standard storage class to any other storage class.\nAny storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes.\nThe S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes.\nThe S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class.\nThe S3 Glacier storage class to the S3 Glacier Deep Archive storage class.\n\nAmazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q320",
    "questionText": "A healthcare startup needs to enforce compliance and regulatory guidelines for objects stored in Amazon S3. One of the key requirements is to provide adequate protection against accidental deletion of objects.\n\nAs a solutions architect, what are your recommendations to address these guidelines? (Select two) ?",
    "options": [
      {
        "text": "Establish a process to get managerial approval for deleting Amazon S3 objects",
        "isCorrect": false
      },
      {
        "text": "Create an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager",
        "isCorrect": false
      },
      {
        "text": "Enable versioning on the Amazon S3 bucket",
        "isCorrect": true
      },
      {
        "text": "Change the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object",
        "isCorrect": false
      },
      {
        "text": "Enable multi-factor authentication (MFA) delete on the Amazon S3 bucket",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nEnable versioning on the Amazon S3 bucket\n\nVersioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket.\nVersioning-enabled buckets enable you to recover objects from accidental deletion or overwrite.\n\nFor example:\n\nIf you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.\nIf you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version. You can always restore the previous version. Hence, this is the correct option.\n\nVersioning Overview:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\n\nEnable multi-factor authentication (MFA) delete on the Amazon S3 bucket\n\nTo provide additional protection, multi-factor authentication (MFA) delete can be enabled. MFA delete requires secondary authentication to take place before objects can be permanently deleted from an Amazon S3 bucket. Hence, this is the correct option.\n\nIncorrect options:\n\nCreate an event trigger on deleting any Amazon S3 object. The event invokes an Amazon Simple Notification Service (Amazon SNS) notification via email to the IT manager - Sending an event trigger after object deletion does not meet the objective of preventing object deletion by mistake because the object has already been deleted. So, this option is incorrect.\n\nEstablish a process to get managerial approval for deleting Amazon S3 objects - This option for getting managerial approval is just a distractor.\n\nChange the configuration on Amazon S3 console so that the user needs to provide additional confirmation while deleting any Amazon S3 object - There is no provision to set up Amazon S3 configuration to ask for additional confirmation before deleting an object. This option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q321",
    "questionText": "A company runs a data processing workflow that takes about 60 minutes to complete. The workflow can withstand disruptions and it can be started and stopped multiple times.\n\nWhich is the most cost-effective solution to build a solution for the workflow?",
    "options": [
      {
        "text": "Use AWS Lambda function to run the workflow processes",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 on-demand instances to run the workflow processes",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 reserved instances to run the workflow processes",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 spot instances to run the workflow processes",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon EC2 spot instances to run the workflow processes\n\nAmazon EC2 instance types:\n\nvia - https://aws.amazon.com/ec2/pricing/\n\nAmazon EC2 Spot instances allow you to request spare Amazon EC2 computing capacity for up to 90% off the On-Demand price.\n\nSpot instances are recommended for:\n\nApplications that have flexible start and end times\nApplications that are feasible only at very low compute prices\nUsers with urgent computing needs for large amounts of additional capacity\n\nFor the given use case, spot instances offer the most cost-effective solution as the workflow can withstand disruptions and can be started and stopped multiple times.\n\nFor example, considering a process that runs for an hour and needs about 1024 MB of memory, spot instance pricing for a t2.micro instance (having 1024 MB of RAM) is $0.0035 per hour.\n\nSpot instance pricing:\n\nvia - https://aws.amazon.com/ec2/spot/pricing/\n\nContrast this with the pricing of a Lambda function (having 1024 MB of allocated memory), which comes out to $0.0000000167 per 1ms or $0.06 per hour ($0.0000000167 * 1000 * 60  * 60 per hour).\n\nAWS Lambda function pricing:\n\nvia - https://aws.amazon.com/lambda/pricing/\n\nThus, a spot instance turns out to be about 20 times cost effective than a Lambda function to meet the requirements of the given use case.\n\nIncorrect options:\n\nUse AWS Lambda function to run the workflow processes - As mentioned in the explanation above, a Lambda function turns out to be 20 times more expensive than a spot instance to meet the workflow requirements of the given use case, so this option is incorrect. You should also note that the maximum execution time of a Lambda function is 15 minutes, so the workflow process would be disrupted for sure. On the other hand, it is certainly possible that the workflow process can be completed in a single run on the spot instance (the average frequency of stop instance interruption across all Regions and instance types is <10%).\n\nUse Amazon EC2 on-demand instances to run the workflow processes\n\nUse Amazon EC2 reserved instances to run the workflow processes\n\nYou should note that both on-demand and reserved instances are more expensive than spot instances. In addition, reserved instances have a term of 1 year or 3 years, so they are not suited for the given workflow. Therefore, both these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/ec2/pricing/\n\nhttps://aws.amazon.com/ec2/spot/pricing/\n\nhttps://aws.amazon.com/lambda/pricing/\n\nhttps://aws.amazon.com/ec2/spot/instance-advisor/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q322",
    "questionText": "A US-based healthcare startup is building an interactive diagnostic tool for COVID-19 related assessments. The users would be required to capture their personal health records via this tool. As this is sensitive health information, the backup of the user data must be kept encrypted in Amazon Simple Storage Service (Amazon S3). The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom.\n\nWhich of the following is the BEST solution for this use-case?",
    "options": [
      {
        "text": "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3",
        "isCorrect": true
      },
      {
        "text": "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3\n\nAWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created.\nSSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Therefore SSE-KMS is the correct solution for this use-case.\n\nServer Side Encryption in S3:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\n\nIncorrect options:\n\nUse server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3 - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However this option does not provide the ability to audit trail the usage of the encryption keys.\n\nUse server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3 - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. However this option does not provide the ability to audit trail the usage of the encryption keys.\n\nUse client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3 - Using client-side encryption is ruled out as the startup does not want to provide the encryption keys.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q323",
    "questionText": "An ivy-league university is assisting NASA to find potential landing sites for exploration vehicles of unmanned missions to our neighboring planets. The university uses High Performance Computing (HPC) driven application architecture to identify these landing sites.\n\nWhich of the following Amazon EC2 instance topologies should this application be deployed on?",
    "options": [
      {
        "text": "The Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures",
        "isCorrect": false
      },
      {
        "text": "The Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively",
        "isCorrect": false
      },
      {
        "text": "The Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput",
        "isCorrect": true
      },
      {
        "text": "The Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThe Amazon EC2 instances should be deployed in a cluster placement group so that the underlying workload can benefit from low network latency and high network throughput\n\nThe key thing to understand in this question is that HPC workloads need to achieve low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Cluster placement groups pack instances close together inside an Availability Zone. These are recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is the correct answer.\n\nCluster Placement Group:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nIncorrect options:\n\nThe Amazon EC2 instances should be deployed in a partition placement group so that distributed workloads can be handled effectively - A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone.\nSince a partition placement group can have partitions in multiple Availability Zones in the same region, therefore instances will not have low-latency network performance. Hence the partition placement group is not the right fit for HPC applications.\n\nPartition Placement Group:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nThe Amazon EC2 instances should be deployed in a spread placement group so that there are no correlated failures - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since a spread placement group can span multiple Availability Zones in the same Region, therefore instances will not have low-latency network performance. Hence spread placement group is not the right fit for HPC applications.\n\nSpread Placement Group:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nThe Amazon EC2 instances should be deployed in an Auto Scaling group so that application meets high availability requirements - An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling. You do not use Auto Scaling groups per se to meet HPC requirements.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q324",
    "questionText": "An Electronic Design Automation (EDA) application produces massive volumes of data that can be divided into two categories. The 'hot data' needs to be both processed and stored quickly in a parallel and distributed fashion. The 'cold data' needs to be kept for reference with quick access for reads and updates at a low cost.\n\nWhich of the following AWS services is BEST suited to accelerate the aforementioned chip design process?",
    "options": [
      {
        "text": "Amazon FSx for Windows File Server",
        "isCorrect": false
      },
      {
        "text": "Amazon EMR",
        "isCorrect": false
      },
      {
        "text": "Amazon FSx for Lustre",
        "isCorrect": true
      },
      {
        "text": "AWS Glue",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon FSx for Lustre\n\nAmazon FSx for Lustre makes it easy and cost-effective to launch and run the worldâ€™s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. The open-source Lustre file system is designed for applications that require fast storage â€“ where you want your storage to keep up with your compute. FSx for Lustre integrates with Amazon S3, making it easy to process data sets with the Lustre file system. When linked to an S3 bucket, an FSx for Lustre file system transparently presents S3 objects as files and allows you to write changed data back to S3.\n\nFSx for Lustre provides the ability to both process the 'hot data' in a parallel and distributed fashion as well as easily store the 'cold data' on Amazon S3. Therefore this option is the BEST fit for the given problem statement.\n\nIncorrect options:\n\nAmazon FSx for Windows File Server - Amazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol.  It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\nFSx for Windows does not allow you to present S3 objects as files and does not allow you to write changed data back to S3. Therefore you cannot reference the \"cold data\" with quick access for reads and updates at low cost. Hence this option is not correct.\n\nAmazon EMR - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nEMR does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario.\n\nAWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.\nAWS Glue does not offer the same storage and processing speed as FSx for Lustre. So it is not the right fit for the given high-performance workflow scenario.\n\nReferences:\n\nhttps://aws.amazon.com/fsx/lustre/\n\nhttps://aws.amazon.com/fsx/windows/faqs/",
    "awsService": "EMR",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q325",
    "questionText": "An IT company wants to review its security best-practices after an incident was reported where a new developer on the team was assigned full access to Amazon DynamoDB. The developer accidentally deleted a couple of tables from the production environment while building out a new feature.\n\nWhich is the MOST effective way to address this issue so that such incidents do not recur?",
    "options": [
      {
        "text": "The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur",
        "isCorrect": false
      },
      {
        "text": "Remove full database access for all IAM users in the organization",
        "isCorrect": false
      },
      {
        "text": "Only root user should have full database access in the organization",
        "isCorrect": false
      },
      {
        "text": "Use permissions boundary to control the maximum permissions employees can grant to the IAM principals",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse permissions boundary to control the maximum permissions employees can grant to the IAM principals\n\nA permissions boundary can be used to control the maximum permissions employees can grant to the IAM principals (that is, users and roles) that they create and manage. As the IAM administrator, you can define one or more permissions boundaries using managed policies and allow your employee to create a principal with this boundary. The employee can then attach a permissions policy to this principal. However, the effective permissions of the principal are the intersection of the permissions boundary and permissions policy. As a result, the new principal cannot exceed the boundary that you defined. Therefore, using the permissions boundary offers the right solution for this use-case.\n\nPermission Boundary Example:\n\nvia - https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/\n\nIncorrect options:\n\nRemove full database access for all IAM users in the organization - It is not practical to remove full access for all IAM users in the organization because a select set of users need this access for database administration. So this option is not correct.\n\nThe CTO should review the permissions for each new developer's IAM user so that such incidents don't recur -  Likewise the CTO is not expected to review the permissions for each new developer's IAM user, as this is best done via an automated procedure. This option has been added as a distractor.\n\nOnly root user should have full database access in the organization - As a best practice, the root user should not access the AWS account to carry out any administrative procedures. So this option is not correct.\n\nReference:\n\nhttps://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/",
    "awsService": "DynamoDB",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q326",
    "questionText": "A retail analytics company operates a large-scale data lake on Amazon S3, where they store daily logs of customer transactions, product views, and inventory updates. Each morning, they need to transform and load the data into a data warehouse to support fast analytical queries. The company also wants to enable data analysts to build and train machine learning (ML) models using familiar SQL syntax without writing custom Python code. The architecture must support massively parallel processing (MPP) for fast data aggregation and scoring, and must use serverless AWS services wherever possible to reduce infrastructure management and operational overhead.\n\nWhich solution best meets these requirements?",
    "options": [
      {
        "text": "Use a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models",
        "isCorrect": true
      },
      {
        "text": "Provision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks",
        "isCorrect": false
      },
      {
        "text": "Use an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes",
        "isCorrect": false
      },
      {
        "text": "Run a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse a daily AWS Glue job to transform and clean the data stored in Amazon S3. Load the transformed dataset into Amazon Redshift Serverless, which offers MPP capabilities in a serverless model. Enable analysts to use Amazon Redshift ML to build and train ML models\n\nThis solution is fully serverless and optimized for high-performance analytics and ML. AWS Glue provides serverless ETL, and Amazon Redshift Serverless offers a MPP data warehouse that scales compute independently. Amazon Redshift ML lets analysts create, train, and deploy ML models using SQL by integrating with Amazon SageMaker under the hood â€” abstracting away the complexity of building models. This architecture meets all core requirements: serverless, SQL-based ML, and scalable MPP analytics.\n\n\nvia - https://aws.amazon.com/blogs/big-data/create-train-and-deploy-machine-learning-models-in-amazon-redshift-using-sql-with-amazon-redshift-ml/\n\nIncorrect options:\n\nProvision and run a daily Amazon EMR cluster with Apache Spark to process and transform the S3 data. Load the results into Amazon Redshift (provisioned). Enable ML model development by integrating Redshift with Amazon SageMaker notebooks for advanced modeling tasks - While EMR is suitable for large-scale transformation using Spark and Redshift supports MPP, this solution requires managing both EMR clusters and Redshift clusters. Additionally, ML modeling via SageMaker notebooks requires Python coding skills and breaks the requirement for SQL-based ML. This option introduces higher operational overhead and deviates from the serverless and SQL-only requirements.\n\nRun a daily AWS Glue job to process and transform the raw files in S3 and register the outputs as Amazon Athena tables in AWS Glue Data Catalog. Allow analysts to build ML models using Amazon Athena ML, with SQL-based predictions on top of S3 data without moving it to a warehouse - Although AWS Glue offers serverless ETL and RDS for PostgreSQL is widely used, RDS is not a MPP data warehouse, and performance may not scale well for large, concurrent analytical queries. While Aurora ML allows calling SageMaker models from SQL, itâ€™s designed for transactional workloads, not large-scale ML workflows. This approach doesn't align with the analytics and MPP scale required by the company.\n\nUse an AWS Glue job to transform and load data into Amazon RDS for PostgreSQL. Allow analysts to run machine learning models using Amazon Aurora ML integrated with PostgreSQL, leveraging Amazon SageMaker endpoints behind the scenes - Amazon Athena is serverless and can run SQL queries on S3-based data using the Glue Data Catalog, and Athena ML allows applying SageMaker models to query outputs. However, Athena does not support MPP-level performance for large-scale data warehousing workloads and is primarily designed for ad hoc querying. Additionally, Athena ML is limited to inference only, not for training ML models via SQL.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/big-data/create-train-and-deploy-machine-learning-models-in-amazon-redshift-using-sql-with-amazon-redshift-ml/\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/serverless-whatis.html\n\nhttps://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html\n\nhttps://github.com/awslabs/aws-athena-query-federation/wiki/AthenaML-Tutorial",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q327",
    "questionText": "A financial services company operates a containerized microservices architecture using Kubernetes in its on-premises data center. Due to strict industry regulations and internal security policies, all application data and workloads must remain physically within the on-premises environment. The companyâ€™s infrastructure team wants to modernize its Kubernetes stack and take advantage of AWS-managed services and APIs, including automated Kubernetes upgrades, Amazon CloudWatch integration, and access to AWS IAM features â€” but without migrating any data or compute resources to the cloud.\n\nWhich AWS solution will best meet the companyâ€™s requirements for modernization while ensuring that all data remains on premises?",
    "options": [
      {
        "text": "Set up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads",
        "isCorrect": false
      },
      {
        "text": "Deploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the companyâ€™s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share",
        "isCorrect": false
      },
      {
        "text": "Use an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowballâ€™s offline data transfer features. Use the Snowball console to orchestrate workloads in batches",
        "isCorrect": false
      },
      {
        "text": "Install an AWS Outposts rack in the companyâ€™s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nInstall an AWS Outposts rack in the companyâ€™s data center. Use Amazon EKS Anywhere on Outposts to run containerized workloads locally while integrating with AWS APIs\n\nThis is the only option that allows the company to run Amazon EKS-compatible Kubernetes clusters entirely on premises while maintaining full data residency and regulatory compliance. AWS Outposts Rack brings native AWS services, including EKS, EC2, S3, and CloudWatch, into the customerâ€™s local data center. mazon EKS Anywhere is a deployment option for Amazon Elastic Kubernetes Service (EKS) that allows you to create and manage Kubernetes clusters on your own infrastructure â€” such as on-premises servers or edge locations, outside of AWS. With EKS Anywhere running on Outposts, the company can modernize its Kubernetes stack with familiar AWS tooling without sending any data to the cloud.\n\n\nvia - https://aws.amazon.com/blogs/big-data/create-train-and-deploy-machine-learning-models-in-amazon-redshift-using-sql-with-amazon-redshift-ml/\n\n\nvia - https://aws.amazon.com/blogs/big-data/create-train-and-deploy-machine-learning-models-in-amazon-redshift-using-sql-with-amazon-redshift-ml/\n\nIncorrect options:\n\nSet up a dedicated AWS Direct Connect connection between the on-premises environment and an AWS Region. Deploy Amazon EKS in the cloud and connect it to the local Kubernetes cluster. Use IAM roles and API Gateway to integrate authentication and traffic flow for hybrid workloads -  Although AWS Direct Connect provides a private, low-latency connection between an on-prem data center and AWS, it does not meet data residency requirements, as workloads and data would still reside in the cloud. The use of Amazon EKS in the cloud violates the requirement to keep all workloads and data on premises. This hybrid model is suitable for low-sensitivity environments but not for compliance-restricted industries.\n\nDeploy Amazon ECS with Fargate in a nearby AWS Local Zone. Use CloudWatch Logs to forward events to the primary region. Connect the Local Zone to the companyâ€™s data center over a VPN. Configure containers to pull data from on-premises storage through a mounted file share - AWS Local Zones are designed to place compute, storage, and select services closer to end-users in metro areas but still operate outside the customerâ€™s data center and are managed by AWS. Using ECS in a Local Zone does not satisfy the strict requirement that all data remain physically in the companyâ€™s data center. Additionally, ECS is not Kubernetes-based, and it doesnâ€™t support the same operational model as EKS.\n\nUse an AWS Snowball Edge Compute Optimized device to run EKS-compatible Docker containers on-site. Periodically export application logs and container snapshots to Amazon S3 using Snowballâ€™s offline data transfer features. Use the Snowball console to orchestrate workloads in batches - While Snowball Edge can run containerized applications using AWS IoT Greengrass or custom scripts, it is not intended for persistent production workloads or as a long-term infrastructure solution. Its primary use cases include offline data collection, edge processing, and bulk transfer to AWS. It lacks the scalability, lifecycle management, and Kubernetes support needed for this scenario.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/containers/fully-private-local-clusters-for-amazon-eks-on-aws-outposts-powered-by-vpc-endpoints/\n\nhttps://aws.amazon.com/eks/eks-anywhere/\n\nhttps://aws.amazon.com/outposts/rack/\n\nhttps://aws.amazon.com/about-aws/global-infrastructure/localzones/\n\nhttps://docs.aws.amazon.com/snowball/latest/developer-guide/whatisedge.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q328",
    "questionText": "A leading video streaming service delivers billions of hours of content from Amazon Simple Storage Service (Amazon S3) to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline.\n\nWhich of the following is the MOST cost-effective strategy for storing this intermediary query data?",
    "options": [
      {
        "text": "Store the intermediary query results in Amazon S3 Standard storage class",
        "isCorrect": true
      },
      {
        "text": "Store the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class",
        "isCorrect": false
      },
      {
        "text": "Store the intermediary query results in Amazon S3 Standard-Infrequent Access storage class",
        "isCorrect": false
      },
      {
        "text": "Store the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nStore the intermediary query results in Amazon S3 Standard storage class\n\nAmazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nAs there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options.\n\nIncorrect options:\n\nStore the intermediary query results in Amazon S3 Glacier Instant Retrieval storage class - Amazon S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives.\n\nThe minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.\n\nStore the intermediary query results in Amazon S3 Standard-Infrequent Access storage class - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.\n\nStore the intermediary query results in Amazon S3 One Zone-Infrequent Access storage class - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.\n\nTo summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost optimal for the given use-case.\n\nReference:\n\nhttps://aws.amazon.com/s3/storage-classes/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q329",
    "questionText": "The engineering team at an in-home fitness company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home.\n\nAs a solutions architect, which of the following solutions would you recommend? (Select two)",
    "options": [
      {
        "text": "Power the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements",
        "isCorrect": false
      },
      {
        "text": "Power the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements",
        "isCorrect": true
      },
      {
        "text": "Power the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements",
        "isCorrect": true
      },
      {
        "text": "Power the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements",
        "isCorrect": false
      },
      {
        "text": "Power the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nPower the on-demand, live leaderboard using Amazon ElastiCache for Redis as it meets the in-memory, high availability, low latency requirements\n\nAmazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\nElastiCache for Redis can be used to power the live leaderboard, so this option is correct.\n\nAmazon ElastiCache for Redis Overview:\n\n\nPower the on-demand, live leaderboard using Amazon DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements\n\nAmazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.\nDAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard.\n\nIncorrect options:\n\nPower the on-demand, live leaderboard using Amazon Neptune as it meets the in-memory, high availability, low latency requirements - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune is not an in-memory database, so this option is not correct.\n\nPower the on-demand, live leaderboard using Amazon DynamoDB as it meets the in-memory, high availability, low latency requirements - DynamoDB is not an in-memory database, so this option is not correct.\n\nPower the on-demand, live leaderboard using Amazon RDS for Aurora as it meets the in-memory, high availability, low latency requirements - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database, so this option is not correct.\n\nReferences:\n\nhttps://aws.amazon.com/elasticache/\n\nhttps://aws.amazon.com/elasticache/redis/\n\nhttps://aws.amazon.com/dynamodb/dax/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q330",
    "questionText": "The engineering team at a data analytics company has observed that its flagship application functions at its peak performance when the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances have a CPU utilization of about 50%. The application is built on a fleet of Amazon EC2 instances managed under an Auto Scaling group. The workflow requests are handled by an internal Application Load Balancer that routes the requests to the instances.\n\nAs a solutions architect, what would you recommend so that the application runs near its peak performance state?",
    "options": [
      {
        "text": "Configure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%",
        "isCorrect": true
      },
      {
        "text": "Configure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "isCorrect": false
      },
      {
        "text": "Configure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%",
        "isCorrect": false
      },
      {
        "text": "Configure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50%",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure the Auto Scaling group to use target tracking policy and set the CPU utilization as the target metric with a target value of 50%\n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies.\n\nWith target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value.\n\nFor example, you can use target tracking scaling to:\n\nConfigure a target tracking scaling policy to keep the average aggregate CPU utilization of your Auto Scaling group at 50 percent. This meets the requirements specified in the given use-case and therefore, this is the correct option.\n\nTarget Tracking Policy Overview:\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\n\nIncorrect options:\n\nConfigure the Auto Scaling group to use step scaling policy and set the CPU utilization as the target metric with a target value of 50%\n\nConfigure the Auto Scaling group to use simple scaling policy and set the CPU utilization as the target metric with a target value of 50%\n\nWith step scaling and simple scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process.\nNeither step scaling nor simple scaling can be configured to use a target metric for CPU utilization, hence both these options are incorrect.\n\nConfigure the Auto Scaling group to use a Amazon Cloudwatch alarm triggered on a CPU utilization threshold of 50% - An Auto Scaling group cannot directly use a Cloudwatch alarm as the source for a scale-in or scale-out event, hence this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q331",
    "questionText": "An IT consultant is helping the owner of a medium-sized business set up an AWS account. What are the security recommendations he must follow while creating the AWS account root user? (Select two)",
    "options": [
      {
        "text": "Create a strong password for the AWS account root user",
        "isCorrect": true
      },
      {
        "text": "Encrypt the access keys and save them on Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Create AWS account root user access keys and share those keys only with the business owner",
        "isCorrect": false
      },
      {
        "text": "Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future",
        "isCorrect": false
      },
      {
        "text": "Enable Multi Factor Authentication (MFA) for the AWS account root user account",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nCreate a strong password for the AWS account root user\n\nEnable Multi Factor Authentication (MFA) for the AWS account root user account\n\nHere are some of the best practices while creating an AWS account root user:\n\n1) Use a strong password to help protect account-level access to the AWS Management Console.\n2) Never share your AWS account root user password or access keys with anyone.\n3) If you do have an access key for your AWS account root user, delete it. If you must keep it, rotate (change) the access key regularly. You should not encrypt the access keys and save them on Amazon S3.\n4) If you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to.\n5) Enable AWS multi-factor authentication (MFA) on your AWS account root user account.\n\nAWS Root Account Security Best Practices:\n\nvia - https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\n\nIncorrect options:\n\nEncrypt the access keys and save them on Amazon S3 - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Even an encrypted access key for the root user poses a significant security risk. Therefore, this option is incorrect.\n\nCreate AWS account root user access keys and share those keys only with the business owner - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Hence, this option is incorrect.\n\nSend an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future - AWS recommends that you should never share your AWS account root user password or access keys with anyone. Sending an email with AWS account root user credentials creates a security risk as it can be misused by anyone reading the email. Hence, this option is incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q332",
    "questionText": "The DevOps team at an e-commerce company wants to perform some maintenance work on a specific Amazon EC2 instance that is part of an Auto Scaling group using a step scaling policy. The team is facing a maintenance challenge - every time the team deploys a maintenance patch, the instance health check status shows as out of service for a few minutes. This causes the Auto Scaling group to provision another replacement instance immediately.\n\nAs a solutions architect, which are the MOST time/resource efficient steps that you would recommend so that the maintenance work can be completed at the earliest? (Select two)",
    "options": [
      {
        "text": "Put the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service",
        "isCorrect": true
      },
      {
        "text": "Take a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue",
        "isCorrect": false
      },
      {
        "text": "Delete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy",
        "isCorrect": false
      },
      {
        "text": "Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again",
        "isCorrect": true
      },
      {
        "text": "Suspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nPut the instance into the Standby state and then update the instance by applying the maintenance patch. Once the instance is ready, you can exit the Standby state and then return the instance to service - You can put an instance that is in the InService state into the Standby state, update some software or troubleshoot the instance, and then return the instance to service. Instances that are on standby are still part of the Auto Scaling group, but they do not actively handle application traffic.\n\nHow Standby State Works:\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html\n\nSuspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can manually set the instance's health status back to healthy and activate the ReplaceUnhealthy process type again - The ReplaceUnhealthy process terminates instances that are marked as unhealthy and then creates new instances to replace them. Amazon EC2 Auto Scaling stops replacing instances that are marked as unhealthy. Instances that fail EC2 or Elastic Load Balancing health checks are still marked as unhealthy. As soon as you resume the ReplaceUnhealthly process, Amazon EC2 Auto Scaling replaces instances that were marked unhealthy while this process was suspended.\n\nIncorrect options:\n\nTake a snapshot of the instance, create a new Amazon Machine Image (AMI) and then launch a new instance using this AMI. Apply the maintenance patch to this new instance and then add it back to the Auto Scaling Group by using the manual scaling policy. Terminate the earlier instance that had the maintenance issue - Taking the snapshot of the existing instance to create a new AMI and then creating a new instance in order to apply the maintenance patch is not time/resource optimal, hence this option is ruled out.\n\nDelete the Auto Scaling group and apply the maintenance fix to the given instance. Create a new Auto Scaling group and add all the instances again using the manual scaling policy - It's not recommended to delete the Auto Scaling group just to apply a maintenance patch on a specific instance.\n\nSuspend the ScheduledActions process type for the Auto Scaling group and apply the maintenance patch to the instance. Once the instance is ready, you can you can manually set the instance's health status back to healthy and activate the ScheduledActions process type again - Amazon EC2 Auto Scaling does not execute scaling actions that are scheduled to run during the suspension period. This option is not relevant to the given use-case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/health-checks-overview.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q333",
    "questionText": "A leading carmaker would like to build a new car-as-a-sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the carmaker does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data.\n\nGiven these constraints, which of the following solutions is the BEST fit to develop this car-as-a-sensor service?",
    "options": [
      {
        "text": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "isCorrect": true
      },
      {
        "text": "Ingest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "isCorrect": false
      },
      {
        "text": "Ingest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing",
        "isCorrect": false
      },
      {
        "text": "Ingest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nIngest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an AWS Lambda function in batches and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\nAWS manages all ongoing operations and underlying infrastructure needed to provide a highly available and scalable message queuing service. With SQS, there is no upfront cost, no need to acquire, install, and configure messaging software, and no time-consuming build-out and maintenance of supporting infrastructure. SQS queues are dynamically created and scale automatically so you can build and grow applications quickly and efficiently.\n\nAs there is no need to manually provision the capacity, so this is the correct option.\n\nIncorrect options:\n\nIngest the sensor data in Amazon Kinesis Data Firehose, which directly writes the data into an auto-scaled Amazon DynamoDB table for downstream processing -Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic.\n\nFirehose cannot directly write into a DynamoDB table, so this option is incorrect.\n\nIngest the sensor data in an Amazon Simple Queue Service (Amazon SQS) standard queue, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nIngest the sensor data in Amazon Kinesis Data Streams, which is polled by an application running on an Amazon EC2 instance and the data is written into an auto-scaled Amazon DynamoDB table for downstream processing\n\nUsing an application on an Amazon EC2 instance is ruled out as the carmaker wants to use fully serverless components. So both these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/sqs/\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\n\nhttps://aws.amazon.com/kinesis/data-streams/faqs/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q334",
    "questionText": "A major bank is using Amazon Simple Queue Service (Amazon SQS) to migrate several core banking applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the bank expects a peak rate of about 1000 messages per second to be processed via SQS. It is important that the messages are processed in order.\n\nWhich of the following options can be used to implement this system?",
    "options": [
      {
        "text": "Use Amazon SQS standard queue to process the messages",
        "isCorrect": false
      },
      {
        "text": "Use Amazon SQS FIFO (First-In-First-Out) queue to process the messages",
        "isCorrect": false
      },
      {
        "text": "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate",
        "isCorrect": true
      },
      {
        "text": "Use Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 4 messages per operation to process the messages at the peak rate\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues.\n\nFor FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent.\n\nBy default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. Therefore you need to process 4 messages per operation so that the FIFO queue can support up to 1200 messages per second, which is well within the peak rate.\n\nFIFO Queues Overview:\n\nvia - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\n\nIncorrect options:\n\nUse Amazon SQS standard queue to process the messages - As messages need to be processed in order, therefore standard queues are ruled out.\n\nUse Amazon SQS FIFO (First-In-First-Out) queue to process the messages - By default, FIFO queues support up to 300 messages per second and this is not sufficient to meet the message processing throughput per the given use-case. Hence this option is incorrect.\n\nUse Amazon SQS FIFO (First-In-First-Out) queue in batch mode of 2 messages per operation to process the messages at the peak rate - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 4 messages per operation, so that the FIFO queue can support up to 1200 messages per second. With 2 messages per operation, you can only support up to 600 messages per second.\n\nReferences:\n\nhttps://aws.amazon.com/sqs/\n\nhttps://aws.amazon.com/sqs/features/",
    "awsService": "SQS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q335",
    "questionText": "A video analytics company runs data-intensive batch processing workloads that generate large log files and metadata daily. These files are currently stored in an on-premises NFS-based storage system located in the company's primary data center. However, the storage system is becoming increasingly difficult to scale and is unable to meet the company's growing storage demands. The IT team wants to migrate to a cloud-based storage solution that minimizes costs, retains NFS compatibility, and supports automated tiering of rarely accessed data to lower-cost storage. The team prefers to continue using existing NFS-based tools and protocols for compatibility with their current application stack.\n\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "text": "Provision an Amazon Elastic File System (Amazon EFS) file system with the One Zoneâ€“IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files",
        "isCorrect": false
      },
      {
        "text": "Deploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering",
        "isCorrect": false
      },
      {
        "text": "Deploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes",
        "isCorrect": true
      },
      {
        "text": "Use Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDeploy an AWS Storage Gateway File Gateway on premises. Configure it to present an NFS-compatible file share to the workloads. Store the uploaded files in Amazon S3, and use S3 Lifecycle policies to automatically transition infrequently accessed objects to lower-cost storage classes\n\nThis is the most cost-effective and operationally seamless solution. File Gateway presents an NFS-compatible interface to on-premises applications, allowing the existing workloads to write to it as they would to a traditional NFS server. Behind the scenes, the data is stored in Amazon S3, allowing the company to leverage S3's virtually unlimited scalability. With S3 Lifecycle policies, files can be automatically transitioned to low-cost storage classes like S3 Standard-IA or Glacier, further optimizing costs over time. This approach requires minimal changes to applications and eliminates the scalability bottleneck.\n\nHow File Gateway Works:\n\nvia - https://docs.aws.amazon.com/filegateway/latest/files3/file-gateway-concepts.html\n\nIncorrect options:\n\nProvision an Amazon Elastic File System (Amazon EFS) file system with the One Zoneâ€“IA storage class. Use AWS DataSync to migrate the NFS data to EFS. Configure the application to mount the file system over NFS and activate lifecycle management to tier infrequently accessed files - Amazon EFS is designed for shared file systems in the cloud, and while it supports NFS and lifecycle management, it is not cost-optimal for hybrid setups where on-prem NFS compatibility is required. Additionally, One Zone-IA stores data in a single Availability Zone, making it less resilient. This solution also assumes a full data migration and reconfiguration of the applications, increasing complexity and operational overhead.\n\nDeploy an AWS Storage Gateway Volume Gateway in cached mode. Attach it as a block device to an on-premises file server and mount NFS on top. Store snapshots in Amazon S3 Glacier Deep Archive, and use AWS Backup to manage recovery operations and tiering - Volume Gateway provides iSCSI block storage, not file-based access, meaning you would have to mount NFS over a block volume, which is not ideal for compatibility or scalability. Although Volume Gateway can store snapshots in S3 and Glacier, it's not optimized for NFS workloads or direct object access via S3. It also lacks native support for S3 lifecycle tiering, relying instead on backup and snapshot mechanisms.\n\nHow Volume Gateway Works:\n\nvia - https://docs.aws.amazon.com/storagegateway/latest/vgw/StorageGatewayConcepts.html\n\nUse Amazon FSx for Windows File Server to replace the NFS workload. Enable data deduplication and automatic backups. Use Amazon S3 Glacier to move snapshots to a cost-efficient storage tier. Reconfigure the analytics application to access files using SMB protocol - Amazon FSx for Windows File Server is intended for Windows-based SMB workloads, not NFS-based environments. It offers useful features like deduplication and backups, but it requires the application to be rewritten or reconfigured to use SMB instead of NFS. Moreover, its integration with Glacier is only applicable to backups and not individual files, and it does not support S3 lifecycle tiering, making it less cost-efficient.\n\nReferences:\n\nhttps://docs.aws.amazon.com/filegateway/latest/files3/what-is-file-s3.html\n\nhttps://docs.aws.amazon.com/filegateway/latest/files3/file-gateway-concepts.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q336",
    "questionText": "An IT security consultancy is working on a solution to protect data stored in Amazon S3 from any malicious activity as well as check for any vulnerabilities on Amazon EC2 instances.\n\nAs a solutions architect, which of the following solutions would you suggest to help address the given requirement?",
    "options": [
      {
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "isCorrect": true
      },
      {
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances\n\nAmazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.\n\nHow Amazon GuardDuty works:\n\nvia - https://aws.amazon.com/guardduty/\n\nAmazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Amazon Inspector assessments are offered to you as pre-defined rules packages mapped to common security best practices and vulnerability definitions.\n\nIncorrect options:\n\nUse Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances\n\nUse Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances\n\nUse Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/guardduty/\n\nhttps://aws.amazon.com/inspector/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q337",
    "questionText": "The engineering team at an e-commerce company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection.\n\nAs a solutions architect, which of the following solutions would you recommend to the company?",
    "options": [
      {
        "text": "Use AWS Direct Connect to establish a connection between the data center and AWS Cloud",
        "isCorrect": false
      },
      {
        "text": "Use AWS site-to-site VPN to establish a connection between the data center and AWS Cloud",
        "isCorrect": false
      },
      {
        "text": "Use AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud",
        "isCorrect": true
      },
      {
        "text": "Use AWS Transit Gateway to establish a connection between the data center and AWS Cloud",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Direct Connect plus virtual private network (VPN) to establish a connection between the data center and AWS Cloud\n\nAWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.\n\nWith AWS Direct Connect plus VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.\n\nThis solution combines the AWS managed benefits of the VPN solution with low latency, increased bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection. Therefore, AWS Direct Connect plus VPN is the correct solution for this use-case.\n\nAWS Direct Connect Plus VPN:\n\nvia - https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\n\nIncorrect options:\n\nUse AWS site-to-site VPN to establish a connection between the data center and AWS Cloud - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.\nHowever, Site-to-site VPN cannot provide low latency and high throughput connection, therefore this option is ruled out.\n\nUse AWS Transit Gateway to establish a connection between the data center and AWS Cloud - AWS Transit Gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. AWS Transit Gateway by itself cannot establish a low latency and high throughput connection between a data center and AWS Cloud. Hence this option is incorrect.\n\nUse AWS Direct Connect to establish a connection between the data center and AWS Cloud - AWS Direct Connect by itself cannot provide an encrypted connection between a data center and AWS Cloud, so this option is ruled out.\n\nReferences:\n\nhttps://aws.amazon.com/directconnect/\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html",
    "awsService": "General",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q338",
    "questionText": "One of the biggest football leagues in Europe has granted the distribution rights for live streaming its matches in the USA to a silicon valley based streaming services company. As per the terms of distribution, the company must make sure that only users from the USA are able to live stream the matches on their platform. Users from other countries in the world must be denied access to these live-streamed matches.\n\nWhich of the following options would allow the company to enforce these streaming restrictions? (Select two)",
    "options": [
      {
        "text": "Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights",
        "isCorrect": true
      },
      {
        "text": "Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nUse Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights\n\nGeolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights.\n\nUse georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution\n\nYou can use georestriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution. When a user requests your content, Amazon CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following:\nAllow your users to access your content only if they're in one of the countries on a whitelist of approved countries.\nPrevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. So this option is also correct.\n\nAmazon Route 53 Routing Policy Overview:\n\nvia - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nIncorrect options:\n\nUse Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights - Use latency-based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server.\n\nUse Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software.\n\nUse Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records.\n\nWeighted routing or failover routing or latency routing cannot be used to restrict the distribution of content to only the locations in which you have distribution rights. So all three options above are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo",
    "awsService": "Route 53",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q339",
    "questionText": "A biotech research company needs to perform data analytics on real-time lab results provided by a partner organization. The partner stores these lab results in an Amazon RDS for MySQL instance within the partnerâ€™s own AWS account. The research company has a private VPC that does not have internet access, Direct Connect, or a VPN connection. However, the company must establish secure and private connectivity to the RDS database in the partnerâ€™s VPC. The solution must allow the research company to connect from its VPC while minimizing complexity and complying with data security requirements.\n\nWhich solution will meet these requirements?",
    "options": [
      {
        "text": "Instruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the companyâ€™s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet",
        "isCorrect": false
      },
      {
        "text": "Set up VPC peering between the companyâ€™s VPC and the partnerâ€™s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the companyâ€™s VPC to the database. Modify the RDS subnet route tables to allow access from the companyâ€™s CIDR block",
        "isCorrect": false
      },
      {
        "text": "Configure a client VPN endpoint in the companyâ€™s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partnerâ€™s VPC and route RDS traffic via this connection",
        "isCorrect": false
      },
      {
        "text": "Instruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research companyâ€™s VPC",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nInstruct the partner to create a Network Load Balancer (NLB) in front of the Amazon RDS for MySQL instance. Use AWS PrivateLink to expose the NLB as an interface VPC endpoint in the research companyâ€™s VPC\n\nThis is the correct and most secure solution. Since the RDS instance cannot be directly exposed via PrivateLink, the partner can create a Network Load Balancer (NLB) in front of a proxy or custom endpoint that connects to RDS (e.g., using RDS Proxy or an EC2-based MySQL proxy). The partner then creates a PrivateLink service (interface endpoint service) that the research company can connect to privately via an interface VPC endpoint in its own VPC. This architecture keeps traffic within the AWS network without requiring an internet gateway, VPN, or Direct Connect.\n\n\nvia - https://docs.aws.amazon.com/vpc/latest/privatelink/what-is-privatelink.html\n\nShare your services through AWS PrivateLink:\n\nvia - https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-share-your-services.html\n\nIncorrect options:\n\nInstruct the partner to enable public access on the Amazon RDS instance and add a security group rule to allow inbound access from the companyâ€™s IP range. The company accesses the database over the public internet through a NAT Gateway configured in a private subnet - This option violates the requirement for no internet access in the company's VPC. Publicly exposing RDS by enabling public access and accessing it via NAT over the internet also raises serious security and compliance concerns. Furthermore, a NAT Gateway cannot exist in a subnet that has no internet gateway, making this solution infeasible in the given environment.\n\nSet up VPC peering between the companyâ€™s VPC and the partnerâ€™s VPC. Use AWS Transit Gateway in the partner's account to route traffic from the companyâ€™s VPC to the database. Modify the RDS subnet route tables to allow access from the companyâ€™s CIDR block - VPC peering allows direct private communication, but you cannot use Transit Gateway and VPC peering together for a single pair of VPCsâ€”this is an architectural contradiction. You either use Transit Gateway to route traffic between VPCs or use VPC peeringâ€”not both. Additionally, RDS traffic routing through Transit Gateway requires more complex configuration and is unnecessary in this scenario.\n\nConfigure a client VPN endpoint in the companyâ€™s account. Have researchers connect to the VPN from their local machines. Establish a Direct Connect gateway to the partnerâ€™s VPC and route RDS traffic via this connection - This option introduces multiple incorrect assumptions. First, Direct Connect is not available, as per the questionâ€™s conditions. Second, Client VPN is for user-based access, not VPC-to-VPC communication. This solution involves unnecessary complexity, adds VPN management, and fails to satisfy the requirement of connecting from the VPC itself to the RDS instance.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/what-is-privatelink.html\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-share-your-services.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nhttps://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html\n\nhttps://docs.aws.amazon.com/vpn/latest/clientvpn-admin/what-is.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q340",
    "questionText": "A junior scientist working with the Deep Space Research Laboratory at NASA is trying to upload a high-resolution image of a nebula into Amazon S3. The image size is approximately 3 gigabytes. The junior scientist is using Amazon S3 Transfer Acceleration (Amazon S3TA) for faster image upload. It turns out that Amazon S3TA did not result in an accelerated transfer.\n\nGiven this scenario, which of the following is correct regarding the charges for this image transfer?",
    "options": [
      {
        "text": "The junior scientist does not need to pay any transfer charges for the image upload",
        "isCorrect": true
      },
      {
        "text": "The junior scientist only needs to pay S3TA transfer charges for the image upload",
        "isCorrect": false
      },
      {
        "text": "The junior scientist only needs to pay Amazon S3 transfer charges for the image upload",
        "isCorrect": false
      },
      {
        "text": "The junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThe junior scientist does not need to pay any transfer charges for the image upload\n\nThere are no S3 data transfer charges when data is transferred in from the internet. Also with S3TA, you pay only for transfers that are accelerated. Therefore the junior scientist does not need to pay any transfer charges for the image upload because S3TA did not result in an accelerated transfer.\n\nAmazon S3 Transfer Acceleration (S3TA) Overview:\n\nvia - https://aws.amazon.com/s3/transfer-acceleration/\n\nIncorrect options:\n\nThe junior scientist only needs to pay S3TA transfer charges for the image upload - Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid.\n\nThe junior scientist only needs to pay Amazon S3 transfer charges for the image upload - There are no S3 data transfer charges when data is transferred in from the internet. So this option is incorrect.\n\nThe junior scientist needs to pay both S3 transfer charges and S3TA transfer charges for the image upload - There are no Amazon S3 data transfer charges when data is transferred in from the internet. Since S3TA did not result in an accelerated transfer, there are no S3TA transfer charges to be paid.\n\nReferences:\n\nhttps://aws.amazon.com/s3/transfer-acceleration/\n\nhttps://aws.amazon.com/s3/pricing/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q341",
    "questionText": "The solo founder at a tech startup has just created a brand new AWS account. The founder has provisioned an Amazon EC2 instance 1A which is running in AWS Region A. Later, he takes a snapshot of the instance 1A and then creates a new Amazon Machine Image (AMI) in Region A from this snapshot. This AMI is then copied into another Region B. The founder provisions an instance 1B in Region B using this new AMI in Region B.\n\nAt this point in time, what entities exist in Region B?",
    "options": [
      {
        "text": "1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B",
        "isCorrect": true
      },
      {
        "text": "1 Amazon EC2 instance and 1 AMI exist in Region B",
        "isCorrect": false
      },
      {
        "text": "1 Amazon EC2 instance and 1 snapshot exist in Region B",
        "isCorrect": false
      },
      {
        "text": "1 Amazon EC2 instance and 2 AMIs exist in Region B",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\n1 Amazon EC2 instance, 1 AMI and 1 snapshot exist in Region B\n\nAn Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance.\nWhen the new AMI is copied from Region A into Region B, it automatically creates a snapshot in Region B because AMIs are based on the underlying snapshots. Further, an instance is created from this AMI in Region B. Hence, we have\n1 Amazon EC2 instance, 1 AMI and 1 snapshot in Region B.\n\nAmazon Machine Image (AMI) Overview:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nIncorrect options:\n\n1 Amazon EC2 instance and 1 AMI exist in Region B\n\n1 Amazon EC2 instance and 1 snapshot exist in Region B\n\n1 Amazon EC2 instance and 2 AMIs exist in Region B\n\nAs mentioned earlier in the explanation, when the new AMI is copied from Region A into Region B, it also creates a snapshot in Region B because AMIs are based on the underlying snapshots. In addition, an instance is created from this AMI in Region B. So, we have 1 Amazon EC2 instance, 1 AMI and 1 snapshot in Region B. Hence all three options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q342",
    "questionText": "The sourcing team at the US headquarters of a global e-commerce company is preparing a spreadsheet of the new product catalog. The spreadsheet is saved on an Amazon Elastic File System (Amazon EFS) created in us-east-1 region. The sourcing team counterparts from other AWS regions such as Asia Pacific and Europe also want to collaborate on this spreadsheet.\n\nAs a solutions architect, what is your recommendation to enable this collaboration with the LEAST amount of operational overhead?",
    "options": [
      {
        "text": "The spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection",
        "isCorrect": true
      },
      {
        "text": "The spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions",
        "isCorrect": false
      },
      {
        "text": "The spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region",
        "isCorrect": false
      },
      {
        "text": "The spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThe spreadsheet on the Amazon Elastic File System (Amazon EFS) can be accessed in other AWS regions by using an inter-region VPC peering connection\n\nAmazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\n\nAmazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. Amazon EC2 instances can access your file system across AZs, regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN.\n\nYou can connect to Amazon EFS file systems from EC2 instances in other AWS regions using an inter-region VPC peering connection, and from on-premises servers using an AWS VPN connection. So this is the correct option.\n\nIncorrect options:\n\nThe spreadsheet will have to be copied in Amazon S3 which can then be accessed from any AWS region\n\nThe spreadsheet data will have to be moved into an Amazon RDS for MySQL database which can then be accessed from any AWS region\n\nCopying the spreadsheet into Amazon S3 or Amazon RDS for MySQL database is not the correct solution as it involves a lot of operational overhead. For Amazon RDS, one would need to write custom code to replicate the spreadsheet functionality running off of the database. S3 does not allow in-place edit of an object. Additionally, it's also not POSIX compliant. So one would need to develop a custom application to \"simulate in-place edits\" to support collabaration as per the use-case. So both these options are ruled out.\n\nThe spreadsheet will have to be copied into Amazon EFS file systems of other AWS regions as Amazon EFS is a regional service and it does not allow access from other AWS regions - Creating copies of the spreadsheet into Amazon EFS file systems of other AWS regions would mean no collaboration would be possible between the teams. In this case, each team would work on \"its own file\" instead of a single file accessed and updated by all teams. Hence this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/efs/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q343",
    "questionText": "A research group runs its flagship application on a fleet of Amazon EC2 instances for a specialized task that must deliver high random I/O performance. Each instance in the fleet would have access to a dataset that is replicated across the instances by the application itself. Because of the resilient application architecture, the specialized task would continue to be processed even if any instance goes down, as the underlying application would ensure the replacement instance has access to the required dataset.\n\nWhich of the following options is the MOST cost-optimal and resource-efficient solution to build this fleet of Amazon EC2 instances?",
    "options": [
      {
        "text": "Use Amazon Elastic Block Store (Amazon EBS) based EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 instances with Amazon EFS mount points",
        "isCorrect": false
      },
      {
        "text": "Use Instance Store based Amazon EC2 instances",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EC2 instances with access to Amazon S3 based storage",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Instance Store based Amazon EC2 instances\n\nAn instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host instance. Instance store is ideal for the temporary storage of information that changes frequently such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Instance store volumes are included as part of the instance's usage cost.\n\nAs Instance Store based volumes provide high random I/O performance at low cost (as the storage is part of the instance's usage cost) and the resilient architecture can adjust for the loss of any instance, therefore you should use Instance Store based Amazon EC2 instances for this use-case.\n\nAmazon EC2 Instance Store Overview:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nIncorrect options:\n\nUse Amazon Elastic Block Store (Amazon EBS) based EC2 instances - Amazon Elastic Block Store (Amazon EBS) based volumes would need to use provisioned IOPS (io1) as the storage type and that would incur additional costs. As we are looking for the most cost-optimal solution, this option is ruled out.\n\nUse Amazon EC2 instances with Amazon EFS mount points - Using Amazon Elastic File System (Amazon EFS) implies that extra resources would have to be provisioned (compared to using instance store where the storage is located on disks that are physically attached to the host instance itself). As we are looking for the most resource-efficient solution, this option is also ruled out.\n\nUse Amazon EC2 instances with access to Amazon S3 based storage - Using Amazon EC2 instances with access to Amazon S3 based storage does not deliver high random I/O performance, this option is just added as a distractor.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q344",
    "questionText": "A software engineering intern at an e-commerce company is documenting the process flow to provision Amazon EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes Human Resources payroll data. He wants to highlight those volume types that cannot be used as a boot volume.\n\nCan you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)",
    "options": [
      {
        "text": "General Purpose Solid State Drive (gp2)",
        "isCorrect": false
      },
      {
        "text": "Throughput Optimized Hard disk drive (st1)",
        "isCorrect": true
      },
      {
        "text": "Provisioned IOPS Solid state drive (io1)",
        "isCorrect": false
      },
      {
        "text": "Instance Store",
        "isCorrect": false
      },
      {
        "text": "Cold Hard disk drive (sc1)",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nThroughput Optimized Hard disk drive (st1)\n\nCold Hard disk drive (sc1)\n\nThe Amazon EBS volume types fall into two categories:\n\nSolid state drive (SSD) backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS.\n\nHard disk drive (HDD) backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS.\n\nThroughput Optimized HDD (st1) and Cold HDD (sc1) volume types CANNOT be used as a boot volume, so these two options are correct.\n\nPlease see this detailed overview of the volume types for Amazon EBS volumes.\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\n\nIncorrect options:\n\nGeneral Purpose Solid State Drive (gp2)\n\nProvisioned IOPS Solid state drive (io1)\n\nInstance Store\n\nGeneral Purpose SSD (gp2), Provisioned IOPS SSD (io1), and Instance Store can be used as a boot volume.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q345",
    "questionText": "The payroll department at a company initiates several computationally intensive workloads on Amazon EC2 instances at a designated hour on the last day of every month. The payroll department has noticed a trend of severe performance lag during this hour. The engineering team has figured out a solution by using Auto Scaling Group for these Amazon EC2 instances and making sure that 10 Amazon EC2 instances are available during this peak usage hour. For normal operations only 2 Amazon EC2 instances are enough to cater to the workload.\n\nAs a solutions architect, which of the following steps would you recommend to implement the solution?",
    "options": [
      {
        "text": "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "isCorrect": false
      },
      {
        "text": "Configure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "isCorrect": true
      },
      {
        "text": "Configure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "isCorrect": false
      },
      {
        "text": "Configure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the desired capacity of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nScheduled scaling allows you to set your own scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date.\n\nA scheduled action sets the minimum, maximum, and desired sizes to what is specified by the scheduled action at the time specified by the scheduled action. For the given use case, the correct solution is to set the desired capacity to 10. When we want to specify a range of instances, then we must use min and max values.\n\nIncorrect options:\n\nConfigure your Auto Scaling group by creating a scheduled action that kicks-off at the designated hour on the last day of the month. Set the min count as well as the max count of instances to 10. This causes the scale-out to happen before peak traffic kicks in at the designated hour - As mentioned earlier in the explanation, only when we want to specify a range of instances, then we must use min and max values. As the given use-case requires exactly 10 instances to be available during the peak hour, so we must set the desired capacity to 10. Hence this option is incorrect.\n\nConfigure your Auto Scaling group by creating a target tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nConfigure your Auto Scaling group by creating a simple tracking policy and setting the instance count to 10 at the designated hour. This causes the scale-out to happen before peak traffic kicks in at the designated hour\n\nTarget tracking policy or simple tracking policy cannot be used to effect a scaling action at a certain designated hour. Both these options have been added as distractors.\n\nReference:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q346",
    "questionText": "A technology blogger wants to write a review on the comparative pricing for various storage types available on AWS Cloud. The blogger has created a test file of size 1 gigabytes with some random data. Next he copies this test file into AWS S3 Standard storage class, provisions an Amazon EBS volume (General Purpose SSD (gp2)) with 100 gigabytes of provisioned storage and copies the test file into the Amazon EBS volume, and lastly copies the test file into an Amazon EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file.\n\nWhat is the correct order of the storage charges incurred for the test file on these three storage types?",
    "options": [
      {
        "text": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS",
        "isCorrect": false
      },
      {
        "text": "Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS",
        "isCorrect": true
      },
      {
        "text": "Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS",
        "isCorrect": false
      },
      {
        "text": "Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS < Cost of test file storage on Amazon EBS\n\nWith Amazon EBS Elastic Volumes, you pay only for the resources that you use. The Amazon EFS Standard Storage pricing is $0.30 per GB per month. Therefore the cost for storing the test file on EFS is $0.30 for the month.\n\nFor Amazon EBS General Purpose SSD (gp2) volumes, the charges are $0.10 per GB-month of provisioned storage. Therefore, for a provisioned storage of 100GB for this use-case, the monthly cost on EBS is $0.10*100 = $10. This cost is irrespective of how much storage is actually consumed by the test file.\n\nFor S3 Standard storage, the pricing is $0.023 per GB per month. Therefore, the monthly storage cost on S3 for the test file is $0.023.\n\nTherefore this is the correct option.\n\nIncorrect options:\n\nCost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS < Cost of test file storage on Amazon EFS\n\nCost of test file storage on Amazon EFS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EBS\n\nCost of test file storage on Amazon EBS < Cost of test file storage on Amazon S3 Standard < Cost of test file storage on Amazon EFS\n\nFollowing the computations shown earlier in the explanation, these three options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/ebs/pricing/\n\nhttps://aws.amazon.com/s3/pricing/(https://aws.amazon.com/s3/pricing/)\n\nhttps://aws.amazon.com/efs/pricing/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q347",
    "questionText": "A retail company runs a customer management system backed by a Microsoft SQL Server database. The system is tightly integrated with applications that rely on T-SQL queries. The company wants to modernize its infrastructure by migrating to Amazon Aurora PostgreSQL, but it needs to avoid major modifications to the existing application logic.\n\nWhich combination of actions should the company take to achieve this goal with minimal application refactoring? (Select two)",
    "options": [
      {
        "text": "Configure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Aurora Global Database to replicate data across regions for compatibility",
        "isCorrect": false
      },
      {
        "text": "Deploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands",
        "isCorrect": true
      },
      {
        "text": "Use AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data",
        "isCorrect": true
      },
      {
        "text": "Use AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nDeploy Babelfish for Aurora PostgreSQL to enable support for T-SQL commands\n\nBabelfish allows Aurora PostgreSQL to understand T-SQL (Microsoft SQL Server's query language) and SQL Server wire protocol, enabling applications to communicate with Aurora using SQL Server-style queries with minimal code changes. This is ideal for minimizing application code refactoring.\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html\n\nUse AWS Schema Conversion Tool (AWS SCT) along with AWS Database Migration Service (AWS DMS) to migrate the schema and data\n\nThe AWS Schema Conversion Tool (SCT) helps convert the SQL Server schema to PostgreSQL-compatible syntax, and AWS Database Migration Service (DMS) can move the actual data with minimal downtime. These tools are designed for database migration and are essential for schema and data transfer.\n\nIncorrect options:\n\nConfigure Amazon Aurora PostgreSQL with a custom endpoint that emulates Microsoft SQL Server behavior -  Aurora endpoints do not provide protocol-level emulation of SQL Server unless Babelfish is explicitly enabled. There is no feature to make Aurora natively emulate SQL Server behavior without Babelfish.\n\nUse Amazon Aurora Global Database to replicate data across regions for compatibility - Aurora Global Database helps with cross-region disaster recovery and read scalability, but it does not assist with SQL Server compatibility or reduce application code changes.\n\nUse AWS Glue to convert T-SQL queries to PostgreSQL-compatible SQL during the migration - AWS Glue is primarily used for ETL and data transformation, not for application SQL query conversion. It cannot translate T-SQL into PostgreSQL syntax.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/babelfish-compatibility.html",
    "awsService": "General",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q348",
    "questionText": "The development team at an e-commerce startup has set up multiple microservices running on Amazon EC2 instances under an Application Load Balancer. The team wants to route traffic to multiple back-end services based on the URL path of the HTTP header. So it wants requests for https://www.example.com/orders to go to a specific microservice and requests for https://www.example.com/products to go to another microservice.\n\nWhich of the following features of Application Load Balancers can be used for this use-case?",
    "options": [
      {
        "text": "Query string parameter-based routing",
        "isCorrect": false
      },
      {
        "text": "HTTP header-based routing",
        "isCorrect": false
      },
      {
        "text": "Host-based Routing",
        "isCorrect": false
      },
      {
        "text": "Path-based Routing",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nPath-based Routing\n\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions.\n\nIf your application is composed of several individual services, an Application Load Balancer can route a request to a service based on the content of the request. Here are the different types -\n\nHost-based Routing:\n\nYou can route a client request based on the Host field of the HTTP header allowing you to route to multiple domains from the same load balancer.\n\nPath-based Routing:\n\nYou can route a client request based on the URL path of the HTTP header.\n\nHTTP header-based routing:\n\nYou can route a client request based on the value of any standard or custom HTTP header.\n\nHTTP method-based routing:\n\nYou can route a client request based on any standard or custom HTTP method.\n\nQuery string parameter-based routing:\n\nYou can route a client request based on the query string or query parameters.\n\nSource IP address CIDR-based routing:\n\nYou can route a client request based on source IP address CIDR from where the request originates.\n\nPath-based Routing Overview:\n\nYou can use path conditions to define rules that route requests based on the URL in the request (also known as path-based routing).\n\nThe path pattern is applied only to the path of the URL, not to its query parameters.\n\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions\n\nIncorrect options:\n\nQuery string parameter-based routing\n\nHTTP header-based routing\n\nHost-based Routing\n\nAs mentioned earlier in the explanation, none of these three types of routing support requests based on the URL path of the HTTP header. Hence these three are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q349",
    "questionText": "The product team at a startup has figured out a market need to support both stateful and stateless client-server communications via the application programming interface (APIs) developed using its platform. You have been hired by the startup as a solutions architect to build a solution to fulfill this market need using Amazon API Gateway.\n\nWhich of the following would you identify as correct?",
    "options": [
      {
        "text": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "isCorrect": true
      },
      {
        "text": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server",
        "isCorrect": false
      },
      {
        "text": "Amazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "isCorrect": false
      },
      {
        "text": "Amazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server\n\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the front door for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications.\n\nHow Amazon API Gateway Works:\n\nvia - https://aws.amazon.com/api-gateway/\n\nAmazon API Gateway creates RESTful APIs that:\n\nAre HTTP-based.\n\nEnable stateless client-server communication.\n\nImplement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE.\n\nAmazon API Gateway creates WebSocket APIs that:\n\nAdhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server.\nRoute incoming messages based on message content.\n\nSo Amazon API Gateway supports stateless RESTful APIs as well as stateful WebSocket APIs. Therefore this option is correct.\n\nIncorrect options:\n\nAmazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server\n\nAmazon API Gateway creates RESTful APIs that enable stateless client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server\n\nAmazon API Gateway creates RESTful APIs that enable stateful client-server communication and Amazon API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server\n\nThese three options contradict the earlier details provided in the explanation. To summarize, Amazon API Gateway supports stateless RESTful APIs and stateful WebSocket APIs. Hence these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html",
    "awsService": "EBS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q350",
    "questionText": "The engineering team at a Spanish professional football club has built a notification system for its website using Amazon Simple Notification Service (Amazon SNS) notifications which are then handled by an AWS Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak football season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the website.\n\nAs a solutions architect, which of the following would you suggest as the BEST possible solution to this issue?",
    "options": [
      {
        "text": "Amazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit",
        "isCorrect": false
      },
      {
        "text": "Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit",
        "isCorrect": true
      },
      {
        "text": "The engineering team needs to provision more servers running the Amazon SNS service",
        "isCorrect": false
      },
      {
        "text": "The engineering team needs to provision more servers running the AWS Lambda service",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for AWS Lambda, so the team needs to contact AWS support to raise the account limit\n\nAmazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\n\nHow Amazon SNS Works:\n\nvia - https://aws.amazon.com/sns/\n\nWith AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consumeâ€”thereâ€™s no charge when your code isnâ€™t running.\n\nAWS Lambda currently supports 1000 concurrent executions per AWS account per region. If your Amazon SNS message deliveries to AWS Lambda contribute to crossing these concurrency quotas, your Amazon SNS message deliveries will be throttled. You need to contact AWS support to raise the account limit. Therefore this option is correct.\n\nIncorrect options:\n\nAmazon SNS has hit a scalability limit, so the team needs to contact AWS support to raise the account limit - Amazon SNS leverages the proven AWS cloud to dynamically scale with your application. You don't need to contact AWS support, as SNS is a fully managed service, taking care of the heavy lifting related to capacity planning, provisioning, monitoring, and patching. Therefore, this option is incorrect.\n\nThe engineering team needs to provision more servers running the Amazon SNS service\n\nThe engineering team needs to provision more servers running the AWS Lambda service\n\nAs both AWS Lambda and Amazon SNS are serverless and fully managed services, the engineering team cannot provision more servers. Both of these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/sns/\n\nhttps://aws.amazon.com/sns/faqs/",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q351",
    "questionText": "A file-hosting service uses Amazon Simple Storage Service (Amazon S3) under the hood to power its storage offerings. Currently all the customer files are uploaded directly under a single Amazon S3 bucket. The engineering team has started seeing scalability issues where customer file uploads have started failing during the peak access hours with more than 5000 requests per second.\n\nWhich of the following is the MOST resource efficient and cost-optimal way of addressing this issue?",
    "options": [
      {
        "text": "Change the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets",
        "isCorrect": false
      },
      {
        "text": "Change the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket",
        "isCorrect": false
      },
      {
        "text": "Change the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files",
        "isCorrect": false
      },
      {
        "text": "Change the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nChange the application architecture to create customer-specific custom prefixes within the single Amazon S3 bucket and then upload the daily files into those prefixed locations\n\nAmazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.\n\nThere are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second. Please see this example for more clarity on prefixes:\nif you have a file f1 stored in an S3 object path like so s3://your_bucket_name/folder1/sub_folder_1/f1, then /folder1/sub_folder_1/ becomes the prefix for file f1.\n\nSome data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. These data lake applications achieve single-instance transfer rates that maximize the network interface used for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance. These applications then aggregate throughput across multiple instances to get multiple terabits per second. Therefore creating customer-specific custom prefixes within the single bucket and then uploading the daily files into those prefixed locations is the BEST solution for the given constraints.\n\nOptimizing Amazon S3 Performance:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\n\nIncorrect options:\n\nChange the application architecture to create a new Amazon S3 bucket for each customer and then upload each customer's files directly under the respective buckets - Creating a new Amazon S3 bucket for each new customer is an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some customers may use the service sparingly but the bucket name is locked for them forever. Moreover, this is really not required as we can use S3 prefixes to improve the performance.\n\nChange the application architecture to create a new Amazon S3 bucket for each day's data and then upload the daily files directly under that day's bucket - Creating a new Amazon S3 bucket for each new day's data is also an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some of the bucket names may not be available for daily data processing. Moreover, this is really not required as we can use S3 prefixes to improve the performance.\n\nChange the application architecture to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 for storing the customers' uploaded files - Amazon EFS is a costlier storage option compared to Amazon S3, so it is ruled out.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q352",
    "questionText": "A new DevOps engineer has just joined a development team and wants to understand the replication capabilities for Amazon RDS Multi-AZ deployment as well as Amazon RDS Read-replicas.\n\nWhich of the following correctly summarizes these capabilities for the given database?",
    "options": [
      {
        "text": "Multi-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "isCorrect": false
      },
      {
        "text": "Multi-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "isCorrect": true
      },
      {
        "text": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "isCorrect": false
      },
      {
        "text": "Multi-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nMulti-AZ follows synchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones (AZs) within a single region.\n\nAmazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance.\n\nAmazon RDS replicates all databases in the source DB instance. Read replicas can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region.\n\nExam Alert:\n\nPlease review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS:\n\nvia - https://aws.amazon.com/rds/features/multi-az/\n\nIncorrect Options:\n\nMulti-AZ follows asynchronous replication and spans one Availability Zone (AZ) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nMulti-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow synchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nMulti-AZ follows asynchronous replication and spans at least two Availability Zones (AZs) within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone (AZ), Cross-AZ, or Cross-Region\n\nThese three options contradict the earlier details provided in the explanation. To summarize, Multi-AZ deployment follows synchronous replication for Amazon RDS. Hence these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/multi-az/\n\nhttps://aws.amazon.com/rds/features/read-replicas/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q353",
    "questionText": "A healthcare analytics company centralizes clinical and operational datasets in an Amazon S3â€“based data lake. Incoming data is ingested in Apache Parquet format from multiple hospitals and wearable health devices. To ensure quality and standardization, the company applies several transformation steps: anomaly filtering, datetime normalization, and aggregation by patient cohort. The company needs a solution to support a code-free interface that enables data engineers and business analysts to collaborate on data preparation workflows. The company also requires data lineage tracking, data profiling capabilities, and an easy way to share transformation logic across teams without writing or managing code.\n\nWhich AWS solution best meets these requirements?",
    "options": [
      {
        "text": "Use Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions",
        "isCorrect": false
      },
      {
        "text": "Create Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor",
        "isCorrect": false
      },
      {
        "text": "Use AWS Glue Studioâ€™s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column",
        "isCorrect": false
      },
      {
        "text": "Use AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Glue DataBrew to visually build transformation workflows on top of the raw Parquet files in S3. Use DataBrew recipes to track, audit, and share the transformation steps with others. Enable data profiling to inspect column statistics, null values, and data types across datasets\n\nAWS Glue DataBrew is a fully managed, serverless data preparation tool that enables users to visually clean, transform, and normalize raw datasetsâ€”all without writing any code. It is specifically designed for data analysts, business users, and data engineers who need to prepare data for analytics or machine learning workflows using an intuitive point-and-click interface.\n\nWhat is AWS Glue DataBrew?:\n\nvia - https://docs.aws.amazon.com/databrew/latest/dg/what-is.html\n\nIn the scenario, the company ingests Apache Parquet files into Amazon S3 and needs to apply multiple transformation steps such as anomaly filtering, standardizing date formats, and computing aggregates. With DataBrew, users can perform these tasks through a drag-and-drop UI and configure complex workflows using reusable â€œrecipesâ€â€”collections of ordered transformation steps that can be saved, versioned, and shared with others in the organization.\n\nFurthermore, DataBrew provides built-in data profiling capabilities. It automatically generates column-level statistics such as distributions, cardinality, null value counts, standard deviations, and outlier detection. This visibility enables better data quality assessment before the data is used for reporting or machine learning.\n\nDataBrewâ€™s recipe model supports auditability and traceability, satisfying the requirement for data lineage. All transformation steps are recorded and can be exported or replicated across projects. The output of a DataBrew job is written back to Amazon S3, keeping the transformed data accessible for downstream tools like Amazon Athena, Redshift, or QuickSight.\n\nBecause it's serverless, thereâ€™s no infrastructure to manage, and it integrates seamlessly with other AWS services. This makes it ideal for teams that want a no-code, collaborative, auditable, and profile-rich data transformation tool for Parquet-based data lakes in S3.\n\nIncorrect options:\n\nUse Amazon AppFlow to move and transform Parquet files in S3. Configure AppFlow transformations and mappings within the visual interface. Share flows with collaborators through AWS IAM policies and scheduled executions - Amazon AppFlow is used for data movement and basic transformation between SaaS applications (like Salesforce, Zendesk) and AWS services, but it does not support transforming or managing datasets already in S3 at scale. It also lacks comprehensive transformation workflows, data profiling, and data lineage capabilities required for complex ETL pipelines involving Parquet files.\n\nCreate Amazon Athena SQL queries to perform transformation steps directly on S3. Store queries in AWS Glue Data Catalog and share saved queries with other users through Amazon Athena's query editor - Athena enables running ad hoc SQL queries directly on S3-stored data and is suitable for technical users. However, it requires SQL skills, lacks a visual transformation interface, and does not provide built-in data profiling or visual lineage. While users can save and share queries, Athena is better suited for exploration and reporting than for building shareable, no-code transformation pipelines.\n\nUse AWS Glue Studioâ€™s visual canvas to design data transformation workflows on top of the Parquet files in Amazon S3. Configure Glue Studio jobs to run these transformations without writing code. Share the job definitions with team members for reuse. Use the visual job editor to track transformation progress and inspect profiling statistics for each dataset column - While AWS Glue Studio does offer a low-code visual interface for building ETL workflows, it is still primarily intended for developers and data engineers. The canvas generates and manages Apache Spark scripts, which may require customization and still involve code for advanced logic. More importantly, Glue Studio does not natively provide built-in data profiling or column-level statistics, unlike AWS Glue DataBrew. Additionally, Glue Studio is less suited for collaboration among non-technical users, and lacks the ability to share visual \"recipes\" with reusable, human-readable steps like DataBrew offers.\n\nReferences:\n\nhttps://docs.aws.amazon.com/databrew/latest/dg/what-is.html\n\nhttps://aws.amazon.com/blogs/big-data/use-aws-glue-databrew-recipes-in-your-aws-glue-studio-visual-etl-jobs/\n\nhttps://docs.aws.amazon.com/appflow/latest/userguide/what-is-appflow.html\n\nhttps://docs.aws.amazon.com/athena/latest/ug/querying.html\n\nhttps://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/emr-serverless.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q354",
    "questionText": "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes.\n\nAs a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
    "options": [
      {
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "isCorrect": true
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": true
      },
      {
        "text": "Amazon Relational Database Service (Amazon RDS)",
        "isCorrect": false
      },
      {
        "text": "Amazon OpenSearch Service",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nAmazon DynamoDB Accelerator (DAX)\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement â€“ from milliseconds to microseconds â€“ even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option.\n\nDAX Overview:\n\nvia - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\n\nAmazon ElastiCache\n\nAmazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option.\n\nIncorrect options:\n\nAmazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Amazon RDS cannot be used as a caching layer for Amazon DynamoDB.\n\nAmazon OpenSearch Service - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. It cannot be used as a caching layer for Amazon DynamoDB.\n\nAmazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used as a caching layer for Amazon DynamoDB.\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/dax/\n\nhttps://aws.amazon.com/elasticache/faqs/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q355",
    "questionText": "Amazon CloudFront offers a multi-tier cache in the form of regional edge caches that improve latency. However, there are certain content types that bypass the regional edge cache, and go directly to the origin.\n\nWhich of the following content types skip the regional edge cache? (Select two)",
    "options": [
      {
        "text": "Dynamic content, as determined at request time (cache-behavior configured to forward all headers)",
        "isCorrect": true
      },
      {
        "text": "Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin",
        "isCorrect": true
      },
      {
        "text": "E-commerce assets such as product photos",
        "isCorrect": false
      },
      {
        "text": "User-generated videos",
        "isCorrect": false
      },
      {
        "text": "Static content such as style sheets, JavaScript files",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nDynamic content, as determined at request time (cache-behavior configured to forward all headers)\n\nAmazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\n\nCloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.\n\nDynamic content, as determined at request time (cache-behavior configured to forward all headers), does not flow through regional edge caches, but goes directly to the origin. So this option is correct.\n\nProxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin\n\nProxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin from the POPs and do not proxy through the regional edge caches. So this option is also correct.\n\nHow Amazon CloudFront Delivers Content:\n\nvia - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html\n\nIncorrect Options:\n\nE-commerce assets such as product photos\n\nUser-generated videos\n\nStatic content such as style sheets, JavaScript files\n\nThe following type of content flows through the regional edge caches - user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos and static content such as style sheets, JavaScript files. Hence these three options are not correct.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html",
    "awsService": "CloudFront",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q356",
    "questionText": "A company uses Amazon S3 buckets for storing sensitive customer data. The company has defined different retention periods for different objects present in the Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected.\n\nWhich of the following options represent a valid configuration for setting up retention periods for objects in Amazon S3 buckets? (Select two)",
    "options": [
      {
        "text": "When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version",
        "isCorrect": true
      },
      {
        "text": "You cannot place a retention period on an object version through a bucket default setting",
        "isCorrect": false
      },
      {
        "text": "When you use bucket default settings, you specify a Retain Until Date for the object version",
        "isCorrect": false
      },
      {
        "text": "Different versions of a single object can have different retention modes and periods",
        "isCorrect": true
      },
      {
        "text": "The bucket default settings will override any explicit retention mode or period you request on an object version",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nWhen you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version\n\nYou can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires.\n\nDifferent versions of a single object can have different retention modes and periods\n\nLike all other Object Lock settings, retention periods apply to individual object versions. Different versions of a single object can have different retention modes and periods.\n\nFor example, suppose that you have an object that is 15 days into a 30-day retention period, and you PUT an object into Amazon S3 with the same name and a 60-day retention period. In this case, your PUT succeeds, and Amazon S3 creates a new version of the object with a 60-day retention period. The older version maintains its original retention period and becomes deletable in 15 days.\n\nIncorrect options:\n\nYou cannot place a retention period on an object version through a bucket default setting - You can place a retention period on an object version either explicitly or through a bucket default setting.\n\nWhen you use bucket default settings, you specify a Retain Until Date for the object version - When you use bucket default settings, you don't specify a Retain Until Date. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected.\n\nThe bucket default settings will override any explicit retention mode or period you request on an object version - If your request to place an object version in a bucket contains an explicit retention mode and period, those settings override any bucket default settings for that object version.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q357",
    "questionText": "A logistics company is building a multi-tier application to track the location of its trucks during peak operating hours. The company wants these data points to be accessible in real-time in its analytics platform via a REST API. The company has hired you as an AWS Certified Solutions Architect Associate to build a multi-tier solution to store and retrieve this location data for analysis.\n\nWhich of the following options addresses the given use case?",
    "options": [
      {
        "text": "Leverage Amazon Athena with Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Leverage Amazon QuickSight with Amazon Redshift",
        "isCorrect": false
      },
      {
        "text": "Leverage Amazon API Gateway with Amazon Kinesis Data Analytics",
        "isCorrect": true
      },
      {
        "text": "Leverage Amazon API Gateway with AWS Lambda",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nLeverage Amazon API Gateway with Amazon Kinesis Data Analytics\n\nYou can use Kinesis Data Analytics to transform and analyze streaming data in real-time with Apache Flink. Kinesis Data Analytics enables you to quickly build end-to-end stream processing applications for log analytics, clickstream analytics, Internet of Things (IoT), ad tech, gaming, etc. The four most common use cases are streaming extract-transform-load (ETL), continuous metric generation, responsive real-time analytics, and interactive querying of data streams. Kinesis Data Analytics for Apache Flink applications provides your application 50 GB of running application storage per Kinesis Processing Unit (KPU).\n\nAmazon API Gateway is a fully managed service that allows you to publish, maintain, monitor, and secure APIs at any scale. Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs and REST APIs, as well as an option to create WebSocket APIs.\n\nAmazon API Gateway:\n\nvia - https://aws.amazon.com/blogs/aws/amazon-rds-custom-for-oracle-new-control-capabilities-in-database-environment/\n\nFor the given use case, you can use Amazon API Gateway to create a REST API that handles incoming requests having location data from the trucks and sends it to the Kinesis Data Analytics application on the back end.\n\nAmazon Kinesis Data Analytics:\n\nvia - https://aws.amazon.com/kinesis/data-analytics/\n\nIncorrect options:\n\nLeverage Amazon Athena with Amazon S3 - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena cannot be used to build a REST API to consume data from the source. So this option is incorrect.\n\nLeverage Amazon QuickSight with Amazon Redshift - QuickSight is a cloud-native, serverless business intelligence service. Quicksight cannot be used to build a REST API to consume data from the source. Redshift is a fully managed AWS cloud data warehouse. So this option is incorrect.\n\nLeverage Amazon API Gateway with AWS Lambda - You cannot use Lambda to store and retrieve the location data for analysis, so this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html\n\nhttps://aws.amazon.com/kinesis/data-analytics/\n\nhttps://aws.amazon.com/kinesis/data-analytics/faqs/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q358",
    "questionText": "A development team requires permissions to list an Amazon S3 bucket and delete objects from that bucket. A systems administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows the principle of least privilege.\n\n    \"Version\": \"2021-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::example-bucket\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n\n\nWhich statement should a solutions architect add to the policy to address this issue?",
    "options": [
      {
        "text": "{\n    \"Action\": [\n        \"s3:*Object\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}",
        "isCorrect": false
      },
      {
        "text": "{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}",
        "isCorrect": true
      },
      {
        "text": "{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket*\"\n    ],\n    \"Effect\": \"Allow\"\n}",
        "isCorrect": false
      },
      {
        "text": "{\n    \"Action\": [\n        \"s3:*\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\n**\n\n{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n\n\n**\n\nThe main elements of a policy statement are:\n\n\nEffect: Specifies whether the statement will Allow or Deny an action (Allow is the effect defined here).\nAction: Describes a specific action or actions that will either be allowed or denied to run based on the Effect entered. API actions are unique to each service (DeleteObject is the action defined here).\nResource: Specifies the resourcesâ€”for example, an Amazon S3 bucket or objectsâ€”that the policy applies to in Amazon Resource Name (ARN) format ( example-bucket/* is the resource defined here).\n\n\nThis policy provides the necessary delete permissions on the resources of the Amazon S3 bucket to the group.\n\nIncorrect options:\n\n**\n\n{\n    \"Action\": [\n        \"s3:*Object\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n\n\n** - This policy is incorrect as the action value is invalid\n\n**\n\n{\n    \"Action\": [\n        \"s3:*\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket/*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n\n\n** - This policy is incorrect since it allows all actions on the resource, which violates the principle of least privilege, as required by the given use case.\n\n**\n\n{\n    \"Action\": [\n        \"s3:DeleteObject\"\n    ],\n    \"Resource\": [\n        \"arn:aws:s3:::example-bucket*\"\n    ],\n    \"Effect\": \"Allow\"\n}\n\n\n** - This is incorrect, as the resource name is incorrect. It should have a /* after the bucket name.\n\nReference:\n\nhttps://aws.amazon.com/blogs/security/techniques-for-writing-least-privilege-iam-policies/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q359",
    "questionText": "A gaming company is looking at improving the availability and performance of its global flagship application which utilizes User Datagram Protocol and needs to support fast regional failover in case an AWS Region goes down. The company wants to continue using its own custom Domain Name System (DNS) service.\n\nWhich of the following AWS services represents the best solution for this use-case?",
    "options": [
      {
        "text": "AWS Elastic Load Balancing (ELB)",
        "isCorrect": false
      },
      {
        "text": "AWS Global Accelerator",
        "isCorrect": true
      },
      {
        "text": "Amazon CloudFront",
        "isCorrect": false
      },
      {
        "text": "Amazon Route 53",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAWS Global Accelerator\n\nAWS Global Accelerator utilizes the Amazon global network, allowing you to improve the performance of your applications by lowering first-byte latency (the round trip time for a packet to go from a client to your endpoint and back again) and jitter (the variation of latency), and increasing throughput (the amount of time it takes to transfer data) as compared to the public internet.\n\nAWS Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover.\n\nIncorrect options:\n\nAmazon CloudFront - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\n\nAWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery), while Global Accelerator improves performance for a wide range of applications over TCP or UDP.\n\nAWS Elastic Load Balancing (ELB) - Both of the services, ELB and Global Accelerator solve the challenge of routing user requests to healthy application endpoints. AWS Global Accelerator relies on ELB to provide the traditional load balancing features such as support for internal and non-AWS endpoints, pre-warming, and Layer 7 routing. However, while ELB provides load balancing within one Region, AWS Global Accelerator provides traffic management across multiple Regions.\n\nA regional ELB load balancer is an ideal target for AWS Global Accelerator. By using a regional ELB load balancer, you can precisely distribute incoming application traffic across backends, such as Amazon EC2 instances or Amazon ECS tasks, within an AWS Region.\n\nIf you have workloads that cater to a global client base, AWS recommends that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to manage your resources.\n\nAmazon Route 53 - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. Route 53 is ruled out as the company wants to continue using its own custom DNS service.\n\nReference:\n\nhttps://aws.amazon.com/global-accelerator/faqs/",
    "awsService": "ELB",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q360",
    "questionText": "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Solutions Architect Associate to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution.\n\nWhich of the following will you recommend to meet these requirements?",
    "options": [
      {
        "text": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database",
        "isCorrect": false
      },
      {
        "text": "Push score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB",
        "isCorrect": true
      },
      {
        "text": "Push score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB",
        "isCorrect": false
      },
      {
        "text": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nPush score updates to Amazon Kinesis Data Streams which uses an AWS Lambda function to process these updates and then store these processed updates in Amazon DynamoDB\n\nTo help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications.\n\nAWS Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in Amazon DynamoDB.\n\nIncorrect options:\n\nPush score updates to an Amazon Simple Queue Service (Amazon SQS) queue which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process these updates in the Amazon SQS queue and then store these processed updates in an Amazon RDS MySQL database\n\nPush score updates to Amazon Kinesis Data Streams which uses a fleet of Amazon EC2 instances (with Auto Scaling) to process the updates in Amazon Kinesis Data Streams and then store these processed updates in Amazon DynamoDB\n\nPush score updates to an Amazon Simple Notification Service (Amazon SNS) topic, subscribe an AWS Lambda function to this Amazon SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2 instance\n\nThese three options use Amazon EC2 instances as part of the solution architecture. The use-case seeks to minimize the management overhead required to maintain the solution. However, Amazon EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q361",
    "questionText": "While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs.\n\nWhich of the following represents the best solution for the given scenario?",
    "options": [
      {
        "text": "Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow",
        "isCorrect": false
      },
      {
        "text": "Run Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards",
        "isCorrect": false
      },
      {
        "text": "AWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded",
        "isCorrect": false
      },
      {
        "text": "Create an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nCreate an Amazon CloudWatch metric filter that processes AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an Amazon SNS notification to the required team\n\nAWS CloudTrail log data can be ingested into Amazon CloudWatch to monitor and identify your AWS account activity against security threats, and create a governance framework for security best practices. You can analyze log trail event data in CloudWatch using features such as Logs Insight, Contributor Insights, Metric filters, and CloudWatch Alarms.\n\nAWS CloudTrail integrates with the Amazon CloudWatch service to publish the API calls being made to resources or services in the AWS account. The published event has invaluable information that can be used for compliance, auditing, and governance of your AWS accounts. Below we introduce several features available in CloudWatch to monitor API activity, analyze the logs at scale, and take action when malicious activity is discovered, without provisioning your infrastructure.\n\nFor the AWS Cloudtrail logs available in Amazon CloudWatch Logs, you can begin searching and filtering the log data by creating one or more metric filters. Use these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set a CloudWatch Alarm on.\n\nNote: AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with write API calls by continuously analyzing CloudTrail management events.\n\nInsights events are logged when AWS CloudTrail detects unusual write management API activity in your account. If you have AWS CloudTrail Insights enabled and CloudTrail detects unusual activity, Insights events are delivered to the destination Amazon S3 bucket for your trail. You can also see the type of insight and the incident time when you view Insights events on the CloudTrail console. Unlike other types of events captured in a CloudTrail trail, Insights events are logged only when CloudTrail detects changes in your account's API usage that differ significantly from the account's typical usage patterns.\n\nIncorrect options:\n\nConfigure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow -  AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and Amazon CloudWatch logs are the only destinations possible.\n\nRun Amazon Athena SQL queries against AWS CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards - Generating reports and visualizations help in understanding and analyzing patterns but is not useful as a near-real-time automatic solution for the given problem.\n\nAWS Trusted Advisor publishes metrics about check results to Amazon CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded - When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your check results to Amazon CloudWatch. You can view the metrics in CloudWatch. You can also create alarms to detect status changes to Trusted Advisor checks and status changes for resources, and service quota usage (formerly referred to as limits). The alarm will then notify you when you reach or exceed a service quota for your AWS account. However, the alarm is triggered only when the service limit is reached. We need a solution that raises an alarm when the number of API calls randomly increases or an abnormal pattern is detected. Hence, this option is not the right fit for the given use case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html\n\nhttps://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q362",
    "questionText": "A retail company's dynamic website is hosted using on-premises servers in its data center in the United States. The company is launching its website in Asia, and it wants to optimize the website loading times for new users in Asia. The website's backend must remain in the United States. The website is being launched in a few days, and an immediate solution is needed.\n\nWhat would you recommend?",
    "options": [
      {
        "text": "Use Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers",
        "isCorrect": true
      },
      {
        "text": "Migrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia",
        "isCorrect": false
      },
      {
        "text": "Leverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon CloudFront with a custom origin pointing to the on-premises servers\n\nAmazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. Amazon CloudFront uses standard cache control headers you set on your files to identify static and dynamic content. You can use different origins for different types of content on a single site â€“ e.g. Amazon S3 for static objects, Amazon EC2 for dynamic content, and custom origins for third-party content.\n\nAmazon CloudFront:\n\nvia - https://aws.amazon.com/cloudfront/\n\nAn origin server stores the original, definitive version of your objects. If you're serving content over HTTP, your origin server is either an Amazon S3 bucket or an HTTP server, such as a web server. Your HTTP server can run on an Amazon Elastic Compute Cloud (Amazon EC2) instance or on a server that you manage; these servers are also known as custom origins.\n\n\nvia - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\n\nAmazon CloudFront employs a global network of edge locations and regional edge caches that cache copies of your content close to your viewers. Amazon CloudFront ensures that end-user requests are served by the closest edge location. As a result, viewer requests travel a short distance, improving performance for your viewers. Therefore for the given use case, the users in Asia will enjoy a low latency experience while using the website even though the on-premises servers continue to be in the US.\n\nIncorrect options:\n\nUse Amazon CloudFront with a custom origin pointing to the DNS record of the website on Amazon Route 53 - This option has been added as a distractor. CloudFront cannot have a custom origin pointing to the DNS record of the website on Route 53.\n\nMigrate the website to Amazon S3. Use S3 cross-region replication (S3 CRR) between AWS Regions in the US and Asia - The use case states that the company operates a dynamic website. You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. By contrast, a dynamic website relies on server-side processing, including server-side scripts, such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites.  So this option is incorrect.\n\nLeverage a Amazon Route 53 geo-proximity routing policy pointing to on-premises servers - Since the on-premises servers continue to be in the US, so even using a Route 53 geo-proximity routing policy that directs the users in Asia to the on-premises servers in the US would not reduce the latency for the users in Asia. So this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/cloudfront/\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q363",
    "questionText": "A company is in the process of migrating its on-premises SMB file shares to AWS so the company can get out of the business of managing multiple file servers across dozens of offices. The company has 200 terabytes of data in its file servers. The existing on-premises applications and native Windows workloads should continue to have low latency access to this data which needs to be stored on a file system service without any disruptions after the migration. The company also wants any new applications deployed on AWS to have access to this migrated data.\n\nWhich of the following is the best solution to meet this requirement?",
    "options": [
      {
        "text": "Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS",
        "isCorrect": true
      },
      {
        "text": "Use Amazon Storage Gatewayâ€™s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS",
        "isCorrect": false
      },
      {
        "text": "Use AWS Storage Gatewayâ€™s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon S3. The applications deployed on AWS can access this data directly from Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Use Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS\n\nFor user or team file shares, and file-based application migrations, Amazon FSx File Gateway provides low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. For applications deployed on AWS, you may access your file shares directly from Amazon FSx in AWS.\n\nFor your native Windows workloads and users, or your SMB clients, Amazon FSx for Windows File Server provides all of the benefits of a native Windows SMB environment that is fully managed and secured and scaled like any other AWS service. You get detailed reporting, replication, backup, failover, and support for native Windows tools like DFS and Active Directory.\n\nAmazon FSx File Gateway:\n\nvia - https://aws.amazon.com/storagegateway/file/\n\nIncorrect options:\n\nUse Amazon Storage Gatewayâ€™s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon FSx for Windows File Server. The applications deployed on AWS can access this data directly from Amazon FSx in AWS - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB.\n\nAWS Storage Gatewayâ€™s File Gateway does not support file shares in Amazon FSx for Windows File Server, so this option is incorrect.\n\nAWS Storage Gatewayâ€™s File Gateway:\n\n\nUse AWS Storage Gatewayâ€™s File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon S3. The applications deployed on AWS can access this data directly from Amazon S3 - When you need to access S3 using a file system protocol, you should use File Gateway. You get a local cache in the gateway that provides high throughput and low latency over SMB.\n\nThe given use case requires low latency access to data which needs to be stored on a file system service after migration. Since S3 is an object storage service, so this option is incorrect.\n\nUse Amazon FSx File Gateway to provide low-latency, on-premises access to fully managed file shares in Amazon EFS. The applications deployed on AWS can access this data directly from Amazon EFS - Amazon FSx File Gateway provides access to fully managed file shares in Amazon FSx for Windows File Server and it does not support EFS. You should also note that EFS uses the Network File System version 4 (NFS v4) protocol and it does not support SMB protocol. Therefore this option is incorrect for the given use case.\n\nReferences:\n\nhttps://aws.amazon.com/storagegateway/file/fsx/\n\nhttps://aws.amazon.com/storagegateway/faqs/\n\nhttps://aws.amazon.com/blogs/storage/aws-reinvent-recap-choosing-storage-for-on-premises-file-based-workloads/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q364",
    "questionText": "A data analytics company measures what the consumers watch and what advertising theyâ€™re exposed to. This real-time data is ingested into its on-premises data center and subsequently, the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 gigabytes.\n\nWhich of the following is the fastest way to upload the daily compressed file into Amazon S3?",
    "options": [
      {
        "text": "Upload the compressed file in a single operation",
        "isCorrect": false
      },
      {
        "text": "Upload the compressed file using multipart upload",
        "isCorrect": false
      },
      {
        "text": "FTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket",
        "isCorrect": false
      },
      {
        "text": "Upload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUpload the compressed file using multipart upload with Amazon S3 Transfer Acceleration (Amazon S3TA)\n\nAmazon S3 Transfer Acceleration (Amazon S3TA) enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFrontâ€™s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\n\nMultipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.\n\nIncorrect options:\n\nUpload the compressed file in a single operation - In general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct.\n\nUpload the compressed file using multipart upload - Although using multipart upload would certainly speed up the process, combining with Amazon S3 Transfer Acceleration (Amazon S3TA) would further improve the transfer speed. Therefore just using multipart upload is not the correct option.\n\nFTP the compressed file into an Amazon EC2 instance that runs in the same region as the Amazon S3 bucket. Then transfer the file from the Amazon EC2 instance into the Amazon S3 bucket -  This is a roundabout process of getting the file into Amazon S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into Amazon S3.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q365",
    "questionText": "A large financial institution operates an on-premises data center with hundreds of petabytes of data managed on Microsoftâ€™s Distributed File System (DFS). The CTO wants the organization to transition into a hybrid cloud environment and run data-intensive analytics workloads that support DFS.\n\nWhich of the following AWS services can facilitate the migration of these workloads?",
    "options": [
      {
        "text": "Amazon FSx for Windows File Server",
        "isCorrect": true
      },
      {
        "text": "Microsoft SQL Server on AWS",
        "isCorrect": false
      },
      {
        "text": "Amazon FSx for Lustre",
        "isCorrect": false
      },
      {
        "text": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon FSx for Windows File Server\n\nAmazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol.  It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\nAmazon FSx supports the use of Microsoftâ€™s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size. So this option is correct.\n\nHow Amazon FSx for Windows File Server Works:\n\nvia - https://aws.amazon.com/fsx/windows/\n\nIncorrect options:\n\nAmazon FSx for Lustre\n\nAmazon FSx for Lustre makes it easy and cost-effective to launch and run the worldâ€™s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters.\nFSx for Lustre does not support Microsoftâ€™s Distributed File System (DFS), so this option is incorrect.\n\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nAWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. AWS Managed Microsoft AD is built on the actual Microsoft Active Directory and does not require you to synchronize or replicate data from your existing Active Directory to the cloud. AWS Managed Microsoft AD does not support Microsoftâ€™s Distributed File System (DFS), so this option is incorrect.\n\nMicrosoft SQL Server on AWS\n\nMicrosoft SQL Server on AWS offers you the flexibility to run Microsoft SQL Server database on AWS Cloud. Microsoft SQL Server on AWS does not support Microsoftâ€™s Distributed File System (DFS), so this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/fsx/windows/",
    "awsService": "General",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q366",
    "questionText": "A financial services company runs a Kubernetes-based microservices application in its on-premises data center. The application uses the Advanced Message Queuing Protocol (AMQP) to interact with a message queue. The company is experiencing rapid growth and its on-prem infrastructure cannot scale fast enough. The company wants to migrate the application to AWS with minimal code changes and reduce infrastructure management overhead. The messaging component must continue using AMQP, and the solution should offer high scalability and low operational effort.\n\nWhich combination of options will together meet these requirements? (Select two)",
    "options": [
      {
        "text": "Deploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) using AWS Fargate to avoid managing EC2 nodes",
        "isCorrect": true
      },
      {
        "text": "Use Amazon Simple Queue Service (Amazon SQS) as the replacement for the AMQP message broker. Refactor the application to use SQS SDKs and polling logic",
        "isCorrect": false
      },
      {
        "text": "Replace the current messaging system with Amazon MQ, a fully managed broker that supports AMQP natively. Integrate the application with the Amazon MQ endpoint without modifying the existing message format",
        "isCorrect": true
      },
      {
        "text": "Deploy the application to Amazon ECS on EC2, and integrate the messaging workflow using Amazon SNS for asynchronous pub/sub delivery",
        "isCorrect": false
      },
      {
        "text": "Run the application on Amazon EC2 Auto Scaling groups and use a self-hosted RabbitMQ instance on EC2 to preserve AMQP compatibility",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nDeploy the containerized application to Amazon Elastic Kubernetes Service (Amazon EKS) using AWS Fargate to avoid managing EC2 nodes\n\nAmazon EKS provides a Kubernetes-compatible, managed container orchestration service, which allows the company to reuse its existing Kubernetes manifests, Helm charts, and service configurations. By deploying workloads using AWS Fargate, the company avoids provisioning and managing EC2 worker nodes, aligning with the goal of reducing infrastructure overhead while maintaining scalability and agility.\n\nReplace the current messaging system with Amazon MQ, a fully managed broker that supports AMQP natively. Integrate the application with the Amazon MQ endpoint without modifying the existing message format\n\nAmazon MQ is a fully managed message broker service that supports AMQP (as well as MQTT, STOMP, and JMS). This allows the company to retain its existing messaging code and AMQP configuration without any major changes. Amazon MQ handles availability, durability, and patching, reducing the operational burden significantly.\n\nIncorrect options:\n\nUse Amazon Simple Queue Service (Amazon SQS) as the replacement for the AMQP message broker. Refactor the application to use SQS SDKs and polling logic - Amazon SQS does not support AMQP. To use SQS, the company would need to rewrite the messaging logic in the application to conform to SQSâ€™s polling-based message model and SDKs. This introduces significant rework and goes against the requirement of minimal code changes.\n\nDeploy the application to Amazon ECS on EC2, and integrate the messaging workflow using Amazon SNS for asynchronous pub/sub delivery - Although ECS supports containerized workloads, using EC2 launch type introduces infrastructure management responsibilities (e.g., patching, scaling, monitoring). Additionally, Amazon SNS does not support AMQP and is intended for pub/sub, not point-to-point messaging. This option fails to meet both the protocol and operational efficiency requirements.\n\nRun the application on Amazon EC2 Auto Scaling groups and use a self-hosted RabbitMQ instance on EC2 to preserve AMQP compatibility - Hosting RabbitMQ on EC2 can preserve AMQP compatibility, but it comes with the burden of managing the message broker, including patching, scaling, monitoring, and failover logic. This conflicts with the requirement of minimizing operational overhead, especially when a fully managed AMQP service (Amazon MQ) is available.\n\nReferences:\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/fargate.html\n\nhttps://docs.aws.amazon.com/amazon-mq/latest/developer-guide/welcome.html\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\n\nhttps://docs.aws.amazon.com/sns/latest/dg/welcome.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q367",
    "questionText": "A Hollywood studio is planning a series of promotional events leading up to the launch of the trailer of its next sci-fi thriller. The executives at the studio want to create a static website with lots of animations in line with the theme of the movie. The studio has hired you as a solutions architect to build a scalable serverless solution.\n\nWhich of the following represents the MOST cost-optimal and high-performance solution?",
    "options": [
      {
        "text": "Build the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution",
        "isCorrect": true
      },
      {
        "text": "Host the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin",
        "isCorrect": false
      },
      {
        "text": "Host the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin",
        "isCorrect": false
      },
      {
        "text": "Host the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nBuild the website as a static website hosted on Amazon S3. Create an Amazon CloudFront distribution with Amazon S3 as the origin. Use Amazon Route 53 to create an alias record that points to your Amazon CloudFront distribution\n\nYou can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket.\n\nAmazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\n\nYou can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away. Therefore, this option is correct.\n\nHosting a static website on Amazon S3:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\n\nIncorrect options:\n\nHost the website on AWS Lambda. Create an Amazon CloudFront distribution with Lambda as the origin\n\nWith AWS Lambda, you can run code without provisioning or managing servers. You can't host a website on Lambda. Also, you can't have CloudFront in front of Lambda. So this option is incorrect.\n\nHost the website on an Amazon EC2 instance. Create a Amazon CloudFront distribution with the Amazon EC2 instance as the custom origin\n\nHost the website on an instance in the studio's on-premises data center. Create an Amazon CloudFront distribution with this instance as the custom origin\n\nHosting the website on an Amazon EC2 instance or a data-center specific instance is ruled out as the studio wants a serverless solution. So both these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-cloudfront-walkthrough.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q368",
    "questionText": "A social photo-sharing web application is hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Elastic Load Balancer. The app gives the users the ability to upload their photos and also shows a leaderboard on the homepage of the app. The uploaded photos are stored in Amazon Simple Storage Service (Amazon S3) and the leaderboard data is maintained in Amazon DynamoDB. The Amazon EC2 instances need to access both Amazon S3 and Amazon DynamoDB for these features.\n\nAs a solutions architect, which of the following solutions would you recommend as the MOST secure option?",
    "options": [
      {
        "text": "Attach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB",
        "isCorrect": true
      },
      {
        "text": "Save the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB",
        "isCorrect": false
      },
      {
        "text": "Configure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI",
        "isCorrect": false
      },
      {
        "text": "Encrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAttach the appropriate IAM role to the Amazon EC2 instance profile so that the instance can access Amazon S3 and Amazon DynamoDB\n\nApplications that run on an Amazon EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the Amazon EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each Amazon EC2 instance when it's time to rotate the credentials.\n\nInstead, you should use an IAM role to manage temporary credentials for applications that run on an Amazon EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a username and password or access keys) to an Amazon EC2 instance. The role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an Amazon EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests. Therefore, this option is correct.\n\n\nvia - https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\n\nIncorrect options:\n\nSave the AWS credentials (access key Id and secret access token) in a configuration file within the application code on the Amazon EC2 instances. Amazon EC2 instances can use these credentials to access Amazon S3 and Amazon DynamoDB\n\nConfigure AWS CLI on the Amazon EC2 instances using a valid IAM user's credentials. The application code can then invoke shell scripts to access Amazon S3 and Amazon DynamoDB via AWS CLI\n\nEncrypt the AWS credentials via a custom encryption library and save it in a secret directory on the Amazon EC2 instances. The application code can then safely decrypt the AWS credentials to make the API calls to Amazon S3 and Amazon DynamoDB\n\nKeeping the AWS credentials (encrypted or plain text) on the Amazon EC2 instance is a bad security practice, therefore these three options using the AWS credentials are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q369",
    "questionText": "A multinational logistics company is migrating its core systems to AWS. As part of this migration, the company has built an Amazon S3â€“based data lake to ingest and analyze supply chain data from external carriers and vendors. While some vendors have adopted the companyâ€™s modern REST-based APIs for S3 uploads, others operate legacy systems that rely exclusively on SFTP for file transfers. These vendors are unable or unwilling to modify their workflows to support S3 APIs. The company wants to provide these vendors with an SFTP-compatible solution that allows direct uploads to Amazon S3, and must use fully managed AWS services to avoid managing any infrastructure. It must also support identity federation so that internal teams can map vendor access securely to specific S3 buckets or prefixes.\n\nWhich combination of options will provide a scalable and low-maintenance solution for this use case? (Select two)",
    "options": [
      {
        "text": "Deploy a fully managed AWS Transfer Family endpoint with SFTP enabled. Configure it to store uploaded files directly in an Amazon S3 bucket. Set up IAM roles mapped to each vendor for secure bucket or prefix access",
        "isCorrect": true
      },
      {
        "text": "Set up an Amazon EC2 instance with a custom SFTP server using OpenSSH. Configure cron jobs to upload received files to S3. Use Amazon CloudWatch to monitor EC2 health and disk usage",
        "isCorrect": false
      },
      {
        "text": "Configure Amazon S3 bucket policies to use IAM role-based access control for each vendor. Combine this with Transfer Family identity provider integration using Amazon Cognito or a custom identity provider for fine-grained permissions",
        "isCorrect": true
      },
      {
        "text": "Use AWS Transfer Family with SFTP for file uploads. Integrate the SFTP access control with Amazon Route 53 private hosted zones to create vendor-specific upload subdomains pointing to the SFTP endpoint",
        "isCorrect": false
      },
      {
        "text": "Use Amazon AppFlow to extract data from the legacy vendor systems and transform it into S3-compliant uploads. Schedule batch sync jobs to trigger every hour and send logs to CloudWatch for audit purposes",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nDeploy a fully managed AWS Transfer Family endpoint with SFTP enabled. Configure it to store uploaded files directly in an Amazon S3 bucket. Set up IAM roles mapped to each vendor for secure bucket or prefix access\n\nTo meet the needs of vendors that rely on legacy SFTP-based systems, the company can deploy a fully managed AWS Transfer Family endpoint with SFTP enabled. This service allows vendors to use their existing SFTP clients to upload files, while the company seamlessly stores those files in Amazon S3. To ensure secure and isolated access for each vendor, AWS Transfer Family supports mapping individual SFTP users to specific IAM roles. Each IAM role is configured with permissions that restrict access to a designated S3 bucket or prefix (e.g., s3://company-ingest/vendor1/), enforcing least-privilege access and preventing data leakage between vendors. This role-based model also allows for auditing and logging through AWS CloudTrail, giving the company visibility into who is uploading what and when.\n\nConfigure Amazon S3 bucket policies to use IAM role-based access control for each vendor. Combine this with Transfer Family identity provider integration using Amazon Cognito or a custom identity provider for fine-grained permissions\n\nTo extend this solution with greater flexibility and centralized identity control, the company can integrate AWS Transfer Family with an identity provider, such as Amazon Cognito, an external SAML 2.0 provider, or a custom identity system. This enables federated authentication, where users authenticate through the identity provider and are then dynamically mapped to IAM roles based on attributes like username or group membership. These IAM roles enforce fine-grained S3 access through policies that can include conditions such as IP-based restrictions, encryption requirements, or access to specific folders. This identity federation model ensures scalable, secure, and compliant user management, allowing the company to onboard and manage vendor users without maintaining custom scripts or SFTP server infrastructure. By combining AWS Transfer Family for SFTP access and IAM roles for access control via identity provider integration, the company achieves a low-maintenance, secure, and scalable solution for supporting legacy SFTP uploads into their S3-based data lake.\n\nIncorrect options:\n\nSet up an Amazon EC2 instance with a custom SFTP server using OpenSSH. Configure cron jobs to upload received files to S3. Use Amazon CloudWatch to monitor EC2 health and disk usage - Hosting your own SFTP server on Amazon EC2 introduces significant operational overhead, including patching, security hardening, high availability, backup management, and scaling. Although this approach is technically feasible, it does not meet the requirement of using fully managed AWS services. It also creates long-term maintenance burden for a simple use case.\n\nUse AWS Transfer Family with SFTP for file uploads. Integrate the SFTP access control with Amazon Route 53 private hosted zones to create vendor-specific upload subdomains pointing to the SFTP endpoint - Amazon Route 53 private hosted zones are used for internal DNS resolution and do not directly support public subdomain routing for SFTP endpoints. AWS Transfer Family already provides custom hostnames or AWS-hosted endpoints, and subdomain mapping adds complexity without functional gain. Route 53 is unnecessary here and does not simplify or enhance the solution.\n\nUse Amazon AppFlow to extract data from the legacy vendor systems and transform it into S3-compliant uploads. Schedule batch sync jobs to trigger every hour and send logs to CloudWatch for audit purposes - Amazon AppFlow is a SaaS integration service designed for API-based applications like Salesforce, Zendesk, or Google Analytics. It does not support SFTP-based integrations or custom connectors for legacy systems. AppFlow is not appropriate for handling direct file uploads from vendors using legacy SFTP software.\n\nReferences:\n\nhttps://docs.aws.amazon.com/transfer/latest/userguide/what-is-aws-transfer-family.html\n\nhttps://aws.amazon.com/blogs/storage/architecting-secure-and-compliant-managed-file-transfers-with-aws-transfer-family-sftp-connectors-and-pgp-encryption/\n\nhttps://docs.aws.amazon.com/appflow/latest/userguide/what-is-appflow.html\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zones-private.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q370",
    "questionText": "An IT company has an Access Control Management (ACM) application that uses Amazon RDS for MySQL but is running into performance issues despite using Read Replicas. The company has hired you as a solutions architect to address these performance-related challenges without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale.\n\nWhich of the following will you recommend as the MOST cost-effective and high-performance solution?",
    "options": [
      {
        "text": "Use Amazon Aurora Global Database to enable fast local reads with low latency in each region",
        "isCorrect": true
      },
      {
        "text": "Spin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region",
        "isCorrect": false
      },
      {
        "text": "Spin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon Aurora Global Database to enable fast local reads with low latency in each region\n\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database.\n\nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.\n\nAmazon Aurora Global Database Features:\n\nvia - https://aws.amazon.com/rds/aurora/global-database/\n\nIncorrect options:\n\nUse Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.\n\nGlobal Tables builds upon DynamoDBâ€™s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read, and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. Given that the use-case wants you to continue with the underlying schema of the relational database, DynamoDB is not the right choice as it's a NoSQL database.\n\nAmazon DynamoDB Global Tables Overview:\n\nvia - https://aws.amazon.com/dynamodb/global-tables/\n\nSpin up a Amazon Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. Amazon Redshift is not suited to be used as a transactional relational database, so this option is not correct.\n\nSpin up Amazon EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases - Setting up Amazon EC2 instances in multiple regions with manually managed MySQL databases represents a maintenance nightmare and is not the correct choice for this use-case.\n\nReferences:\n\nhttps://aws.amazon.com/rds/aurora/global-database/\n\nhttps://aws.amazon.com/dynamodb/global-tables/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q371",
    "questionText": "A silicon valley based startup has a content management application with the web-tier running on Amazon EC2 instances and the database tier running on Amazon Aurora. Currently, the entire infrastructure is located in us-east-1 region. The startup has 90% of its customers in the US and Europe. The engineering team is getting reports of deteriorated application performance from customers in Europe with high application load time.\n\nAs a solutions architect, which of the following would you recommend addressing these performance issues? (Select two)",
    "options": [
      {
        "text": "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53",
        "isCorrect": true
      },
      {
        "text": "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53",
        "isCorrect": false
      },
      {
        "text": "Create Amazon Aurora read replicas in the eu-west-1 region",
        "isCorrect": true
      },
      {
        "text": "Create Amazon Aurora Multi-AZ standby instance in the eu-west-1 region",
        "isCorrect": false
      },
      {
        "text": "Setup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nSetup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable latency routing policy in Amazon Route 53\n\nAmazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Use latency based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server.\n\nAs customers in Europe are facing performance issues with high application load time, you can use latency based routing to reduce the latency. Hence this is the correct option.\n\nAmazon Route 53 Routing Policy Overview:\n\nvia - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nCreate Amazon Aurora read replicas in the eu-west-1 region\n\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance.\n\nAmazon Aurora read replicas can be used to scale out reads across regions. This will improve the application performance for users in Europe. Therefore, this is also a correct option for the given use-case.\n\nIncorrect options:\n\nSetup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable geolocation routing policy in Amazon Route 53 - Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. You cannot use geolocation routing to reduce latency, hence this option is incorrect.\n\nSetup another fleet of Amazon EC2 instances for the web tier in the eu-west-1 region. Enable failover routing policy in Amazon Route 53 - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records. You cannot use failover routing to reduce latency, hence this option is incorrect.\n\nCreate Amazon Aurora Multi-AZ standby instance in the eu-west-1 region - Amazon Aurora Multi-AZ enhances the availability and durability for the database, it does not help in read scaling, so it is not a correct option for the given use-case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nhttps://aws.amazon.com/blogs/aws/new-cross-region-read-replicas-for-amazon-aurora/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q372",
    "questionText": "A video conferencing platform serves users worldwide through a globally distributed deployment of Amazon EC2 instances behind Network Load Balancers (NLBs) in several AWS Regions. The platform's architecture currently allows clients to connect to any Region via public endpoints, depending on how DNS resolves. However, users in regions far from the load balancers frequently experience high latency and slow connection times, especially during session initiation. The company wants to optimize the experience for global users by reducing end-to-end latency and load time while keeping the existing NLBs and EC2-based application infrastructure in place.\n\nWhich solution will best meet these requirements?",
    "options": [
      {
        "text": "Deploy a standard accelerator in AWS Global Accelerator and register the existing regional NLBs as endpoints. Use the accelerator to route user requests through AWSâ€™s global edge network to the closest healthy Regional NLB",
        "isCorrect": true
      },
      {
        "text": "Deploy Amazon CloudFront with HTTP caching enabled in front of the NLBs. Use CloudFront edge locations to serve user requests faster and reduce the load on the backend EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Replace all Network Load Balancers (NLBs) with Application Load Balancers (ALBs) in each Region. Register the EC2 instances as targets behind the ALBs and use cross-zone load balancing for latency distribution",
        "isCorrect": false
      },
      {
        "text": "Configure Amazon Route 53 with latency-based routing policies to direct users to the Region with the lowest response time. Use health checks to fail over to another Region if a specific NLB becomes unhealthy",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDeploy a standard accelerator in AWS Global Accelerator and register the existing regional NLBs as endpoints. Use the accelerator to route user requests through AWSâ€™s global edge network to the closest healthy Regional NLB\n\nAWS Global Accelerator provides static anycast IP addresses that route incoming user traffic through the AWS global edge network to the closest AWS Regional endpoint (in this case, the NLBs). It supports TCP and UDP, making it ideal for real-time applications. Global Accelerator reduces latency by using optimized AWS routes instead of the unpredictable public internet. By configuring existing NLBs as endpoints, the company avoids modifying its core infrastructure while dramatically improving global performance.\n\nHow AWS Global Accelerator works:\n\nvia - https://aws.amazon.com/blogs/networking-and-content-delivery/configuring-client-ip-address-preservation-with-a-network-load-balancer-in-aws-global-accelerator/\n\nIncorrect options:\n\nDeploy Amazon CloudFront with HTTP caching enabled in front of the NLBs. Use CloudFront edge locations to serve user requests faster and reduce the load on the backend EC2 instances - Amazon CloudFront is a global CDN that excels at caching static and HTTP-based content, but it does not support TCP/UDP protocols that are typically used with Network Load Balancers or gaming and real-time applications. Since NLBs operate at Layer 4 (TCP/UDP) and CloudFront works at Layer 7 (HTTP/HTTPS), this integration is not possible. CloudFront is not suitable for accelerating TCP traffic to NLBs.\n\nReplace all Network Load Balancers (NLBs) with Application Load Balancers (ALBs) in each Region. Register the EC2 instances as targets behind the ALBs and use cross-zone load balancing for latency distribution - Application Load Balancers (ALBs) operate at Layer 7 and are designed for HTTP/HTTPS workloads, while NLBs operate at Layer 4 and are better suited for real-time, low-latency TCP/UDP traffic, such as gaming or video conferencing. Replacing NLBs with ALBs may break protocol compatibility and reduce performance for non-HTTP workloads. Cross-zone load balancing does not address global latency since it works only within a Region, not across Regions.\n\nConfigure Amazon Route 53 with latency-based routing policies to direct users to the Region with the lowest response time. Use health checks to fail over to another Region if a specific NLB becomes unhealthy - Latency-based routing in Route 53 can help direct DNS queries to the Region with the lowest latency, but it does not guarantee global network optimization. It also relies on public DNS resolution and internet paths, which may not always route users optimally. Additionally, Route 53 does not provide the transport-level acceleration or failover performance that Global Accelerator offers.\n\nReferences:\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/configuring-client-ip-address-preservation-with-a-network-load-balancer-in-aws-global-accelerator/\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q373",
    "questionText": "An IT company is working on a client project to build a Supply Chain Management application. The web-tier of the application runs on an Amazon EC2 instance and the database tier is on Amazon RDS MySQL. For beta testing, all the resources are currently deployed in a single Availability Zone (AZ). The development team wants to improve application availability before the go-live.\n\nGiven that all end users of the web application would be located in the US, which of the following would be the MOST resource-efficient solution?",
    "options": [
      {
        "text": "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration",
        "isCorrect": false
      },
      {
        "text": "Deploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration",
        "isCorrect": true
      },
      {
        "text": "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration",
        "isCorrect": false
      },
      {
        "text": "Deploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDeploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration\n\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Therefore, deploying the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer would improve the availability of the application.\n\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each Availability Zone (AZ) runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.\nDeploying the Amazon RDS MySQL database in Multi-AZ configuration would improve availability and hence this is the correct option.\n\nIncorrect options:\n\nDeploy the web-tier Amazon EC2 instances in two Availability Zones (AZs), behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration\n\nDeploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in read replica configuration\n\nAmazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Read replicas are meant to address scalability issues. You cannot use read replicas for improving availability, so both these options are incorrect.\n\nExam Alert:\n\nPlease review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS:\n\nvia - https://aws.amazon.com/rds/features/multi-az/\n\nDeploy the web-tier Amazon EC2 instances in two regions, behind an Elastic Load Balancer. Deploy the Amazon RDS MySQL database in Multi-AZ configuration - As Elastic Load Balancing does not work across regions, so this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/rds/features/multi-az/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q374",
    "questionText": "A financial services company has deployed its flagship application on Amazon EC2 instances. Since the application handles sensitive customer data, the security team at the company wants to ensure that any third-party Secure Sockets Layer certificate (SSL certificate) SSL/Transport Layer Security (TLS) certificates configured on Amazon EC2 instances via the AWS Certificate Manager (ACM) are renewed before their expiry date. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution that notifies the security team 30 days before the certificate expiration. The solution should require the least amount of scripting and maintenance effort.\n\nWhat will you recommend?",
    "options": [
      {
        "text": "Monitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team",
        "isCorrect": false
      },
      {
        "text": "Monitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team",
        "isCorrect": false
      },
      {
        "text": "Leverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days",
        "isCorrect": false
      },
      {
        "text": "Leverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nLeverage AWS Config managed rule to check if any third-party SSL/TLS certificates imported into ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days\n\nAWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks.\n\nAWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.\n\n\nvia - https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\n\nAWS Config provides AWS-managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. You can leverage an AWS Config managed rule to check if any ACM certificates in your account are marked for expiration within the specified number of days. Certificates provided by ACM are automatically renewed. ACM does not automatically renew the certificates that you import. The rule is NON_COMPLIANT if your certificates are about to expire.\n\n\nvia - https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\n\nYou can configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic. For example, when a resource is updated, you can get a notification sent to your email, so that you can view the changes. You can also be notified when AWS Config evaluates your custom or managed rules against your resources.\n\nIncorrect options:\n\nMonitor the days to expiry Amazon CloudWatch metric for certificates imported into ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team - AWS Certificate Manager (ACM) does not attempt to renew third-party certificates that are imported. Also, an administrator needs to reconfigure missing DNS records for certificates that use DNS validation if the record was removed for any reason after the certificate was issued. Metrics and events provide you visibility into such certificates that require intervention to continue the renewal process. Amazon CloudWatch metrics and Amazon EventBridge events are enabled for all certificates that are managed by ACM. Users can monitor days to expiry as a metric for ACM certificates through Amazon CloudWatch. An Amazon EventBridge expiry event is published for any certificate that is at least 45 days away from expiry by default. Users can build alarms to monitor certificates based on days to expiry and also trigger custom actions such as calling a Lambda function or paging an administrator.\n\nIt is certainly possible to use the days to expiry CloudWatch metric to build a CloudWatch alarm to monitor the imported ACM certificates. The alarm will, in turn, trigger a notification to the security team. But this option needs more configuration effort than directly using the AWS Config managed rule that is available off-the-shelf.\n\nLeverage AWS Config managed rule to check if any SSL/TLS certificates created via ACM are marked for expiration within 30 days. Configure the rule to trigger an Amazon SNS notification to the security team if any certificate expires within 30 days\n\nMonitor the days to expiry Amazon CloudWatch metric for certificates created via ACM. Create a CloudWatch alarm to monitor such certificates based on the days to expiry metric and then trigger a custom action of notifying the security team\n\nAny SSL/TLS certificates created via ACM do not need any monitoring/intervention for expiration. ACM automatically renews such certificates. Hence both these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/acm-certificate-expiration-check.html\n\nhttps://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q375",
    "questionText": "A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key.\n\nWhich of the following Amazon S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?",
    "options": [
      {
        "text": "Server-Side Encryption with Amazon S3 managed keys (SSE-S3)",
        "isCorrect": false
      },
      {
        "text": "Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)",
        "isCorrect": false
      },
      {
        "text": "Server-Side Encryption with Customer-Provided Keys (SSE-C)",
        "isCorrect": true
      },
      {
        "text": "Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nServer-Side Encryption with Customer-Provided Keys (SSE-C)\n\nYou have the following options for protecting data at rest in Amazon S3:\n\nServer-Side Encryption â€“ Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.\n\nClient-Side Encryption â€“ Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.\n\nFor the given use-case, the company wants to manage the encryption keys via its custom application and let Amazon S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C).\n\nPlease review these three options for Server Side Encryption on Amazon S3:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\n\nIncorrect options:\n\nServer-Side Encryption with Amazon S3 managed keys (SSE-S3) - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.\n\nServer-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) - Unless you specify otherwise, buckets use SSE-S3 by default to encrypt objects. However, you can choose to configure buckets to use server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) instead. Amazon S3 uses server-side encryption with AWS KMS (SSE-KMS) to encrypt your S3 object data. Also, when SSE-KMS is requested for the object, the S3 checksum as part of the object's metadata, is stored in encrypted form.\n\nClient-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3 - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q376",
    "questionText": "A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system.\n\nWhich of the following represents the MOST operationally efficient way to meet this requirement?",
    "options": [
      {
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours",
        "isCorrect": false
      },
      {
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system",
        "isCorrect": false
      },
      {
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours",
        "isCorrect": true
      },
      {
        "text": "Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours\n\nAWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services.\n\nYou can use AWS DataSync to migrate data located on-premises, at the edge, or in other clouds to Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon FSx for Lustre, Amazon FSx for OpenZFS, and Amazon FSx for NetApp ONTAP.\n\nAWS DataSync:\n\nvia - https://aws.amazon.com/datasync/\n\nTo establish a private connection between your virtual private cloud (VPC) and the Amazon EFS API, you can create an interface VPC endpoint. You can also access the interface VPC endpoint from on-premises environments or other VPCs using AWS VPN, AWS Direct Connect, or VPC peering.\n\nAWS Direct Connect provides three types of virtual interfaces: public, private, and transit.\n\nAWS Direct Connect VIFs:\n\nvia - https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/\n\nFor the given use case, you can send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF.\n\nUsing task scheduling in AWS DataSync, you can periodically execute a transfer task from your source storage system to the destination. You can use the DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours.\n\nIncorrect options:\n\nConfigure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately. You cannot use VPC peering to transfer data over the Direct Connect connection from the on-premises systems to AWS. So this option is incorrect.\n\nConfigure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system - You can use a public virtual interface to connect to AWS resources that are reachable by a public IP address such as an Amazon Simple Storage Service (Amazon S3) bucket or AWS public endpoints. Although it is theoretically possible to set up this solution, however, it is not the most operationally efficient solution, since it involves sending data via AWS DataSync to Amazon S3 and then in turn using an AWS Lambda function to finally send data to Amazon EFS.\n\nConfigure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system - You can access Amazon S3 from your VPC using gateway VPC endpoints. You cannot use the Amazon S3 gateway endpoint to transfer data over the AWS Direct Connect connection from the on-premises systems to Amazon S3. So this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/datasync/\n\nhttps://aws.amazon.com/blogs/storage/transferring-files-from-on-premises-to-aws-and-back-without-leaving-your-vpc-using-aws-datasync/\n\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-vpc-endpoints.html\n\nhttps://aws.amazon.com/datasync/faqs/\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/\n\nhttps://docs.aws.amazon.com/datasync/latest/userguide/task-scheduling.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q377",
    "questionText": "A big-data consulting firm is working on a client engagement where the extract, transform, and load (ETL) workloads are currently handled via a Hadoop cluster deployed in the on-premises data center. The client wants to migrate their ETL workloads to AWS Cloud. The AWS Cloud solution needs to be highly available with about 50 Amazon Elastic Compute Cloud (Amazon EC2) instances per Availability Zone (AZ).\n\nAs a solutions architect, which of the following Amazon EC2 placement groups would you recommend for handling the distributed ETL workload?",
    "options": [
      {
        "text": "Cluster placement group",
        "isCorrect": false
      },
      {
        "text": "Spread placement group",
        "isCorrect": false
      },
      {
        "text": "Partition placement group",
        "isCorrect": true
      },
      {
        "text": "Both Spread placement group and Partition placement group",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nPartition placement group\n\nYou can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:\n\nPartition placement group â€“ spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. Therefore, this is the correct option for the given use-case.\n\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nIncorrect options:\n\nCluster placement group\n\nCluster Placement Group â€“ packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. This is not suited for distributed and replicated workloads such as Hadoop.\n\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nSpread placement group\n\nSpread Placement Group â€“ strictly places a small group of instances across distinct underlying hardware to reduce correlated failures. This is not suited for distributed and replicated workloads such as Hadoop.\n\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nBoth Spread placement group and Partition placement group - As mentioned earlier, the spread placement group is not suited for distributed and replicated workloads such as Hadoop. So this option is also incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q378",
    "questionText": "An e-commerce application uses an Amazon Aurora Multi-AZ deployment for its database. While analyzing the performance metrics, the engineering team has found that the database reads are causing high input/output (I/O) and adding latency to the write requests against the database.\n\nAs an AWS Certified Solutions Architect Associate, what would you recommend to separate the read requests from the write requests?",
    "options": [
      {
        "text": "Provision another Amazon Aurora database and link it to the primary database as a read replica",
        "isCorrect": false
      },
      {
        "text": "Set up a read replica and modify the application to use the appropriate endpoint",
        "isCorrect": true
      },
      {
        "text": "Configure the application to read from the Multi-AZ standby instance",
        "isCorrect": false
      },
      {
        "text": "Activate read-through caching on the Amazon Aurora database",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up a read replica and modify the application to use the appropriate endpoint\n\nAn Amazon Aurora DB cluster consists of one or more DB instances and a cluster volume that manages the data for those DB instances. An Aurora cluster volume is a virtual database storage volume that spans multiple Availability Zones (AZs), with each Availability Zone (AZ) having a copy of the DB cluster data. Two types of DB instances make up an Aurora DB cluster:\n\nPrimary DB instance â€“ Supports read and write operations, and performs all of the data modifications to the cluster volume. Each Aurora DB cluster has one primary DB instance.\n\nAurora Replica â€“ Connects to the same storage volume as the primary DB instance and supports only read operations. Each Aurora DB cluster can have up to 15 Aurora Replicas in addition to the primary DB instance. Aurora automatically fails over to an Aurora Replica in case the primary DB instance becomes unavailable. You can specify the failover priority for Aurora Replicas. Aurora Replicas can also offload read workloads from the primary DB instance.\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\n\nAurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer.\n\nWhile setting up a Multi-AZ deployment for Aurora, you create an Aurora replica or reader node in a different Availability Zone (AZ).\n\nMulti-AZ for Aurora:\n\n\nYou use the reader endpoint for read-only connections for your Aurora cluster. This endpoint uses a load-balancing mechanism to help your cluster handle a query-intensive workload. The reader endpoint is the endpoint that you supply to applications that do reporting or other read-only operations on the cluster. The reader endpoint load-balances connections to available Aurora Replicas in an Aurora DB cluster.\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\n\nIncorrect options:\n\nProvision another Amazon Aurora database and link it to the primary database as a read replica - You cannot provision another Aurora database and then link it as a read-replica for the primary database. This option is ruled out.\n\nConfigure the application to read from the Multi-AZ standby instance - This option has been added as a distractor as Aurora does not have any entity called standby instance. You create a standby instance while setting up a Multi-AZ deployment for Amazon RDS and NOT for Aurora.\n\nMulti-AZ for Amazon RDS:\n\n\nActivate read-through caching on the Amazon Aurora database - Amazon Aurora does not have built-in support for read-through caching, so this option just serves as a distractor. To implement caching, you will need to integrate something like Amazon ElastiCache and that would need code changes for the application.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html",
    "awsService": "General",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q379",
    "questionText": "An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets.\n\nAs a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
    "options": [
      {
        "text": "Use Amazon S3 Bucket Policies",
        "isCorrect": true
      },
      {
        "text": "Use Identity and Access Management (IAM) policies",
        "isCorrect": false
      },
      {
        "text": "Use Access Control Lists (ACLs)",
        "isCorrect": false
      },
      {
        "text": "Use Security Groups",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon S3 Bucket Policies\n\nBucket policies in Amazon S3 can be used to add or deny permissions across some or all of the objects within a single bucket. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources.\n\nYou can further restrict access to specific resources based on certain conditions. For example, you can restrict access based on request time (Date Condition), whether the request was sent using SSL (Boolean Conditions), a requesterâ€™s IP address (IP Address Condition), or based on the requester's client application (String Conditions). To identify these conditions, you use policy keys.\n\nTypes of access control in Amazon S3:\n\nvia - https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf\n\nIncorrect options:\n\nUse Identity and Access Management (IAM) policies - AWS IAM enables organizations with many employees to create and manage multiple users under a single AWS account. IAM policies are attached to the users, enabling centralized control of permissions for users under your AWS Account to access buckets or objects. With IAM policies, you can only grant users within your own AWS account permission to access your Amazon S3 resources. So, this is not the right choice for the current requirement.\n\nUse Access Control Lists (ACLs) - Within Amazon S3, you can use ACLs to give read or write access on buckets or objects to groups of users. With ACLs, you can only grant other AWS accounts (not specific users) access to your Amazon S3 resources. So, this is not the right choice for the current requirement.\n\nUse Security Groups - A security group acts as a virtual firewall for Amazon EC2 instances to control incoming and outgoing traffic. Amazon S3 does not support Security Groups, this option just acts as a distractor.\n\nReference:\n\nhttps://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q380",
    "questionText": "An IT company has built a solution wherein an Amazon Redshift cluster writes data to an Amazon S3 bucket belonging to a different AWS account. However, it is found that the files created in the Amazon S3 bucket using the UNLOAD command from the Amazon Redshift cluster are not even accessible to the Amazon S3 bucket owner.\n\nWhat could be the reason for this denial of permission for the bucket owner?",
    "options": [
      {
        "text": "When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console",
        "isCorrect": false
      },
      {
        "text": "The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress",
        "isCorrect": false
      },
      {
        "text": "When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures",
        "isCorrect": false
      },
      {
        "text": "By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nBy default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster - By default, an Amazon S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. Because the Amazon Redshift data files from the UNLOAD command were put into your bucket by another account, you (the bucket owner) don't have default permission to access those files.\n\nTo get access to the data files, an AWS Identity and Access Management (IAM) role with cross-account permissions must run the UNLOAD command again. Follow these steps to set up the Amazon Redshift cluster with cross-account permissions to the bucket:\n\n\nFrom the account of the Amazon S3 bucket, create an IAM role (Bucket Role) with permissions to the bucket.\nFrom the account of the Amazon Redshift cluster, create another IAM role (Cluster Role) with permissions to assume the Bucket Role.\nUpdate the Bucket Role to grant bucket access and create a trust relationship with the Cluster Role.\nFrom the Amazon Redshift cluster, run the UNLOAD command using the Cluster Role and Bucket Role.\n\n\nThis solution doesn't apply to Amazon Redshift clusters or Amazon S3 buckets that use server-side encryption with AWS Key Management Service (AWS KMS).\n\nIncorrect options:\n\nWhen objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console - By default, an Amazon S3 object is owned by the AWS account that uploaded it. So, the bucket owner will not have any default permissions on the objects. Therefore, this option is incorrect.\n\nThe owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress - This is an incorrect statement, given only as a distractor.\n\nWhen two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures - This is an incorrect statement, given only as a distractor.\n\nReference:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-redshift-unload/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q381",
    "questionText": "A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B.\n\nAs a Solutions Architect, which of the following will you recommend to meet this requirement?",
    "options": [
      {
        "text": "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role",
        "isCorrect": true
      },
      {
        "text": "AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda",
        "isCorrect": false
      },
      {
        "text": "Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket",
        "isCorrect": false
      },
      {
        "text": "The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role\n\nIf the IAM role that you create for the Lambda function is in the same AWS account as the bucket, then you don't need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Instead, you can grant the permissions on the IAM role and then verify that the bucket policy doesn't explicitly deny access to the Lambda function role. If the IAM role and the bucket are in different accounts, then you need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Therefore, this is the right way of giving access to AWS Lambda for the given use-case.\n\nComplete list of steps to be followed:\n\nvia - https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/\n\nIncorrect options:\n\nAWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda - This is an incorrect statement, used only as a distractor.\n\nCreate an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket - When the execution role of AWS Lambda and Amazon S3 bucket to be accessed are from different accounts, then you need to grant Amazon S3 bucket access permissions to the IAM role and also ensure that the bucket policy grants access to the AWS Lambda function's execution role.\n\nThe Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account - Making the Amazon S3 bucket public for the given use-case will be considered as a security bad practice. It's usually done for very few use-cases such as hosting a website on Amazon S3. Therefore this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q382",
    "questionText": "A social media application is hosted on an Amazon EC2 fleet running behind an Application Load Balancer. The application traffic is fronted by an Amazon CloudFront distribution. The engineering team wants to decouple the user authentication process for the application, so that the application servers can just focus on the business logic.\n\nAs a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?",
    "options": [
      {
        "text": "Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer",
        "isCorrect": true
      },
      {
        "text": "Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer\n\nApplication Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules.\n\n\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\n\nExam Alert:\n\nPlease review the following note to understand the differences between Amazon Cognito User Pools and Amazon Cognito Identity Pools:\n\nvia - https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\n\nIncorrect options:\n\nUse Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer - There is no such thing as using Amazon Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token.\n\nUse Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution - You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate AWS Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case.\n\nUse Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution - You cannot use Cognito Identity Pools for managing user authentication, so this option is not correct.\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\n\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q383",
    "questionText": "The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance.\n\nAs a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)",
    "options": [
      {
        "text": "A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive",
        "isCorrect": false
      },
      {
        "text": "The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG)",
        "isCorrect": false
      },
      {
        "text": "The health check grace period for the instance has not expired",
        "isCorrect": true
      },
      {
        "text": "The instance maybe in Impaired status",
        "isCorrect": true
      },
      {
        "text": "The instance has failed the Elastic Load Balancing (ELB) health check status",
        "isCorrect": true
      },
      {
        "text": "A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nThe health check grace period for the instance has not expired\n\nAmazon EC2 Auto Scaling doesn't terminate an instance that came into service based on Amazon EC2 status checks and Elastic Load Balancing (ELB) health checks until the health check grace period expires.\n\nMore on Health check grace period:\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period\n\nThe instance maybe in Impaired status\n\nAmazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status. Instead, Amazon EC2 Auto Scaling waits a few minutes for the instance to recover. Amazon EC2 Auto Scaling might also delay or not terminate instances that fail to report data for status checks. This usually happens when there is insufficient data for the status check metrics in Amazon CloudWatch.\n\nThe instance has failed the Elastic Load Balancing (ELB) health check status\n\nBy default, Amazon EC2 Auto Scaling doesn't use the results of ELB health checks to determine an instance's health status when the group's health check configuration is set to EC2. As a result, Amazon EC2 Auto Scaling doesn't terminate instances that fail ELB health checks. If an instance's status is OutofService on the ELB console, but the instance's status is Healthy on the Amazon EC2 Auto Scaling console, confirm that the health check type is set to ELB.\n\nIncorrect options:\n\nThe Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG) - This is an incorrect statement. Amazon EC2 Auto Scaling terminates Spot instances when capacity is no longer available or the Spot price exceeds your maximum price.\n\nA user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive - This statement is incorrect. If the configuration is updated and ASG needs more number of instances, ASG will launch new, healthy instances and does not keep unhealthy ones alive.\n\nA custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks - This statement is incorrect. You can define custom health checks in Amazon EC2 Auto Scaling. When a custom health check determines that an instance is unhealthy, the check manually triggers SetInstanceHealth and then sets the instance's state to Unhealthy. Amazon EC2 Auto Scaling then terminates the unhealthy instance.\n\nReferences:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance/\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-instance-how-terminated/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q384",
    "questionText": "An engineering team wants to examine the feasibility of the user data feature of Amazon EC2 for an upcoming project.\n\nWhich of the following are true about the Amazon EC2 user data configuration? (Select two)",
    "options": [
      {
        "text": "By default, user data is executed every time an Amazon EC2 instance is re-started",
        "isCorrect": false
      },
      {
        "text": "When an instance is running, you can update user data by using root user credentials",
        "isCorrect": false
      },
      {
        "text": "By default, scripts entered as user data are executed with root user privileges",
        "isCorrect": true
      },
      {
        "text": "By default, user data runs only during the boot cycle when you first launch an instance",
        "isCorrect": true
      },
      {
        "text": "By default, scripts entered as user data do not have root user privileges for executing",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nUser Data is generally used to perform common automated configuration tasks and even run scripts after the instance starts. When you launch an instance in Amazon EC2, you can pass two types of user data - shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text or as a file.\n\nBy default, scripts entered as user data are executed with root user privileges\n\nScripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script.\n\nBy default, user data runs only during the boot cycle when you first launch an instance\n\nBy default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance.\n\nIncorrect options:\n\nBy default, user data is executed every time an Amazon EC2 instance is re-started - As discussed above, this is not a default configuration of the system. But, can be achieved by explicitly configuring the instance.\n\nWhen an instance is running, you can update user data by using root user credentials - You can't change the user data if the instance is running (even by using root user credentials), but you can view it.\n\nBy default, scripts entered as user data do not have root user privileges for executing - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q385",
    "questionText": "A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved.\n\nAs a Solutions Architect, can you identify the Amazon Virtual Private Cloud (Amazon VPC) options to be configured in order to get the private hosted zone to work?",
    "options": [
      {
        "text": "Remove any overlapping namespaces for the private and public hosted zones",
        "isCorrect": false
      },
      {
        "text": "Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations",
        "isCorrect": false
      },
      {
        "text": "Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken",
        "isCorrect": false
      },
      {
        "text": "Enable DNS hostnames and DNS resolution for private hosted zones",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nEnable DNS hostnames and DNS resolution for private hosted zones\n\nDNS hostnames and DNS resolution are required settings for private hosted zones. DNS queries for private hosted zones can be resolved by the Amazon-provided VPC DNS server only. As a result, these options must be enabled for your private hosted zone to work.\n\nDNS hostnames: For non-default virtual private clouds that aren't created using the Amazon VPC wizard, this option is disabled by default. If you create a private hosted zone for a domain and create records in the zone without enabling DNS hostnames, private hosted zones aren't enabled. To use a private hosted zone, this option must be enabled.\n\nDNS resolution: Private hosted zones accept DNS queries only from a VPC DNS server. The IP address of the VPC DNS server is the reserved IP address at the base of the VPC IPv4 network range plus two. Enabling DNS resolution allows you to use the VPC DNS server as a Resolver for performing DNS resolution. Keep this option disabled if you're using a custom DNS server in the DHCP Options set, and you're not using a private hosted zone.\n\nIncorrect options:\n\nRemove any overlapping namespaces for the private and public hosted zones - If you have private and public hosted zones that have overlapping namespaces, such as example.com and accounting.example.com, then the Resolver routes traffic based on the most specific match. It won't result in unresolved queries, hence this option is wrong.\n\nFix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations - When you create a hosted zone, Amazon Route 53 automatically creates a name server (NS) record and a start of authority (SOA) record for the zone for public hosted zone. However, this issue is about the private hosted zone, hence this is an incorrect option.\n\nFix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken - If you have a private hosted zone (example.com) and a Resolver rule that routes traffic to your network for the same domain name, the Resolver rule takes precedence. It won't result in unresolved queries.\n\nReferences:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/vpc-enable-private-hosted-zone/\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-considerations.html\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-public-considerations.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q386",
    "questionText": "An e-commerce company operates multiple AWS accounts and has interconnected these accounts in a hub-and-spoke style using the AWS Transit Gateway. Amazon Virtual Private Cloud (Amazon VPCs) have been provisioned across these AWS accounts to facilitate network isolation.\n\nWhich of the following solutions would reduce both the administrative overhead and the costs while providing shared access to services required by workloads in each of the VPCs?",
    "options": [
      {
        "text": "Build a shared services Amazon Virtual Private Cloud (Amazon VPC)",
        "isCorrect": true
      },
      {
        "text": "Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs)",
        "isCorrect": false
      },
      {
        "text": "Use Fully meshed VPC Peering connection",
        "isCorrect": false
      },
      {
        "text": "Use VPCs connected with AWS Direct Connect",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nBuild a shared services Amazon Virtual Private Cloud (Amazon VPC)\n\nConsider an organization that has built a hub-and-spoke network with AWS Transit Gateway. VPCs have been provisioned into multiple AWS accounts, perhaps to facilitate network isolation or to enable delegated network administration. When deploying distributed architectures such as this, a popular approach is to build a \"shared services VPC, which provides access to services required by workloads in each of the VPCs. This might include directory services or VPC endpoints. Sharing resources from a central location instead of building them in each VPC may reduce administrative overhead and cost.\n\nCentralized VPC Endpoints (multiple VPCs):\n\nvia - https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/\n\nA VPC endpoint allows you to privately connect your VPC to supported AWS services without requiring an Internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Endpoints are virtual devices that are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.\n\nVPC endpoints enable you to reduce data transfer charges resulting from network communication between private VPC resources (such as Amazon Elastic Cloud Computeâ€”or EC2â€”instances) and AWS Services (such as Amazon Quantum Ledger Database, or QLDB). Without VPC endpoints configured, communications that originate from within a VPC destined for public AWS services must egress AWS to the public Internet in order to access AWS services. This network path incurs outbound data transfer charges. Data transfer charges for traffic egressing from Amazon EC2 to the Internet vary based on volume. With VPC endpoints configured, communication between your VPC and the associated AWS service does not leave the Amazon network. If your workload requires you to transfer significant volumes of data between your VPC and AWS, you can reduce costs by leveraging VPC endpoints.\n\nIncorrect options:\n\nUse Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs) - Transit VPC uses customer-managed Amazon Elastic Compute Cloud (Amazon EC2) VPN instances in a dedicated transit VPC with an Internet gateway. This design requires the customer to deploy, configure, and manage EC2-based VPN appliances, which will result in additional EC2, and potentially third-party product and licensing charges. Note that this design will generate additional data transfer charges for traffic traversing the transit VPC: data is charged when it is sent from a spoke VPC to the transit VPC, and again from the transit VPC to the on-premises network or a different AWS Region. Transit VPC is not the right choice here.\n\nMore on Transit VPC:\n\nvia - https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf\n\nUse Fully meshed VPC Peering connection - This approach creates multiple peering connections to facilitate the sharing of information between resources in different VPCs. This design connects multiple VPCs in a fully meshed configuration, with peering connections between each pair of VPCs. With this configuration, each VPC has access to the resources in all other VPCs. Each peering connection requires modifications to all the other VPCsâ€™ route tables and, as the number of VPCs grows, this can be difficult to maintain. And keep in mind that AWS recommends a maximum of 125 peering connections per VPC. It's complex to manage and isn't a right fit for the current scenario.\n\nMore on Fully meshed VPC Peers:\n\nvia - https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf\n\nUse VPCs connected with AWS Direct Connect - This approach is a good alternative for customers who need to connect a high number of VPCs to a central VPC or on-premises resources, or who already have an AWS Direct Connect connection in place. This design also offers customers the ability to incorporate transitive routing into their network design. For example, if VPC A and VPC B are both connected to an on-premises network using AWS Direct Connect connections, then the two VPCs can be connected to each other via AWS Direct Connect. AWS Direct Connect requires physical cables and takes about a month for setting up, this is not an ideal solution for the given scenario.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/\n\nhttps://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q387",
    "questionText": "A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence.\n\nAs a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?",
    "options": [
      {
        "text": "Use Amazon Route 53 weighted routing to spread traffic across different deployments",
        "isCorrect": false
      },
      {
        "text": "Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment",
        "isCorrect": true
      },
      {
        "text": "Use Elastic Load Balancing (ELB) to distribute traffic across deployments",
        "isCorrect": false
      },
      {
        "text": "Use AWS CodeDeploy deployment options to choose the right deployment",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nBlue/green deployment is a technique for releasing applications by shifting traffic between two identical environments running different versions of the application: \"Blue\" is the currently running version and \"green\" the new version. This type of deployment allows you to test features in the green environment without impacting the currently running version of your application. When youâ€™re satisfied that the green version is working properly, you can gradually reroute the traffic from the old blue environment to the new green environment. Blue/green deployments can mitigate common risks associated with deploying software, such as downtime and rollback capability.\n\nUse AWS Global Accelerator to distribute a portion of traffic to a particular deployment\n\nAWS Global Accelerator is a network layer service that directs traffic to optimal endpoints over the AWS global network, this improves the availability and performance of your internet applications. It provides two static anycast IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers, Elastic IP addresses or Amazon EC2 instances, in a single or in multiple AWS regions.\n\nAWS Global Accelerator uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group, and traffic dials to control the percentage of traffic that is directed to an endpoint group (an AWS region where your application is deployed).\n\nWhile relying on the DNS service is a great option for blue/green deployments, it may not fit use-cases that require a fast and controlled transition of the traffic. Some client devices and internet resolvers cache DNS answers for long periods; this DNS feature improves the efficiency of the DNS service as it reduces the DNS traffic across the Internet, and serves as a resiliency technique by preventing authoritative name-server overloads. The downside of this in blue/green deployments is that you donâ€™t know how long it will take before all of your users receive updated IP addresses when you update a record, change your routing preference or when there is an application failure.\n\nWith AWS Global Accelerator, you can shift traffic gradually or all at once between the blue and the green environment and vice-versa without being subject to DNS caching on client devices and internet resolvers, traffic dials and endpoint weights changes are effective within seconds.\n\nIncorrect options:\n\nUse Amazon Route 53 weighted routing to spread traffic across different deployments - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. As discussed earlier, DNS caching is a negative behavior for this use case and hence Amazon Route 53 is not a good option.\n\nUse Elastic Load Balancing (ELB) to distribute traffic across deployments - Elastic Load Balancing (ELB) can distribute traffic across healthy instances. You can also use the Application Load Balancers weighted target groups feature for blue/green deployments as it does not rely on the DNS service. In addition you donâ€™t need to create new ALBs for the green environment. As the use-case refers to a global application, so this option cannot be used for a multi-Region solution which is needed for the given requirement.\n\nUse AWS CodeDeploy deployment options to choose the right deployment - In AWS CodeDeploy, a deployment is the process, and the components involved in the process, of installing content on one or more instances. This content can consist of code, web and configuration files, executables, packages, scripts, and so on. AWS CodeDeploy deploys content that is stored in a source repository, according to the configuration rules you specify. Blue/Green deployment is one of the deployment types that CodeDeploy supports. CodeDeploy is not meant to distribute traffic across instances, so this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/using-aws-global-accelerator-to-achieve-blue-green-deployments\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted",
    "awsService": "ELB",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q388",
    "questionText": "A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines.\n\nWhich of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?",
    "options": [
      {
        "text": "Dedicated Instances",
        "isCorrect": true
      },
      {
        "text": "Spot Instances",
        "isCorrect": false
      },
      {
        "text": "Dedicated Hosts",
        "isCorrect": false
      },
      {
        "text": "On-Demand Instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDedicated Instances\n\nDedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.\n\nA Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.\n\nDifferences between Dedicated Hosts and Dedicated Instances:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\n\nIncorrect options:\n\nSpot Instances -  A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.\n\nDedicated Hosts - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.\n\nOn-Demand Instances - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycleâ€”you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.\n\nHigh Level Overview of Amazon EC2 Instance Purchase Options:\n\nvia - https://aws.amazon.com/ec2/pricing/\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q389",
    "questionText": "You have been hired as a Solutions Architect to advise a company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the Amazon API Gateway. The company would prefer a solution that offers built-in user management.\n\nWhich of the following solutions would you suggest as the best fit for the given use-case?",
    "options": [
      {
        "text": "Use AWS_IAM authorization",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Cognito User Pools",
        "isCorrect": true
      },
      {
        "text": "Use AWS Lambda authorizer for Amazon API Gateway",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Cognito Identity Pools",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon Cognito User Pools\n\nA user pool is a user directory in Amazon Cognito. You can leverage Amazon Cognito User Pools to either provide built-in user management or integrate with external identity providers, such as Facebook, Twitter, Google+, and Amazon. Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).\n\nUser pools provide:\n1. Sign-up and sign-in services.\n2. A built-in, customizable web UI to sign in users.\n3. Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool.\n4. User directory management and user profiles.\n5. Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.\n6. Customized workflows and user migration through AWS Lambda triggers.\n\nAfter creating an Amazon Cognito user pool, in API Gateway, you must then create a COGNITO_USER_POOLS authorizer that uses the user pool.\n\nAmazon Cognito User Pools:\n\nvia - https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html\n\nIncorrect options:\n\nUse AWS_IAM authorization - For consumers who currently are located within your AWS environment or have the means to retrieve AWS Identity and Access Management (IAM) temporary credentials to access your environment, you can use AWS_IAM authorization and add least-privileged permissions to the respective IAM role to securely invoke your API. API Gateway API Keys is not a security mechanism and should not be used for authorization unless itâ€™s a public API. It should be used primarily to track a consumerâ€™s usage across your API.\n\nUse AWS Lambda authorizer for Amazon API Gateway - If you have an existing Identity Provider (IdP), you can use an AWS Lambda authorizer for Amazon API Gateway to invoke a Lambda function to authenticate/validate a given user against your Identity Provider. You can use a Lambda authorizer for custom validation logic based on identity metadata.\n\nA Lambda authorizer can send additional information derived from a bearer token or request context values to your backend service. For example, the authorizer can return a map containing user IDs, user names, and scope. By using Lambda authorizers, your backend does not need to map authorization tokens to user-centric data, allowing you to limit the exposure of such information to just the authorization function.\n\nWhen using Lambda authorizers, AWS strictly advises against passing credentials or any sort of sensitive data via query string parameters or headers, so this is not as secure as using Amazon Cognito User Pools.\n\nIn addition, both these options do not offer built-in user management.\n\nUse Amazon Cognito Identity Pools - The two main components of Amazon Cognito are user pools and identity pools. Identity pools provide AWS credentials to grant your users access to other AWS services. To enable users in your user pool to access AWS resources, you can configure an identity pool to exchange user pool tokens for AWS credentials. So, identity pools aren't an authentication mechanism in themselves and hence aren't a choice for this use case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-enable-cognito-user-pool.html\n\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q390",
    "questionText": "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance.\n\nAs a solutions architect, how will you fix this issue?",
    "options": [
      {
        "text": "Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic",
        "isCorrect": false
      },
      {
        "text": "IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)",
        "isCorrect": false
      },
      {
        "text": "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic",
        "isCorrect": true
      },
      {
        "text": "Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSecurity Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic\n\nSecurity groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.\n\nTo enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.\n\nThe designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.\n\nBy default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.\n\nIf you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway.\n\nIncorrect options:\n\nNetwork access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic - This is incorrect as already discussed.\n\nIAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL) - This is a made-up option and just added as a distractor.\n\nRules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool.\n\nReference:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q391",
    "questionText": "A global media agency is developing a cultural analysis project to explore how major sports stories have evolved over the last five years. The team has collected thousands of archived news bulletins and magazine spreads stored in PDF format. These documents are rich in unstructured text and come from various sources with differing layouts and font styles. The agency wants to better understand how public tone and narrative have shifted over time. The team has chosen to use Amazon Textract for its ability to accurately extract printed and scanned text from complex PDF layouts. They need a solution that can then analyze the emotional tone and subject matter of the extracted text with the least possible operational burden, using fully managed AWS services where possible.\n\nWhich solution will best meet these requirements?",
    "options": [
      {
        "text": "Use Amazon SageMaker to train a custom sentiment analysis model. Store the model outputs in Amazon DynamoDB for structured querying by analysts",
        "isCorrect": false
      },
      {
        "text": "Process the extracted output with AWS Lambda to convert the text into CSV format. Query the data using Amazon Athena and visualize it using Amazon QuickSight.",
        "isCorrect": false
      },
      {
        "text": "Ingest the extracted data into Amazon Redshift using AWS Glue, and use Amazon Rekognition to analyze the tone of the document layouts for sentiment classification.",
        "isCorrect": false
      },
      {
        "text": "Send the extracted text to Amazon Comprehend for entity detection and sentiment analysis. Store the results in Amazon S3 for further access or visualization.",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nSend the extracted text to Amazon Comprehend for entity detection and sentiment analysis. Store the results in Amazon S3 for further access or visualization.\n\nThis solution offers the least operational overhead and leverages fully managed services. Amazon Textract efficiently extracts structured and unstructured text from complex PDF documents. The extracted content is then passed to Amazon Comprehend, which provides pre-trained NLP models capable of performing sentiment analysis, entity recognition, and topic classification. There is no need to build or train custom models. The results can be stored in Amazon S3, making them easily accessible for downstream processing or reporting. This approach is serverless, scalable, and ideal for unstructured data.\n\nAmazon Textract with Amazon Comprehend:\n\nvia - https://aws.amazon.com/blogs/machine-learning/extracting-custom-entities-from-documents-with-amazon-textract-and-amazon-comprehend/\n\nIncorrect options:\n\nUse Amazon SageMaker to train a custom sentiment analysis model. Store the model outputs in Amazon DynamoDB for structured querying by analysts - Although SageMaker provides a powerful platform for building custom ML models, using it here adds unnecessary complexity and overhead. You would need to collect training data, develop a sentiment model, and manage training infrastructure, which contradicts the requirement for low operational overhead. Also, DynamoDB is not optimized for full-text analysis, making it an awkward choice for storing sentiment-rich textual data.\n\nProcess the extracted output with AWS Lambda to convert the text into CSV format. Query the data using Amazon Athena and visualize it using Amazon QuickSight. - This solution involves multiple processing steps and intermediate transformations. Converting text to CSV via Lambda and querying it using Athena adds operational complexity. Additionally, Athena is not built for sentiment analysisâ€”itâ€™s a query engine for structured data. QuickSight provides visualization, but without prior NLP processing (e.g., through Comprehend), it cannot extract sentiment insights from plain text.\n\nIngest the extracted data into Amazon Redshift using AWS Glue, and use Amazon Rekognition to analyze the tone of the document layouts for sentiment classification. - This architecture includes components (e.g., Amazon Rekognition) that are not suitable for text or sentiment analysis. Rekognition is a computer vision service, designed to analyze images and videosâ€”not the emotional tone of text. Loading textual data into Redshift via AWS Glue also involves multiple ETL steps and schema management, which increases overhead without improving the sentiment analysis capabilities.\n\nReferences:\n\nhttps://docs.aws.amazon.com/textract/latest/dg/what-is.html\n\nhttps://docs.aws.amazon.com/comprehend/latest/dg/what-is.html\n\nhttps://aws.amazon.com/blogs/machine-learning/extracting-custom-entities-from-documents-with-amazon-textract-and-amazon-comprehend/\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html\n\nhttps://docs.aws.amazon.com/athena/latest/ug/what-is.html\n\nhttps://docs.aws.amazon.com/rekognition/latest/dg/what-is.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q392",
    "questionText": "A manufacturing analytics company has a large collection of automated scripts that perform data cleanup, validation, and system integration tasks. These scripts are currently run by a local Linux cron scheduler and have an execution time of up to 30 minutes. The company wants to migrate these scripts to AWS without significant changes, and would prefer a containerized, serverless architecture that automatically scales and can respond to event-based triggers in the future. The solution must minimize infrastructure management.\n\nWhich solution will best meet these requirements with minimal refactoring and operational overhead?",
    "options": [
      {
        "text": "Convert each script into a Lambda function and package it in a zip archive. Use Amazon EventBridge Scheduler to run the functions on a fixed schedule. Use Amazon S3 to store function outputs and logs",
        "isCorrect": false
      },
      {
        "text": "Package the scripts into a container image. Use Amazon EventBridge Scheduler to define cron-based recurring schedules. Configure EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS",
        "isCorrect": true
      },
      {
        "text": "Package the scripts into a container image. Deploy the image to AWS Batch with a managed compute environment on Amazon EC2. Define scheduling policies in AWS Batch to trigger jobs according to cron expressions",
        "isCorrect": false
      },
      {
        "text": "Create a container image for each script. Use AWS Step Functions to define a workflow for all scheduled tasks. Use a Wait state to delay execution and run tasks using Step Functionsâ€™ RunTask integration with ECS Fargate",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nPackage the scripts into a container image. Use Amazon EventBridge Scheduler to define cron-based recurring schedules. Configure EventBridge Scheduler to invoke AWS Fargate tasks using Amazon ECS\n\nThis solution best meets all the requirements. Amazon EventBridge Scheduler is a fully managed, serverless scheduler that allows you to define high-scale cron and rate-based jobs without maintaining any scheduling infrastructure. By packaging the legacy cron scripts as containers and running them as ECS Fargate tasks, the company avoids the need to provision EC2 instances or manage runtime environments. The solution is serverless, scalable, and compatible with future event-based triggers using EventBridgeâ€™s native integration.\n\nIncorrect options:\n\nConvert each script into a Lambda function and package it in a zip archive. Use Amazon EventBridge Scheduler to run the functions on a fixed schedule. Use Amazon S3 to store function outputs and logs - While AWS Lambda integrates well with EventBridge Scheduler, Lambda functions have a hard timeout of 15 minutes. This makes them unsuitable for cron jobs that run up to 30 minutes, as stated in the scenario. Additionally, refactoring each script into a separate Lambda function would require significant code and dependency changes, violating the requirement for minimal refactoring.\n\nPackage the scripts into a container image. Deploy the image to AWS Batch with a managed compute environment on Amazon EC2. Define scheduling policies in AWS Batch to trigger jobs according to cron expressions - AWS Batch supports long-running jobs and scheduling, but itâ€™s designed for batch compute jobs with queues and dependency management, not for lightweight, cron-like container jobs. Batch also has longer setup and initialization times and typically requires more configuration and compute environment management. Using EC2-based compute environments increases operational overhead.\n\nCreate a container image for each script. Use AWS Step Functions to define a workflow for all scheduled tasks. Use a Wait state to delay execution and run tasks using Step Functionsâ€™ RunTask integration with ECS Fargate - AWS Step Functions can schedule tasks using Wait states, but this approach introduces unnecessary orchestration complexity for simple cron workloads. Step Functions are best suited for multi-step workflows, not individual recurring jobs.\n\nReferences:\n\nhttps://docs.aws.amazon.com/scheduler/latest/UserGuide/what-is-scheduler.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html\n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/connect-ecs.html\n\nhttps://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q393",
    "questionText": "A silicon valley based startup has a two-tier architecture using Amazon EC2 instances for its flagship application. The web servers (listening on port 443), which have been assigned security group A, are in public subnets across two Availability Zones (AZs) and the MSSQL based database instances (listening on port 1433), which have been assigned security group B, are in two private subnets across two Availability Zones (AZs). The DevOps team wants to review the security configurations of the application architecture.\n\nAs a solutions architect, which of the following options would you select as the MOST secure configuration? (Select two)",
    "options": [
      {
        "text": "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443",
        "isCorrect": false
      },
      {
        "text": "For security group B: Add an inbound rule that allows traffic only from all sources on port 1433",
        "isCorrect": false
      },
      {
        "text": "For security group B: Add an inbound rule that allows traffic only from security group A on port 443",
        "isCorrect": false
      },
      {
        "text": "For security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433",
        "isCorrect": true
      },
      {
        "text": "For security group B: Add an inbound rule that allows traffic only from security group A on port 1433",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nFor security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433\n\nFor security group B: Add an inbound rule that allows traffic only from security group A on port 1433\n\nA security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.\n\nThe following are the characteristics of security group rules:\n\nBy default, security groups allow all outbound traffic.\n\nSecurity group rules are always permissive; you can't create rules that deny access.\n\nSecurity groups are stateful\n\nThe MOST secure configuration for the given use case is:\n\nFor security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 1433\n\nThe above rules make sure that web servers are listening for traffic on all sources on the HTTPS protocol on port 443. The web servers only allow outbound traffic to MSSQL servers in Security Group B on port 1433.\n\nFor security group B: Add an inbound rule that allows traffic only from security group A on port 1433.\nThe above rule makes sure that the MSSQL servers only accept traffic from web servers in security group A on port 1433.\n\nTherefore, both of these options are correct.\n\nIncorrect options:\n\nFor security group A: Add an inbound rule that allows traffic from all sources on port 443. Add an outbound rule with the destination as security group B on port 443 - As the MSSQL based database instances are listening on port 1433, therefore for security group A, the outbound rule should be added on port 443 with the destination as security group B.\n\nFor security group B: Add an inbound rule that allows traffic only from all sources on port 1433 - The inbound rule should allow traffic only from security group A on port 1433. Allowing traffic from all sources will compromise security.\n\nFor security group B: Add an inbound rule that allows traffic only from security group A on port 443 -  The inbound rule should allow traffic only from security group A on port 1433 because the MSSQL based database instances are listening on port 1433.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q394",
    "questionText": "A startup has just developed a video backup service hosted on a fleet of Amazon EC2 instances. The Amazon EC2 instances are behind an Application Load Balancer and the instances are using Amazon Elastic Block Store (Amazon EBS) Volumes for storage. The service provides authenticated users the ability to upload videos that are then saved on the EBS volume attached to a given instance. On the first day of the beta launch, users start complaining that they can see only some of the videos in their uploaded videos backup. Every time the users log into the website, they claim to see a different subset of their uploaded videos.\n\nWhich of the following is the MOST optimal solution to make sure that users can view all the uploaded videos? (Select two)",
    "options": [
      {
        "text": "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos",
        "isCorrect": false
      },
      {
        "text": "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos",
        "isCorrect": true
      },
      {
        "text": "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos",
        "isCorrect": false
      },
      {
        "text": "Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos",
        "isCorrect": true
      },
      {
        "text": "Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nWrite a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos\n\nMount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos\n\nAmazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale.\n\nAmazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.\n\nAmazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.\n\nAs Amazon EBS volumes are attached locally to the Amazon EC2 instances, therefore the uploaded videos are tied to specific Amazon EC2 instances. Every time the user logs in, they are directed to a different instance and therefore their videos get dispersed across multiple EBS volumes. The correct solution is to use either Amazon S3 or Amazon EFS to store the user videos.\n\nIncorrect options:\n\nWrite a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos - Amazon S3 Glacier Deep Archive is meant to be used for long term data archival. It cannot be used to serve static content such as videos or images via a web application. So this option is incorrect.\n\nWrite a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos - Amazon RDS is a relational database and not the right candidate for storing videos.\n\nWrite a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos - Amazon DynamoDB is a NoSQL database and not the right candidate for storing videos.\n\nReference:\n\nhttps://aws.amazon.com/ebs/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q395",
    "questionText": "To improve the performance and security of the application, the engineering team at a company has created an Amazon CloudFront distribution with an Application Load Balancer as the custom origin. The team has also set up an AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution. The security team at the company has noticed a surge in malicious attacks from a specific IP address to steal sensitive data stored on the Amazon EC2 instances.\n\nAs a solutions architect, which of the following actions would you recommend to stop the attacks?",
    "options": [
      {
        "text": "Create a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances",
        "isCorrect": false
      },
      {
        "text": "Create a deny rule for the malicious IP in the Security Groups associated with each of the instances",
        "isCorrect": false
      },
      {
        "text": "Create an IP match condition in the AWS WAF to block the malicious IP address",
        "isCorrect": true
      },
      {
        "text": "Create a ticket with AWS support to take action against the malicious IP",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an IP match condition in the AWS WAF to block the malicious IP address\n\nAWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.\n\nHow AWS WAF Works:\n\nvia - https://aws.amazon.com/waf/\n\nIf you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from. So, this option is correct.\n\nIncorrect options:\n\nCreate a deny rule for the malicious IP in the network access control list (network ACL) associated with each of the instances - Network access control list (network ACL) are not associated with instances. So this option is also ruled out.\n\nCreate a deny rule for the malicious IP in the Security Groups associated with each of the instances - You cannot deny rules in Security Groups. So this option is ruled out.\n\nCreate a ticket with AWS support to take action against the malicious IP - Managing the security of your application is your responsibility, not that of AWS, so you cannot raise a ticket for this issue.\n\nReference:\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q396",
    "questionText": "An application is currently hosted on four Amazon EC2 instances (behind Application Load Balancer) deployed in a single Availability Zone (AZ). To maintain an acceptable level of end-user experience, the application needs at least 4 instances to be always available.\n\nAs a solutions architect, which of the following would you recommend so that the application achieves high availability with MINIMUM cost?",
    "options": [
      {
        "text": "Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)",
        "isCorrect": true
      },
      {
        "text": "Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)",
        "isCorrect": false
      },
      {
        "text": "Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ)",
        "isCorrect": false
      },
      {
        "text": "Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDeploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)\n\nThe correct option is to deploy the instances in three Availability Zones (AZs) and launch two instances in each Availability Zone (AZ). Even if one of the AZs goes out of service, still we shall have 4 instances available and the application can maintain an acceptable level of end-user experience. Therefore, we can achieve high availability with just 6 instances in this case.\n\nIncorrect options:\n\nDeploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ) - When we launch two instances in two AZs, we run the risk of falling below the minimum acceptable threshold of 4 instances if one of the AZs fails. So this option is ruled out.\n\nDeploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ) - When we launch four instances in two AZs, we have to bear costs for 8 instances which is NOT cost-optimal. So this option is ruled out.\n\nDeploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ) - We can't have just two instances in a single AZ as that is below the minimum acceptable threshold of 4 instances.",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q397",
    "questionText": "A junior DevOps engineer wants to change the default configuration for Amazon EBS volume termination. By default, the root volume of an Amazon EC2 instance for an EBS-backed AMI is deleted when the instance terminates.\n\nWhich option below helps change this default behavior to ensure that the volume persists even after the instance terminates?",
    "options": [
      {
        "text": "Set the TerminateOnDelete attribute to true",
        "isCorrect": false
      },
      {
        "text": "Set the DeleteOnTermination attribute to false",
        "isCorrect": true
      },
      {
        "text": "Set the TerminateOnDelete attribute to false",
        "isCorrect": false
      },
      {
        "text": "Set the DeleteOnTermination attribute to true",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet the DeleteOnTermination attribute to false\n\nAn Amazon EC2 instance can be launched from either an instance store-backed AMI or an Amazon EBS-backed AMI. Instances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates.\nThe default behavior can be changed to ensure that the volume persists after the instance terminates. To change the default behavior, set the DeleteOnTermination attribute to false using a block device mapping.\n\nIncorrect options:\n\nSet the TerminateOnDelete attribute to true\n\nSet the TerminateOnDelete attribute to false\n\nBoth these options are incorrect as there is no such attribute as TerminateOnDelete. These options have been added as distractors.\n\nSet the DeleteOnTermination attribute to true - If you set the DeleteOnTermination attribute to true, then the root volume for an AMI backed by Amazon EBS would be deleted when the instance terminates. Therefore, this option is incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q398",
    "questionText": "An application runs big data workloads on Amazon Elastic Compute Cloud (Amazon EC2) instances. The application runs 24x7 all round the year and needs at least 20 instances to maintain a minimum acceptable performance threshold and the application needs 300 instances to handle spikes in the workload. Based on historical workloads processed by the application, it needs 80 instances 80% of the time.\n\nAs a solutions architect, which of the following would you recommend as the MOST cost-optimal solution so that it can meet the workload demand in a steady state?",
    "options": [
      {
        "text": "Purchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)",
        "isCorrect": true
      },
      {
        "text": "Purchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand",
        "isCorrect": false
      },
      {
        "text": "Purchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand",
        "isCorrect": false
      },
      {
        "text": "Purchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nPurchase 80 reserved instances (RIs). Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances)\n\nAs the steady-state workload demand is 80 instances, we can save on costs by purchasing 80 reserved instances. Based on additional workload demand, we can specify a mix of on-demand and spot instances using Application Load Balancer with a launch template to provision the mix of on-demand and spot instances.\n\nPlease see this detailed overview of various types of Amazon EC2 instances from a pricing perspective:\n\nvia - https://aws.amazon.com/ec2/pricing/\n\nIncorrect options:\n\nPurchase 20 on-demand instances. Use Auto Scaling Group to provision the remaining instances as spot instances per the workload demand - Provisioning 20 on-demand instances implies that there would be a shortfall of 60 instances 80% of the time. Provisioning all of these 60 instances as spot instances is highly risky as there is no guarantee regarding the availability of the spot instances, which means we may not even meet the steady-state requirement for the workload, so this option is incorrect.\n\nPurchase 80 on-demand instances. Provision additional on-demand and spot instances per the workload demand (Use Auto Scaling Group with launch template to provision the mix of on-demand and spot instances) - Provisioning 80 on-demand instances would end up costlier than the option where we provision 80 reserved instances. So this option is ruled out.\n\nPurchase 80 spot instances. Use Auto Scaling Group to provision the remaining instances as on-demand instances per the workload demand - The option to purchase 80 spot instances is incorrect, as there is no guarantee regarding the availability of the spot instances, which means we may not even meet the steady-state workload.\n\nReference:\n\nhttps://aws.amazon.com/ec2/pricing/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q399",
    "questionText": "A big data consulting firm needs to set up a data lake on Amazon S3 for a Health-Care client. The data lake is split in raw and refined zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based extract, transform, and load (ETL) job into the refined zone. The business analysts run ad-hoc queries only on the data in the refined zone using Amazon Athena. The team is concerned about the cost of data storage in both the raw and refined zones as the data is increasing at a rate of 1 terabyte daily in each zone.\n\nAs a solutions architect, which of the following would you recommend as the MOST cost-optimal solution? (Select two)",
    "options": [
      {
        "text": "Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation",
        "isCorrect": true
      },
      {
        "text": "Create an AWS Lambda function based job to delete the raw zone data after 1 day",
        "isCorrect": false
      },
      {
        "text": "Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation",
        "isCorrect": false
      },
      {
        "text": "Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format",
        "isCorrect": false
      },
      {
        "text": "Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nSetup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation\n\nYou can manage your objects so that they are stored cost-effectively throughout their lifecycle by configuring their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. For example, you might choose to transition objects to the Amazon S3 Standard-IA storage class 30 days after you created them, or archive objects to the Amazon S3 Glacier storage class one year after creating them.\n\nFor the given use-case, the raw zone consists of the source data, so it cannot be deleted due to compliance reasons. Therefore, you should use a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation.\n\nPlease read more about Amazon S3 Object Lifecycle Management:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\n\nUse AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format\n\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.\nYou cannot transition the refined zone data into Amazon S3 Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Therefore, the best optimization is to have the refined zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the refined zone.\n\nPlease see this example for a AWS Glue ETL Pipeline:\n\nvia - https://aws.amazon.com/glue/\n\nIncorrect options:\n\nCreate an AWS Lambda function based job to delete the raw zone data after 1 day - As mentioned in the use-case, the source data needs to be kept for a minimum of 5 years for compliance reasons. Therefore the data in the raw zone cannot be deleted after 1 day.\n\nSetup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation - You cannot transition the refined zone data into Amazon S3 Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Hence this option is incorrect.\n\nUse AWS Glue ETL job to write the transformed data in the refined zone using CSV format - It is cost-optimal to write the data in the refined zone using a compressed format instead of CSV format. The compressed data would reduce the storage cost incurred on the data in the refined zone. So, this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\n\nhttps://aws.amazon.com/glue/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q400",
    "questionText": "An IT company wants to optimize the costs incurred on its fleet of 100 Amazon EC2 instances for the next year. Based on historical analyses, the engineering team observed that 70 of these instances handle the compute services of its flagship application and need to be always available. The other 30 instances are used to handle batch jobs that can afford a delay in processing.\n\nAs a solutions architect, which of the following would you recommend as the MOST cost-optimal solution?",
    "options": [
      {
        "text": "Purchase 70 reserved instances (RIs) and 30 spot instances",
        "isCorrect": true
      },
      {
        "text": "Purchase 70 on-demand instances and 30 spot instances",
        "isCorrect": false
      },
      {
        "text": "Purchase 70 reserved instances and 30 on-demand instances",
        "isCorrect": false
      },
      {
        "text": "Purchase 70 on-demand instances and 30 reserved instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nPurchase 70 reserved instances (RIs) and 30 spot instances\n\nAs 70 instances need to be always available, these can be purchased as reserved instances for a one-year duration.\nThe other 30 instances responsible for the batch job can be purchased as spot instances. Even if some of the spot instances are interrupted, other spot instances can continue with the job.\n\nPlease see this detailed overview of various types of Amazon EC2 instances from a pricing perspective:\n\nvia - https://aws.amazon.com/ec2/pricing/\n\nIncorrect options:\n\nPurchase 70 on-demand instances and 30 spot instances\n\nPurchase 70 on-demand instances and 30 reserved instances\n\nPurchasing 70 on-demand instances would be costlier than 70 reserved instances, so these two options are ruled out.\n\nPurchase 70 reserved instances and 30 on-demand instances - Purchasing 30 instances as on-demand instances to handle the batch jobs would not be cost-optimal as these instances don't need to be always available. Spot instances are better at handling such batch jobs. So this option is not correct.\n\nReference:\n\nhttps://aws.amazon.com/ec2/pricing/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q401",
    "questionText": "You have multiple AWS accounts within a single AWS Region managed by AWS Organizations and you would like to ensure all Amazon EC2 instances in all these accounts can communicate privately. Which of the following solutions provides the capability at the CHEAPEST cost?",
    "options": [
      {
        "text": "Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager",
        "isCorrect": true
      },
      {
        "text": "Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together",
        "isCorrect": false
      },
      {
        "text": "Create a VPC peering connection between all virtual private cloud (VPCs)",
        "isCorrect": false
      },
      {
        "text": "Create a Private Link between all the Amazon EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager\n\nAWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge.\n\nThe correct solution is to share the subnet(s) within a VPC using RAM. This will allow all Amazon EC2 instances to be deployed in the same VPC (although from different accounts) and easily communicate with one another.\n\nHow AWS Resource Access Manager (AWS RAM) Works:\n\nvia - https://aws.amazon.com/ram/\n\nIncorrect options:\n\nCreate a Private Link between all the Amazon EC2 instances - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.\nPrivate Link is a distractor in this question. Private Link is leveraged to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering and allowing the connections between the two to remain within the AWS network.\n\nCreate a VPC peering connection between all virtual private cloud (VPCs) - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).\nVPC peering connections will work, but won't efficiently scale if you add more accounts (you'll have to create many connections).\n\nCreate an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together - AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway.\nA Transit Gateway will work but will be an expensive solution. Here we want to minimize cost.\n\nReferences:\n\nhttps://aws.amazon.com/ram/\n\nhttps://aws.amazon.com/privatelink/\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n\nhttps://aws.amazon.com/transit-gateway/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q402",
    "questionText": "What does this IAM policy do?\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Mystery Policy\",\n      \"Action\": [\n        \"ec2:RunInstances\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"IpAddress\": {\n          \"aws:SourceIp\": \"34.50.31.0/24\"\n        }\n      }\n    }\n  ]\n}",
    "options": [
      {
        "text": "It allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block",
        "isCorrect": true
      },
      {
        "text": "It allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block",
        "isCorrect": false
      },
      {
        "text": "It allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block",
        "isCorrect": false
      },
      {
        "text": "It allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nIt allows starting an Amazon EC2 instance only when the IP where the call originates is within the 34.50.31.0/24 CIDR block\n\nYou manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations service control policy (SCPs), access control lists (ACLs), and session policies.\n\nConsider the following snippet from the given policy document:\n\n      \"Condition\": {\n        \"IpAddress\": {\n          \"aws:SourceIp\": \"34.50.31.0/24\"\n        }\n      }\n\n\nThe aws:SourceIP in this condition always represents the IP of the caller of the API. That is very helpful if you want to restrict access to certain AWS API for example from the public IP of your on-premises infrastructure.\n\nPlease see this overview of Elastic vs Public vs Private IP addresses:\n\nelastic IP address (EIP) - An elastic IP address (EIP) is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.\n\nPrivate IP address - A private IPv4 address is an IP address that's not reachable over the Internet. You can use private IPv4 addresses for communication between instances in the same VPC.\n\nPublic IP address - A public IP address is an IPv4 address that's reachable from the Internet. You can use public addresses for communication between your instances and the Internet.\n\nPlease note 34.50.31.0/24 is a public IP range, not a private IP range. Private IP ranges are:\n192.168.0.0 - 192.168.255.255 (65,536 IP addresses)\n172.16.0.0 - 172.31.255.255 (1,048,576 IP addresses)\n10.0.0.0 - 10.255.255.255 (16,777,216 IP addresses)\n\nIncorrect options:\n\nIt allows starting an Amazon EC2 instance only when they have a Public IP within the 34.50.31.0/24 CIDR block\n\nIt allows starting an Amazon EC2 instance only when they have an Elastic IP within the 34.50.31.0/24 CIDR block\n\nIt allows starting an Amazon EC2 instance only when they have a Private IP within the 34.50.31.0/24 CIDR block\n\nEach of these three options suggests that the IP addresses of the Amazon EC2 instances must belong to the 34.50.31.0/24 CIDR block for the EC2 instances to start. Actually, the policy states that the AMazon EC2 instance should start only when the IP where the call originates is within the 34.50.31.0/24 CIDR block. Hence these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/iam-restrict-calls-ip-addresses/\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q403",
    "questionText": "What does this IAM policy do?\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Mystery Policy\",\n      \"Action\": [\n        \"ec2:RunInstances\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:RequestedRegion\": \"eu-west-1\"\n        }\n      }\n    }\n  ]\n}",
    "options": [
      {
        "text": "It allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world",
        "isCorrect": true
      },
      {
        "text": "It allows running Amazon EC2 instances anywhere but in the eu-west-1 region",
        "isCorrect": false
      },
      {
        "text": "It allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region",
        "isCorrect": false
      },
      {
        "text": "It allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nIt allows running Amazon EC2 instances only in the eu-west-1 region, and the API call can be made from anywhere in the world\n\nYou manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations service control policy (SCPs), access control lists (ACLs), and session policies.\n\nYou can use the aws:RequestedRegion key to compare the AWS Region that was called in the request with the Region that you specify in the policy. You can use this global condition key to control which Regions can be requested.\n\naws:RequestedRegion represents the target of the API call. So in this example, we can only launch an Amazon EC2 instance in eu-west-1, and we can do this API call from anywhere.\n\nIncorrect options:\n\nIt allows running Amazon EC2 instances anywhere but in the eu-west-1 region\n\nIt allows running Amazon EC2 instances in any region when the API call is originating from the eu-west-1 region\n\nIt allows running Amazon EC2 instances in the eu-west-1 region, when the API call is made from the eu-west-1 region\n\nThese three options contradict the earlier details provided in the explanation. To summarize, aws:RequestedRegion represents the target of the API call. So, we can only launch an Amazon EC2 instance in eu-west-1 region and we can do this API call from anywhere. Hence these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q404",
    "questionText": "You have a team of developers in your company, and you would like to ensure they can quickly experiment with AWS Managed Policies by attaching them to their accounts, but you would like to prevent them from doing an escalation of privileges, by granting themselves the AdministratorAccess managed policy.\n\n**Question:**\nHow should you proceed?",
    "options": [
      {
        "text": "For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves",
        "isCorrect": true
      },
      {
        "text": "Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves",
        "isCorrect": false
      },
      {
        "text": "Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy",
        "isCorrect": false
      },
      {
        "text": "Attach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nFor each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves\n\nAWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.\nHere we have to use an IAM permission boundary. They can only be applied to roles or users, not IAM groups.\n\nPermissions boundaries for IAM entities:\n\nvia - https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\n\nIncorrect options:\n\nCreate a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the AdministratorAccess policy - Service control policy (SCP) is one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organizationâ€™s access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features. Attaching an SCP to an AWS Organizations entity (root, OU, or account) defines a guardrail for what actions the principals can perform.\nIf you consider this option, since AWS Organizations is not mentioned in this question, so we can't apply an SCP.\n\nAttach an IAM policy to your developers, that prevents them from attaching the AdministratorAccess policy - This option is incorrect as the developers can remove this policy from themselves and escalate their privileges.\n\nPut the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves - IAM permission boundary can only be applied to roles or users, not IAM groups. Hence this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html",
    "awsService": "IAM",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q405",
    "questionText": "You would like to migrate an AWS account from an AWS Organization A to an AWS Organization B. What are the steps do to it?",
    "options": [
      {
        "text": "Remove the member account from the old organization. Send an invite to the member account from the new Organization. Accept the invite to the new organization from the member account",
        "isCorrect": true
      },
      {
        "text": "Send an invite to the new organization. Accept the invite to the new organization from the member account. Remove the member account from the old organization",
        "isCorrect": false
      },
      {
        "text": "Send an invite to the new organization. Remove the member account from the old organization. Accept the invite to the new organization from the member account",
        "isCorrect": false
      },
      {
        "text": "Open an AWS Support ticket to ask them to migrate the account",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nRemove the member account from the old organization. Send an invite to the member account from the new Organization. Accept the invite to the new organization from the member account\n\nAWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Using AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. Through integrations with other AWS services, you can use Organizations to define central configurations and resource sharing across accounts in your organization.\n\nTo migrate accounts from one organization to another, you must have root or IAM access to both the member and master accounts. Here are the steps to follow:\n1. Remove the member account from the old organization\n2. Send an invite to the member account from the new Organization\n3. Accept the invite to the new organization from the member account\n\nIncorrect options:\n\nSend an invite to the new organization. Accept the invite to the new organization from the member account. Remove the member account from the old organization\n\nSend an invite to the new organization. Remove the member account from the old organization. Accept the invite to the new organization from the member account\n\nThese two options contradict the steps described earlier for account migration from one organization to another.\n\nOpen an AWS Support ticket to ask them to migrate the account - You don't need to contact AWS support for account migration.\n\nReferences:\n\nhttps://aws.amazon.com/organizations/\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/",
    "awsService": "General",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q406",
    "questionText": "You would like to use AWS Snowball to move on-premises backups into a long term archival tier on AWS. Which solution provides the MOST cost savings?",
    "options": [
      {
        "text": "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day",
        "isCorrect": true
      },
      {
        "text": "Create an AWS Snowball job and target a Amazon S3 Glacier Vault",
        "isCorrect": false
      },
      {
        "text": "Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day",
        "isCorrect": false
      },
      {
        "text": "Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day\n\nAWS Snowball, a part of the AWS Snow Family, is a data migration and edge computing device that comes in two options. Snowball Edge Storage Optimized devices provide both block storage and Amazon S3-compatible object storage, and 40 vCPUs. They are well suited for local storage and large scale data transfer. AWS Snowball Edge Compute Optimized devices provide 52 vCPUs, block and object storage, and an optional GPU for use cases like advanced machine learning and full-motion video analysis in disconnected environments.\n\nAWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 terabytes of usable HDD storage, 40 vCPUs, 1 terabyte of SATA SSD storage, and up to 40 gigabytes network connectivity to address large scale data transfer and pre-processing use cases.\n\nThe original AWS Snowball devices were transitioned out of service and AWS Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the AWS Snowball device on the exam, just remember that the original AWS Snowball device had 80 terabytes of storage space.\n\nFor this scenario, you will want to minimize the time spent in Amazon S3 Standard for all files to avoid unintended Amazon S3 Standard storage charges. To do this, AWS recommends using a zero-day lifecycle policy. From a cost perspective, when using a zero-day lifecycle policy, you are only charged Amazon S3 Glacier Deep Archive rates. When billed, the lifecycle policy is accounted for first, and if the destination is Amazon S3 Glacier Deep Archive, you are charged Amazon S3 Glacier Deep Archive rates for the transferred files.\n\nYou can't move data directly from AWS Snowball into Amazon S3 Glacier, you need to go through Amazon S3 first, and then use a lifecycle policy. So this option is correct.\n\nIncorrect options:\n\nCreate an AWS Snowball job and target a Amazon S3 Glacier Vault\n\nCreate a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault\n\nAmazon S3 Glacier and S3 Glacier Deep Archive are a secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.\nFinally, Amazon S3 Glacier Deep Archive provides more cost savings than Amazon S3 Glacier.\n\nBoth these options are incorrect as you can't move data directly from AWS Snowball into a Amazon S3 Glacier Vault or a Glacier Deep Archive Vault. You need to go through Amazon S3 first and then use a lifecycle policy.\n\nCreate an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day - As Amazon S3 Glacier Deep Archive provides more cost savings than Amazon S3 Glacier, you should use Amazon S3 Glacier Deep Archive for long term archival for this use-case.\n\nReferences:\n\nhttps://aws.amazon.com/snowball/features/\n\nhttps://aws.amazon.com/glacier/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q407",
    "questionText": "You are establishing a monitoring solution for desktop systems, that will be sending telemetry data into AWS every 1 minute. Data for each system must be processed in order, independently, and you would like to scale the number of consumers to be possibly equal to the number of desktop systems that are being monitored.\n\nWhat do you recommend?",
    "options": [
      {
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID",
        "isCorrect": true
      },
      {
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is",
        "isCorrect": false
      },
      {
        "text": "Use an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is",
        "isCorrect": false
      },
      {
        "text": "Use an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and make sure the telemetry data is sent with a Group ID attribute representing the value of the Desktop ID\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\nWe, therefore, need to use an SQS FIFO queue. If we don't specify a GroupID, then all the messages are in absolute order, but we can only have 1 consumer at most.\nTo allow for multiple consumers to read data for each Desktop application, and to scale the number of consumers, we should use the \"Group ID\" attribute. So this is the correct option.\n\nIncorrect options:\n\nUse an Amazon Simple Queue Service (Amazon SQS) FIFO (First-In-First-Out) queue, and send the telemetry data as is - This is incorrect because if we send the telemetry data as is then we will not be able to scale the number of consumers to be equal to the number of desktop systems. In this case, each message will have its consumer. So we should use the \"Group ID\" attribute so that multiple consumers can read data for each Desktop application.\n\nUse an Amazon Simple Queue Service (Amazon SQS) standard queue, and send the telemetry data as is - An Amazon SQS standard queue has no ordering capability so that's ruled out.\n\nUse an Amazon Kinesis Data Stream, and send the telemetry data with a Partition ID that uses the value of the Desktop ID - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\nA Kinesis Data Stream would work and would give us the data for each desktop application within shards, but we can only have as many consumers as shards in Kinesis (which is in practice, much less than the number of producers).\n\nReferences:\n\nhttps://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/\n\nhttps://aws.amazon.com/sqs/faqs/\n\nhttps://aws.amazon.com/kinesis/data-streams/faqs/",
    "awsService": "SQS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q408",
    "questionText": "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect.\n\nWhat do you recommend?",
    "options": [
      {
        "text": "VPC Peering Connection",
        "isCorrect": false
      },
      {
        "text": "Virtual private gateway (VGW)",
        "isCorrect": false
      },
      {
        "text": "AWS Transit Gateway",
        "isCorrect": true
      },
      {
        "text": "AWS PrivateLink",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAWS Transit Gateway\n\nAWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes.\nSo, this is a perfect use-case for the Transit Gateway.\n\nWithout AWS Transit Gateway\n\nvia - https://aws.amazon.com/transit-gateway/\n\nWith AWS Transit Gateway\n\nvia - https://aws.amazon.com/transit-gateway/\n\nIncorrect options:\n\nVPC Peering Connection - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).\n\nVPC Peering helps connect two VPCs and is not transitive. It would require to create many peering connections between all the VPCs to have them connect. This alone wouldn't work, because we would need to also connect the on-premises data center through Direct Connect and Direct Connect Gateway, but that's not mentioned in this answer.\n\nVirtual private gateway (VGW) - A virtual private gateway (VGW), also known as a VPN Gateway, is the endpoint on the VPC side of your VPN connection. You can create a virtual private gateway before creating the VPC itself.\nVPN Gateway is a distractor here because we haven't mentioned a VPN.\n\nAWS PrivateLink - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.\nPrivate Link is utilized to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering, and allowing the connections between the two to remain within the AWS network.\n\nReferences:\n\nhttps://aws.amazon.com/transit-gateway/\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVpnGateway.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q409",
    "questionText": "What is true about Amazon RDS Read Replicas encryption?",
    "options": [
      {
        "text": "If the master database is encrypted, the read replicas are encrypted",
        "isCorrect": true
      },
      {
        "text": "If the master database is encrypted, the read replicas can be either encrypted or unencrypted",
        "isCorrect": false
      },
      {
        "text": "If the master database is unencrypted, the read replicas can be either encrypted or unencrypted",
        "isCorrect": false
      },
      {
        "text": "If the master database is unencrypted, the read replicas are encrypted",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nIf the master database is encrypted, the read replicas are encrypted\n\nAmazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.\n\nOn a database instance running with Amazon RDS encryption, data stored at rest in the underlying storage is encrypted, as are its automated backups, read replicas, and snapshots. Therefore, this option is correct.\n\nAmazon RDS Read Replica Overview:\n\nvia - https://aws.amazon.com/rds/features/read-replicas/\n\nIncorrect options:\n\nIf the master database is encrypted, the read replicas can be either encrypted or unencrypted - If the master database is encrypted, the read replicas are necessarily encrypted, so this option is incorrect.\n\nIf the master database is unencrypted, the read replicas can be either encrypted or unencrypted\n\nIf the master database is unencrypted, the read replicas are encrypted\n\nIf the master database is not encrypted, the read replicas cannot be encrypted, so both these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/read-replicas/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q410",
    "questionText": "Upon a security review of your AWS account, an AWS consultant has found that a few Amazon RDS databases are unencrypted. As a Solutions Architect, what steps must be taken to encrypt the Amazon RDS databases?",
    "options": [
      {
        "text": "Take a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database",
        "isCorrect": true
      },
      {
        "text": "Create a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database",
        "isCorrect": false
      },
      {
        "text": "Enable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ",
        "isCorrect": false
      },
      {
        "text": "Enable encryption on the Amazon RDS database using the AWS Console",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nTake a snapshot of the database, copy it as an encrypted snapshot, and restore a database from the encrypted snapshot. Terminate the previous database\n\nAmazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.\n\nYou can encrypt your Amazon RDS DB instances and snapshots at rest by enabling the encryption option for your Amazon RDS DB instances. Data that is encrypted at rest includes the underlying storage for DB instances, its automated backups, read replicas, and snapshots.\n\nYou can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created. However, because you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. So this is the correct option.\n\nIncorrect options:\n\nCreate a Read Replica of the database, and encrypt the read replica. Promote the read replica as a standalone database, and terminate the previous database - If the master is not encrypted, the read replicas cannot be encrypted. So this option is incorrect.\n\nEnable Multi-AZ for the database, and make sure the standby instance is encrypted. Stop the main database to that the standby database kicks in, then disable Multi-AZ - Multi-AZ is to help with High Availability, not encryption. So this option is incorrect.\n\nEnable encryption on the Amazon RDS database using the AWS Console - There is no direct option to encrypt an Amazon RDS database using the AWS Console.\n\nSteps to encrypt an un-encrypted RDS database:\n1. Create a snapshot of the un-encrypted database\n2. Copy the snapshot and enable encryption for the snapshot\n3. Restore the database from the encrypted snapshot\n4. Migrate applications to the new database, and delete the old database\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q411",
    "questionText": "A company has historically operated only in the us-east-1 region and stores encrypted data in Amazon S3 using SSE-KMS. As part of enhancing its security posture as well as improving the backup and recovery architecture, the company wants to store the encrypted data in Amazon S3 that is replicated into the us-west-1 AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions.\n\nWhich of the following represents the best solution to address these requirements?",
    "options": [
      {
        "text": "Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets",
        "isCorrect": false
      },
      {
        "text": "Create a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region",
        "isCorrect": true
      },
      {
        "text": "Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region",
        "isCorrect": false
      },
      {
        "text": "Enable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a new Amazon S3 bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. Enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in us-east-1 region into this new Amazon S3 bucket in us-east-1 region\n\nAWS KMS supports multi-region keys, which are AWS KMS keys in different AWS regions that can be used interchangeably â€“ as though you had the same key in multiple regions. Each set of related multi-region keys has the same key material and key ID, so you can encrypt data in one AWS region and decrypt it in a different AWS region without re-encrypting or making a cross-region call to AWS KMS.\n\nYou can use multi-region AWS KMS keys in Amazon S3. However, Amazon S3 currently treats multi-region keys as though they were single-region keys, and does not use the multi-region features of the key.\n\nMulti-region AWS KMS keys:\n\nvia - https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\n\nFor the given use case, you must create a new bucket in the us-east-1 region with replication enabled from this new bucket into another bucket in us-west-1 region. This would ensure that the data is available in another region for backup and recovery purposes. You should also enable SSE-KMS encryption on the new bucket in us-east-1 region by using an AWS KMS multi-region key so that the data can be encrypted and decrypted using the same key in both AWS regions. Since the existing data in the current bucket was encrypted using the AWS KMS key restricted to the us-east-1 region, so data must be copied to the new bucket in us-east-1 region for replication as well as multi-region KMS key based encryption to kick-in.\n\nTo require server-side encryption of all objects in a particular Amazon S3 bucket, you can use a policy. For example, the following bucket policy denies the upload object (s3:PutObject) permission to everyone if the request does not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS.\n\n{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}\n\n\nThe following example IAM policies show statements for using AWS KMS server-side encryption with replication.\n\nIn this example, the encryption context is the object ARN. If you use SSE-KMS with an Amazon S3 Bucket Key enabled, you must use the bucket ARN as the encryption context.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n            \"Action\": [\"kms:Decrypt\"],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"List of AWS KMS key ARNs used to encrypt source objects.\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"kms:ViaService\": \"s3.source-bucket-region.amazonaws.com\",\n                    \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::source-bucket-name/key-prefix1/*\"\n                }\n            }\n        },\n\n        {\n            \"Action\": [\"kms:Encrypt\"],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"AWS KMS key ARNs (for the AWS Region of the destination bucket 1). Used to encrypt object replicas created in destination bucket 1.\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"kms:ViaService\": \"s3.destination-bucket-1-region.amazonaws.com\",\n                    \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::destination-bucket-name-1/key-prefix1/*\"\n                }\n            }\n        },\n        {\n            \"Action\": [\"kms:Encrypt\"],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"AWS KMS key ARNs (for the AWS Region of destination bucket 2). Used to encrypt object replicas created in destination bucket 2.\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"kms:ViaService\": \"s3.destination-bucket-2-region.amazonaws.com\",\n                    \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::destination-bucket-2-name/key-prefix1*\"\n                }\n            }\n        }\n    ]\n}\n\n\nIncorrect options:\n\nChange the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region - Amazon S3 batch replication can certainly be used to replicate the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region.\n\nHowever, you cannot convert an existing single-Region key to a multi-Region key. This design ensures that all data protected with existing single-Region keys maintain the same data residency and data sovereignty properties. So this option is incorrect.\n\nEnable replication for the current bucket in us-east-1 region into another bucket in us-west-1 region. Share the existing AWS KMS key from us-east-1 region to us-west-1 region - You cannot share an AWS KMS key to another region, so this option is incorrect.\n\nCreate an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in us-east-1 region to the destination bucket in us-west-1 region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets - This option is a distractor as the daily frequency of data replication would result in significant data loss in case of a disaster. In addition, this option involves significant development effort to create the functionality to reliably replicate the data from source to destination buckets. So this option is not the best fit for the given use case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q412",
    "questionText": "You would like to mount a network file system on Linux instances, where files will be stored and accessed frequently at first, and then infrequently. What solution is the MOST cost-effective?",
    "options": [
      {
        "text": "Amazon EFS Infrequent Access",
        "isCorrect": true
      },
      {
        "text": "Amazon S3 Glacier Deep Archive",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Intelligent Tiering",
        "isCorrect": false
      },
      {
        "text": "Amazon FSx for Lustre",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon EFS Infrequent Access\n\nAmazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability.\n\nAmazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files, not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. Therefore, this is the correct option.\n\nHow Amazon EFS works:\n\nvia - https://aws.amazon.com/efs/\n\nIncorrect options:\n\nAmazon S3 Intelligent Tiering - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.\n\nYou can't mount a network file system on Amazon S3 Intelligent Tiering as it's an object storage service, so this option is incorrect.\n\nAmazon S3 Glacier Deep Archive - Amazon S3 Glacier and S3 Glacier Deep Archive are a secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.\n\nYou can't mount a network file system on Amazon S3 Glacier or S3 Glacier Deep Archive. These are data archiving solutions and hence this option is incorrect.\n\nAmazon FSx for Lustre - Amazon FSx for Lustre makes it easy and cost-effective to launch and run the worldâ€™s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters.\n\nAmazon FSx for Lustre is a file system better suited for distributed computing for HPC (high-performance computing) and is very expensive\n\nReferences:\n\nhttps://aws.amazon.com/efs/\n\nhttps://aws.amazon.com/efs/features/infrequent-access/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q413",
    "questionText": "Consider the following policy associated with an IAM group containing several users:\n\n{\n    \"Version\":\"2012-10-17\",\n    \"Id\":\"EC2TerminationPolicy\",\n    \"Statement\":[\n        {\n            \"Effect\":\"Deny\",\n            \"Action\":\"ec2:*\",\n            \"Resource\":\"*\",\n            \"Condition\":{\n                \"StringNotEquals\":{\n                    \"ec2:Region\":\"us-west-1\"\n                }\n            }\n        },\n        {\n            \"Effect\":\"Allow\",\n            \"Action\":\"ec2:TerminateInstances\",\n            \"Resource\":\"*\",\n            \"Condition\":{\n                \"IpAddress\":{\n                    \"aws:SourceIp\":\"10.200.200.0/24\"\n                }\n            }\n        }\n    ]\n}\n\n\nWhich of the following options is correct?",
    "options": [
      {
        "text": "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200",
        "isCorrect": true
      },
      {
        "text": "Users belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200",
        "isCorrect": false
      },
      {
        "text": "Users belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200",
        "isCorrect": false
      },
      {
        "text": "Users belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUsers belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200\n\nThe given policy denies all EC2 specification actions on all resources when the region of the underlying resource is not us-west-1. The policy allows the terminate EC2 action on all resources when the source IP address is in the CIDR range 10.200.200.0/24, therefore it would allow the user with the source IP 10.200.200.200 to terminate the Amazon EC2 instance.\n\nIncorrect options:\n\nUsers belonging to the IAM user group cannot terminate an Amazon EC2 instance in the us-west-1 region when the user's source IP is 10.200.200.200\n\nUsers belonging to the IAM user group can terminate an Amazon EC2 instance in the us-west-1 region when the EC2 instance's IP address is 10.200.200.200\n\nUsers belonging to the IAM user group can terminate an Amazon EC2 instance belonging to any region except the us-west-1 region when the user's source IP is 10.200.200.200\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q414",
    "questionText": "Your company has a monthly big data workload, running for about 2 hours, which can be efficiently distributed across multiple servers of various sizes, with a variable number of CPUs. The solution for the workload should be able to withstand server failures.\n\nWhich is the MOST cost-optimal solution for this workload?",
    "options": [
      {
        "text": "Run the workload on a Spot Fleet",
        "isCorrect": true
      },
      {
        "text": "Run the workload on Spot Instances",
        "isCorrect": false
      },
      {
        "text": "Run the workload on Reserved Instances (RI)",
        "isCorrect": false
      },
      {
        "text": "Run the workload on Dedicated Hosts",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nRun the workload on a Spot Fleet\n\nThe Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.\n\nA Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Spot Instances provide great cost efficiency, but we need to select an instance type in advance. In this case, we want to use the most cost-optimal option and leave the selection of the cheapest spot instance to a Spot Fleet request, which can be optimized with the lowestPrice strategy. So this is the correct option.\n\nKey Spot Instance Concepts:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\n\nIncorrect options:\n\nRun the workload on Spot Instances - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. Only spot fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated, so spot instances, by themselves, are not the right fit for this use-case.\n\nRun the workload on Reserved Instances (RI) -  Reserved Instances are less cost-optimized than Spot Instances, and most efficient when used continuously. Here the workload is once a month, so this is not efficient.\n\nRun the workload on Dedicated Hosts - Amazon EC2 Dedicated Hosts allow you to use your eligible software licenses from vendors such as Microsoft and Oracle on Amazon EC2 so that you get the flexibility and cost-effectiveness of using your licenses, but with the resiliency, simplicity, and elasticity of AWS. An Amazon EC2 Dedicated Host is a physical server fully dedicated for your use, so you can help address corporate compliance requirement. They're not particularly cost-efficient. So this option is not correct.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html#spot-fleet-allocation-strategy",
    "awsService": "General",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q415",
    "questionText": "A financial services company wants to store confidential data in Amazon S3 and it needs to meet the following data security and compliance norms:\n\n\nEncryption key usage must be logged for auditing purposes\nEncryption Keys must be rotated every year\nThe data must be encrypted at rest\n\n\nWhich is the MOST operationally efficient solution?",
    "options": [
      {
        "text": "Server-side encryption (SSE-S3) with automatic key rotation",
        "isCorrect": false
      },
      {
        "text": "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation",
        "isCorrect": true
      },
      {
        "text": "Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation",
        "isCorrect": false
      },
      {
        "text": "Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nServer-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with automatic key rotation\n\nServer-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it.\n\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 are automatically encrypted at no additional cost and with no impact on performance.\n\nAmazon S3 server-side encryption\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\n\nAWS KMS is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses server-side encryption with AWS KMS (SSE-KMS) to encrypt your S3 object data. Also, when SSE-KMS is requested for the object, the S3 checksum as part of the object's metadata, is stored in encrypted form.\n\nIf you use KMS keys, you can use AWS KMS through the AWS Management Console or the AWS KMS API to do the following:\n\n\nCentrally create, view, edit, monitor, enable or disable, rotate, and schedule deletion of KMS keys.\nDefine the policies that control how and by whom KMS keys can be used.\nAudit their usage to prove that they are being used correctly. Auditing is supported by the AWS KMS API, but not by the AWS KMSAWS Management Console.\n\n\nWhen you enable automatic key rotation for a KMS key, AWS KMS generates new cryptographic material for the KMS key every year.\n\nAWS KMS keys:\n\nvia - https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\n\nFor the given use case, you can set up server-side encryption with AWS KMS Keys (SSE-KMS) with automatic key rotation.\n\nIncorrect options:\n\nServer-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) with manual key rotation - Although it is possible to manually rotate the AWS KMS key, it is not the best fit solution as it is not operationally efficient.\n\nServer-side encryption (SSE-S3) with automatic key rotation - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates. However, with SSE-S3, you cannot log the usage of the encryption key for auditing purposes. So this option is incorrect.\n\nServer-side encryption with customer-provided keys (SSE-C) with automatic key rotation - It is possible to automatically rotate the customer-provided keys but you will need to develop the underlying solution to automate the key rotation. Therefore, this option is not operationally efficient.\n\nReferences:\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q416",
    "questionText": "A Machine Learning research group uses a proprietary computer vision application hosted on an Amazon EC2 instance. Every time the instance needs to be stopped and started again, the application takes about 3 minutes to start as some auxiliary software programs need to be executed so that the application can function. The research group would like to minimize the application boostrap time whenever the system needs to be stopped and then started at a later point in time.\n\nAs a solutions architect, which of the following solutions would you recommend for this use-case?",
    "options": [
      {
        "text": "Use Amazon EC2 Instance Hibernate",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EC2 User-Data",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 Meta-Data",
        "isCorrect": false
      },
      {
        "text": "Create an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon EC2 Instance Hibernate\n\nWhen you hibernate an instance, AWS signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon EBS root volume. AWS then persists the instance's Amazon EBS root volume and any attached Amazon EBS data volumes.\n\nWhen you start your instance:\n\nThe Amazon EBS root volume is restored to its previous state\n\nThe RAM contents are reloaded\n\nThe processes that were previously running on the instance are resumed\n\nPreviously attached data volumes are reattached and the instance retains its instance ID\n\nOverview of Amazon EC2 hibernation:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html\n\nBy using Amazon EC2 hibernate, we have the capability to resume it at any point of time, with the application already launched, thus helping us cut the 3 minutes start time.\n\nIncorrect options:\n\nUse Amazon EC2 User-Data - Amazon EC2 instance user data is the data that you specified in the form of a configuration script while launching your instance. Here, the problem is that the application takes 3 minutes to launch, no matter what. EC2 user data won't help us because it's just here to help us execute a list of commands, not speed them up.\n\nUse Amazon EC2 Meta-Data - Amazon EC2 instance metadata is data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories, for example, host name, events, and security groups. The EC2 meta-data is a distractor and can only help us determine some metadata attributes on our EC2 instances.\n\nCreate an Amazon Machine Image (AMI) and launch your Amazon EC2 instances from that - An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need instances with different configurations.\n\nCreating an AMI may help with all the system dependencies, but it won't help us with speeding up the application start time.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q417",
    "questionText": "A media company is migrating its flagship application from its on-premises data center to AWS for improving the application's read-scaling capability as well as its availability. The existing architecture leverages a Microsoft SQL Server database that sees a heavy read load. The engineering team does a full copy of the production database at the start of the business day to populate a dev database. During this period, application users face high latency leading to a bad user experience.\n\nThe company is looking at alternate database options and migrating database engines if required. What would you suggest?",
    "options": [
      {
        "text": "Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database",
        "isCorrect": false
      },
      {
        "text": "Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database",
        "isCorrect": false
      },
      {
        "text": "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora",
        "isCorrect": true
      },
      {
        "text": "Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nLeverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora\n\nAmazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. An Amazon Aurora DB cluster consists of one or more DB instances and a cluster volume that manages the data for those DB instances. An Aurora cluster volume is a virtual database storage volume that spans multiple Availability Zones (AZs), with each Availability Zone (AZ) having a copy of the Amazon Aurora DB cluster data. Aurora supports Multi-AZ Aurora Replicas that improve the application's read-scaling and availability.\n\nAmazon Aurora Overview:\n\nvia - https://aws.amazon.com/rds/aurora/\n\nAurora backs up your cluster volume automatically and retains restore data for the length of the backup retention period. Aurora backups are continuous and incremental so you can quickly restore to any point within the backup retention period. No performance impact or interruption of database service occurs as backup data is being written.\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html\n\nAutomated backups occur daily during the preferred backup window. If the backup requires more time than allotted to the backup window, the backup continues after the window ends, until it finishes. The backup window can't overlap with the weekly maintenance window for the DB cluster. Aurora backups are continuous and incremental, but the backup window is used to create a daily system backup that is preserved within the backup retention period. The latest restorable time for a DB cluster is the most recent point at which you can restore your DB cluster, typically within 5 minutes of the current time.\n\nFor the given use case, you can create the dev database by restoring from the automated backups of Amazon Aurora.\n\nIncorrect options:\n\nLeverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump - Restoring the dev database via mysqldump would still result in a significant load on the primary DB, so this option fails to address the given requirement.\n\nLeverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database - The standby is there just for handling failover in a Multi-AZ deployment. You cannot access the standby instance and use it as a dev database. Hence this option is incorrect.\n\nLeverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database - Amazon RDS supports Multi-AZ deployments for Microsoft SQL Server by using either SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs). Amazon RDS monitors and maintains the health of your Multi-AZ deployment.\n\nMulti-AZ deployments provide increased availability, data durability, and fault tolerance for DB instances. In the event of planned database maintenance or unplanned service disruption, Amazon RDS automatically fails over to the up-to-date secondary DB instance. For SQL Server, I/O activity is suspended briefly during backup for Multi-AZ deployments.\n\nA read replica is only meant to serve read traffic. The primary purpose of the read replica is to replicate the data in the primary DB instance. A read replica cannot be used as a dev database because it does not allow any database write operations.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q418",
    "questionText": "A media publishing company is migrating its legacy content management application to AWS. Currently, the application and its MySQL database run on a single on-premises virtual machine, which creates a single point of failure and limits scalability. As traffic has increased due to growing reader engagement and video uploads, the company needs to redesign the solution to ensure automatic scaling, high availability, and separation of application and database layers. The company wants to continue using a MySQL-compatible engine and needs a cost-effective, managed solution that minimizes operational overhead.\n\nWhich AWS architecture will best fulfill these requirements?",
    "options": [
      {
        "text": "Migrate the application to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. Use Amazon Aurora Serverless v2 for MySQL to manage the database layer with auto-scaling and built-in high availability",
        "isCorrect": true
      },
      {
        "text": "Deploy the application to EC2 instances registered in a Network Load Balancer target group. Use Amazon ElastiCache for Redis as the database and configure it with Redis Streams for persistent storage",
        "isCorrect": false
      },
      {
        "text": "Containerize the application and deploy it to Amazon ECS with EC2 launch type behind an Application Load Balancer. Use Amazon Neptune to store structured relational data with SQL-like queries",
        "isCorrect": false
      },
      {
        "text": "Host the application on EC2 instances that are part of a target group for an Application Load Balancer. Create an Amazon RDS for MySQL Multi-AZ DB instance to provide high availability and automatic failover for the database",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nMigrate the application to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. Use Amazon Aurora Serverless v2 for MySQL to manage the database layer with auto-scaling and built-in high availability\n\nThis architecture provides automatic horizontal scaling for the application via an Auto Scaling group and managed, serverless scaling for the database layer with Amazon Aurora Serverless v2 for MySQL. Aurora Serverless v2 automatically adjusts capacity based on workload, and offers multi-AZ high availability, backups, and failoverâ€”all without requiring the company to manage the underlying database servers. It supports MySQL compatibility and is a cost-effective alternative to provisioned RDS when workloads are variable.\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.how-it-works.html\n\nIncorrect options:\n\nDeploy the application to EC2 instances registered in a Network Load Balancer target group. Use Amazon ElastiCache for Redis as the database and configure it with Redis Streams for persistent storage - ElastiCache is a key-value store designed for caching, not as a replacement for a relational MySQL database. Redis does not support relational joins, transactions, or ACID compliance in the way MySQL does. Using it as a persistent store for structured application data would require significant re-architecture and compromises consistency.\n\nContainerize the application and deploy it to Amazon ECS with EC2 launch type behind an Application Load Balancer. Use Amazon Neptune to store structured relational data with SQL-like queries - Amazon Neptune is a graph database designed for workloads that require graph models like social networks, recommendation engines, and knowledge graphs. It does not support SQL or relational schemas and is not compatible with MySQL. Using Neptune in place of a MySQL database would require entirely rearchitecting the data model and application queries.\n\nHost the application on EC2 instances that are part of a target group for an Application Load Balancer. Create an Amazon RDS for MySQL Multi-AZ DB instance to provide high availability and automatic failover for the database - While Amazon RDS for MySQL with Multi-AZ offers high availability, it does not support automatic compute scaling for the database layer. If the workload grows, manual intervention is required to resize the instance or configure read replicas for performance. Also, RDS Multi-AZ provides failover, but not the instantaneous, granular auto-scaling offered by Aurora Serverless v2.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.how-it-works.html\n\nhttps://aws.amazon.com/blogs/database/read-scalability-with-amazon-aurora-serverless-v2/\n\nhttps://aws.amazon.com/blogs/database/introducing-scaling-to-0-capacity-with-amazon-aurora-serverless-v2/\n\nhttps://docs.aws.amazon.com/neptune/latest/userguide/intro.html\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/dg/WhatIs.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q419",
    "questionText": "Which of the following IAM policies provides read-only access to the Amazon S3 bucket mybucket and its content?",
    "options": [
      {
        "text": "{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      }\n   ]\n}",
        "isCorrect": true
      },
      {
        "text": "{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\",\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      }\n   ]\n}",
        "isCorrect": false
      },
      {
        "text": "{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\",\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      }\n   ]\n}",
        "isCorrect": false
      },
      {
        "text": "{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      }\n   ]\n}",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\n{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      }\n   ]\n}\n\n\nYou manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, service control policy (SCP)of AWS Organizations, access control list (ACL), and session policies.\n\ns3:ListBucket is applied to buckets, so the ARN is in the form  \"Resource\":\"arn:aws:s3:::mybucket\", without a trailing /\ns3:GetObject is applied to objects within the bucket, so the ARN is in the form \"Resource\":\"arn:aws:s3:::mybucket/*\", with a trailing /* to indicate all objects within the bucket mybucket\n\nTherefore, this is the correct option.\n\nIncorrect options:\n\n{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\",\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      }\n   ]\n}\n\n\nThis option is incorrect as it provides read-only access only to the bucket, not its contents.\n\n{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\",\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      }\n   ]\n}\n\n\nThis option is incorrect as it provides read-only access only to the objects within the bucket and it does not provide listBucket permissions to the bucket itself.\n\n{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:ListBucket\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket/*\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"s3:GetObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::mybucket\"\n      }\n   ]\n}\n\n\nThis option is incorrect as it provides listing access only to the bucket contents.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q420",
    "questionText": "An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination, and accesses a PostgreSQL database managed by Amazon RDS.\n\nHow should you configure the security groups? (Select three)",
    "options": [
      {
        "text": "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432",
        "isCorrect": true
      },
      {
        "text": "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80",
        "isCorrect": true
      },
      {
        "text": "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443",
        "isCorrect": true
      },
      {
        "text": "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80",
        "isCorrect": false
      },
      {
        "text": "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432",
        "isCorrect": false
      },
      {
        "text": "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nThe security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432\n\nThe security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80\n\nThe security group of the Application Load Balancer should have an inbound rule from anywhere on port 443\n\nA security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.\n\nThe following are the characteristics of security group rules:\n1. By default, security groups allow all outbound traffic.\n2. Security group rules are always permissive; you can't create rules that deny access.\n3. Security groups are stateful\n\nPostgreSQL port = 5432\nHTTP port = 80\nHTTPS port = 443\n\nThe traffic goes like this :\nThe client sends an HTTPS request to ALB on port 443. This is handled by the rule - \"The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443\"\n\nThe Application Load Balancer then forwards the request to one of the Amazon EC2 instances. This is handled by the rule - \"The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80\"\n\nThe Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432. This is handled by the rule - \"The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432\"\n\nIncorrect options:\n\nThe security group of the Application Load Balancer should have an inbound rule from anywhere on port 80 - The client sends an HTTPS request to ALB on port 443 and not on port 80, so this is incorrect.\n\nThe security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432 - The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer and not from the security group of the Amazon RDS database, so this option is incorrect.\n\nThe security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80 - The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432 and not on port 80, so this option is incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q421",
    "questionText": "Your company has an on-premises Distributed File System Replication (DFSR) service to keep files synchronized on multiple Windows servers, and would like to migrate to AWS cloud.\n\nWhat do you recommend as a replacement for the DFSR?",
    "options": [
      {
        "text": "Amazon FSx for Windows File Server",
        "isCorrect": true
      },
      {
        "text": "Amazon FSx for Lustre",
        "isCorrect": false
      },
      {
        "text": "Amazon Elastic File System (Amazon EFS)",
        "isCorrect": false
      },
      {
        "text": "Amazon Simple Storage Service (Amazon S3)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon FSx for Windows File Server\n\nAmazon FSx for Windows File Server provides fully managed, highly reliable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol.  It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\nThe Distributed File System Replication (DFSR) service is a new multi-master replication engine that is used to keep folders synchronized on multiple servers. Amazon FSx supports the use of Microsoftâ€™s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size.\n\nAmazon FSx for Windows is a perfect distributed file system, with replication capability, and can be mounted on Windows.\n\nHow Amazon FSx for Windows Works:\n\nvia - https://aws.amazon.com/fsx/windows/\n\nIncorrect options:\n\nAmazon FSx for Lustre - Amazon FSx for Lustre makes it easy and cost-effective to launch and run the worldâ€™s most popular high-performance file system. It is used for workloads such as machine learning, high-performance computing (HPC), video processing, and financial modeling. The open-source Lustre file system is designed for applications that require fast storage â€“ where you want your storage to keep up with your compute. Amazon FSx enables you to use Lustre file systems for any workload where storage speed matters. FSx for Lustre integrates with Amazon S3, making it easy to process data sets with the Lustre file system. Amazon FSx for Lustre is for Linux only, so this option is incorrect.\n\nAmazon Elastic File System (Amazon EFS) - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. Amazon EFS is a network file system but for Linux only, so this option is incorrect.\n\nAmazon Simple Storage Service (Amazon S3) - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 cannot be mounted as a file system on Windows, so this option is incorrect.\n\nReferences:\n\nhttps://docs.microsoft.com/en-us/previous-versions/windows/desktop/dfsr/dfsr-overview\n\nhttps://aws.amazon.com/fsx/windows/\n\nhttps://aws.amazon.com/fsx/lustre/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q422",
    "questionText": "A retail company wants to share sensitive accounting data that is stored in an Amazon RDS database instance with an external auditor. The auditor has its own AWS account and needs its own copy of the database.\n\nWhich of the following would you recommend to securely share the database with the auditor?",
    "options": [
      {
        "text": "Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key",
        "isCorrect": true
      },
      {
        "text": "Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket",
        "isCorrect": false
      },
      {
        "text": "Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket",
        "isCorrect": false
      },
      {
        "text": "Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key\n\nYou can share the AWS Key Management Service (AWS KMS) key that was used to encrypt the snapshot with any accounts that you want to be able to access the snapshot. You can share AWS KMS Key with another AWS account by adding the other account to the AWS KMS key policy.\n\nMaking an encrypted snapshot of the database will give the auditor a copy of the database, as required for the given use case.\n\nIncorrect options:\n\nCreate a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket - Amazon RDS stores the DB snapshots in the Amazon S3 bucket belonging to the same AWS region where the Amazon RDS instance is located. Amazon RDS stores these on your behalf and you do not have direct access to these snapshots in Amazon S3, so it's not possible to grant access to the snapshot objects in Amazon S3.\n\nExport the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket - This solution is feasible though not optimal. It requires a lot of unnecessary work and is difficult to audit when such bulk data is exported into text files.\n\nSet up a read replica of the database and configure IAM standard database authentication to grant the auditor access - Read Replicas make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Creating Read Replicas for audit purposes is overkill. Also, the question mentions that the auditor needs to have their own copy of the database, which is not possible with replicas.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q423",
    "questionText": "Your company has deployed an application that will perform a lot of overwrites and deletes on data and require the latest information to be available anytime data is read via queries on database tables.\n\nAs a Solutions Architect, which database technology will you recommend?",
    "options": [
      {
        "text": "Amazon Relational Database Service (Amazon RDS)",
        "isCorrect": true
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": false
      },
      {
        "text": "Amazon Neptune",
        "isCorrect": false
      },
      {
        "text": "Amazon Simple Storage Service (Amazon S3)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon Relational Database Service (Amazon RDS)\n\nAmazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. RDS allows you to create, read, update, and delete records without any item lock or ambiguity. All RDS transactions must be ACID compliant or be Atomic, Consistent, Isolated, and Durable to ensure data integrity.\n\nAtomicity requires that either transaction as a whole is successfully executed or if a part of the transaction fails, then the entire transaction be invalidated. Consistency mandates the data written to the database as part of the transaction must adhere to all defined rules, and restrictions including constraints, cascades, and triggers. Isolation is critical to achieving concurrency control and makes sure each transaction is independent unto itself. Durability requires that all of the changes made to the database be permanent once a transaction is completed.\nHence, the best fit is Amazon RDS.\n\nIncorrect options:\n\nAmazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could work but it's a better fit as a caching technology to enhance reads.\n\nAmazon Simple Storage Service (Amazon S3) - This option is incorrect as Amazon S3 is not a database technology that supports queries on database tables out of the box. It is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3.\n\nAfter a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. Amazon S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected. Strong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects.\n\nAmazon Neptune - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency.\n\nAmazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS encrypted client connections and encryption at rest. Amazon Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups. Amazon Neptune is a graph database so it's not a good fit.\n\nReferences:\n\nhttps://aws.amazon.com/relational-database/\n\nhttps://aws.amazon.com/rds/\n\nhttps://aws.amazon.com/neptune/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q424",
    "questionText": "A manufacturing company receives unreliable service from its data center provider because the company is located in an area prone to natural disasters. The company is not ready to fully migrate to the AWS Cloud, but it wants a failover environment on AWS in case the on-premises data center fails. The company runs web servers that connect to external vendors. The data available on AWS and on-premises must be uniform.\n\nWhich of the following solutions would have the LEAST amount of downtime?",
    "options": [
      {
        "text": "Set up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Set up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3",
        "isCorrect": true
      },
      {
        "text": "Set up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Set up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up a Amazon Route 53 failover record. Run application servers on Amazon EC2 instances behind an Application Load Balancer in an Auto Scaling group. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3\n\nIf you have multiple resources that perform the same function, you can configure DNS failover so that Route 53 will route your traffic from an unhealthy resource to a healthy resource.\n\nElastic Load Balancing is used to automatically distribute your incoming application traffic across all the Amazon EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group.\n\nAWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. It provides low-latency performance by caching frequently accessed data on-premises while storing data securely and durably in Amazon cloud storage services. Storage Gateway optimizes data transfer to AWS by sending only changed data and compressing data. Storage Gateway also integrates natively with Amazon S3 cloud storage which makes your data available for in-cloud processing.\n\nIncorrect options:\n\nSet up a Amazon Route 53 failover record. Execute an AWS CloudFormation template from a script to provision Amazon EC2 instances behind an Application Load Balancer. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3\n\nSet up a Amazon Route 53 failover record. Run an AWS Lambda function to execute an AWS CloudFormation template to launch two Amazon EC2 instances. Set up AWS Storage Gateway with stored volumes to back up data to Amazon S3. Set up an AWS Direct Connect connection between a VPC and the data center\n\nSet up a Amazon Route 53 failover record. Set up an AWS Direct Connect connection between a VPC and the data center. Run application servers on Amazon EC2 in an Auto Scaling group. Run an AWS Lambda function to execute an AWS CloudFormation template to create an Application Load Balancer\n\nAWS CloudFormation is a convenient provisioning mechanism for a broad range of AWS and third-party resources. It supports the infrastructure needs of many different types of applications such as existing enterprise applications, legacy applications, applications built using a variety of AWS resources, and container-based solutions.\n\nThese three options involve AWS CloudFormation as part of the solution. Now, AWS CloudFormation takes time to provision the resources and hence is not the right solution when LEAST amount of downtime is mandated for the given use case. Therefore, these options are not the right fit for the given requirement.\n\nReferences:\n\nhttps://aws.amazon.com/route53/\n\nhttps://aws.amazon.com/storagegateway/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q425",
    "questionText": "A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of Amazon EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed.\n\nWhich of the following options represents the best solution for the given requirements?",
    "options": [
      {
        "text": "Amazon Elastic File System (EFS) Standardâ€“IA storage class",
        "isCorrect": true
      },
      {
        "text": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class",
        "isCorrect": false
      },
      {
        "text": "Amazon Elastic File System (EFS) Standard storage class",
        "isCorrect": false
      },
      {
        "text": "Amazon Elastic Block Store (EBS)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon Elastic File System (EFS) Standardâ€“IA storage class - Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently accessible storage for up to thousands of Amazon EC2 instances.\n\nThe Amazon S3 Standardâ€“IA storage class reduces storage costs for files that are not accessed every day. It does this without sacrificing the high availability, high durability, elasticity, and POSIX file system access that Amazon EFS provides. AWS recommends Standard-IA storage if you need your full dataset to be readily accessible and want to automatically save on storage costs for files that are less frequently accessed.\n\nIncorrect options:\n\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class - Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere. It is not a file storage service, as is needed in the use case.\n\nAmazon Elastic File System (EFS) Standard storage class - Amazon EFS Standard storage classes are ideal for workloads that require the highest levels of durability and availability. The Amazon EFS Standard storage class is used for frequently accessed files. It is the storage class to which customer data is initially written for Standard storage classes. The company is also looking at cutting costs by optimally storing the infrequently accessed data. Hence, Amazon EFS standard storage class is not the right solution for the given use case.\n\nAmazon Elastic Block Store (EBS) - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single Amazon EC2 instance. Amazon EBS volume cannot be accessed by hundreds of Amazon EC2 instances concurrently. It is not a file storage service, as is needed in the use case.\n\nReference:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/storage-classes.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q426",
    "questionText": "A company is developing a global healthcare application that requires the least possible latency for database read/write operations from users in several geographies across the world. The company has hired you as an AWS Certified Solutions Architect Associate to build a solution using Amazon Aurora that offers an effective recovery point objective (RPO) of seconds and a recovery time objective (RTO) of a minute.\n\nWhich of the following options would you recommend?",
    "options": [
      {
        "text": "Set up an Amazon Aurora serverless Database cluster",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon Aurora provisioned Database cluster",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon Aurora Global Database cluster",
        "isCorrect": true
      },
      {
        "text": "Set up an Amazon Aurora multi-master Database cluster",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up an Amazon Aurora Global Database cluster\n\nAmazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS Regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each Region, and provides disaster recovery from Region-wide outages.\n\nIf your primary Region suffers a performance degradation or outage, you can promote one of the secondary Regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute, even in the event of a complete Regional outage. This provides your application with an effective recovery point objective (RPO) of 1 second and a recovery time objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan.\n\n\nvia - https://aws.amazon.com/rds/aurora/global-database/\n\nIncorrect options:\n\nSet up an Amazon Aurora serverless Database cluster\n\nSet up an Amazon Aurora provisioned Database cluster\n\nBoth these options work in a single AWS Region, so these options are incorrect.\n\nSet up an Amazon Aurora multi-master Database cluster - AWS does not offer the multi-master feature in a Aurora database cluster, so this option acts as a distractor.\n\nReference:\n\nhttps://aws.amazon.com/rds/aurora/global-database/",
    "awsService": "General",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q427",
    "questionText": "The engineering manager for a content management application wants to set up Amazon RDS read replicas to provide enhanced performance and read scalability. The manager wants to understand the data transfer charges while setting up Amazon RDS read replicas.\n\nWhich of the following would you identify as correct regarding the data transfer charges for Amazon RDS read replicas?",
    "options": [
      {
        "text": "There are data transfer charges for replicating data within the same Availability Zone (AZ)",
        "isCorrect": false
      },
      {
        "text": "There are data transfer charges for replicating data within the same AWS Region",
        "isCorrect": false
      },
      {
        "text": "There are data transfer charges for replicating data across AWS Regions",
        "isCorrect": true
      },
      {
        "text": "There are no data transfer charges for replicating data across AWS Regions",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThere are data transfer charges for replicating data across AWS Regions\n\nAmazon RDS Read Replicas provide enhanced performance and durability for Amazon RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\n\nA read replica is billed as a standard DB Instance and at the same rates. You are not charged for the data transfer incurred in replicating data between your source DB instance and read replica within the same AWS Region.\n\n\nvia - https://aws.amazon.com/rds/faqs/\n\nIncorrect options:\n\nThere are data transfer charges for replicating data within the same Availability Zone (AZ)\n\nThere are data transfer charges for replicating data within the same AWS Region\n\nThere are no data transfer charges for replicating data across AWS Regions\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/rds/faqs/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q428",
    "questionText": "A company has recently launched a new mobile gaming application that the users are adopting rapidly. The company uses Amazon RDS MySQL as the database. The engineering team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage.\n\nAs a solutions architect, which of the following solutions would you recommend so that it requires minimum development and systems administration effort to address this requirement?",
    "options": [
      {
        "text": "Enable storage auto-scaling for Amazon RDS MySQL",
        "isCorrect": true
      },
      {
        "text": "Migrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling",
        "isCorrect": false
      },
      {
        "text": "Migrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required",
        "isCorrect": false
      },
      {
        "text": "Create read replica for Amazon RDS MySQL",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nEnable storage auto-scaling for Amazon RDS MySQL\n\nIf your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage. Amazon RDS starts a storage modification for an autoscaling-enabled DB instance when these factors apply:\n\nFree available space is less than 10 percent of the allocated storage.\n\nThe low-storage condition lasts at least five minutes.\n\nAt least six hours have passed since the last storage modification.\n\nThe maximum storage threshold is the limit that you set for autoscaling the DB instance. You can't set the maximum storage threshold for autoscaling-enabled instances to a value greater than the maximum allocated storage.\n\nIncorrect options:\n\nMigrate RDS MySQL database to Amazon Aurora which offers storage auto-scaling - Although Aurora offers automatic storage scaling, this option is ruled out since it involves significant systems administration effort to migrate from Amazon RDS MySQL to Aurora. It is much easier to just enable storage auto-scaling for Amazon RDS MySQL.\n\nMigrate Amazon RDS MySQL database to Amazon DynamoDB which automatically allocates storage space when required - This option is ruled out since Amazon DynamoDB is a NoSQL database which implies significant development effort to change the application logic to connect and query data from the underlying database. It is much easier to just enable storage auto-scaling for Amazon RDS MySQL.\n\nCreate read replica for Amazon RDS MySQL - Read replicas make it easy to take advantage of supported engines' built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create multiple read replicas for a given source DB Instance and distribute your applicationâ€™s read traffic amongst them. This option acts as a distractor as read replicas cannot help to automatically scale storage for the primary database.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q429",
    "questionText": "An analytics company wants to improve the performance of its big data processing workflows running on Amazon Elastic File System (Amazon EFS). Which of the following performance modes should be used for Amazon EFS to address this requirement?",
    "options": [
      {
        "text": "Provisioned Throughput",
        "isCorrect": false
      },
      {
        "text": "Bursting Throughput",
        "isCorrect": false
      },
      {
        "text": "General Purpose",
        "isCorrect": false
      },
      {
        "text": "Max I/O",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nMax I/O\n\nHow Amazon EFS Works:\n\nvia - https://aws.amazon.com/efs/\n\nMax I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second. This scaling is done with a tradeoff of slightly higher latencies for file metadata operations. Highly parallelized applications and workloads, such as big data analysis, media processing, and genomic analysis, can benefit from this mode.\n\n\nvia - https://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nIncorrect options:\n\nProvisioned Throughput\n\nBursting Throughput\n\nThese two options have been added as distractors as these refer to the throughput mode of Amazon EFS and not the performance mode. There are two throughput modes to choose from for your file system, Bursting Throughput and Provisioned Throughput. With Bursting Throughput mode, throughput on Amazon EFS scales as the size of your file system in the standard storage class grows. With Provisioned Throughput mode, you can instantly provision the throughput of your file system (in MiB/s) independent of the amount of data stored.\n\nGeneral Purpose - General Purpose performance mode is ideal for latency-sensitive use cases, like web serving environments, content management systems, home directories, and general file serving. If you don't choose a performance mode when you create your file system, Amazon EFS selects the General Purpose mode for you by default.\n\nReferences:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nhttps://aws.amazon.com/efs/",
    "awsService": "EFS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q430",
    "questionText": "Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ) us-east-1a as it has the most number of instances amongst the Availability Zone (AZs) being used currently. There are 4 instances in the Availability Zone (AZ) us-east-1a like so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour.\n\nWhich of the following instances would be terminated per the default termination policy?",
    "options": [
      {
        "text": "Instance A",
        "isCorrect": false
      },
      {
        "text": "Instance B",
        "isCorrect": true
      },
      {
        "text": "Instance C",
        "isCorrect": false
      },
      {
        "text": "Instance D",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nInstance B\n\nPer the default termination policy, the first priority is given to any allocation strategy for On-Demand vs Spot instances. As no such information has been provided for the given use-case, so this criterion can be ignored. The next priority is to consider any instance with the oldest launch template unless there is an instance that uses a launch configuration. So this rules out Instance A. Next, you need to consider any instance which has the oldest launch configuration. This implies Instance B will be selected for termination and Instance C will also be ruled out as it has the newest launch configuration. Instance D, which is closest to the next billing hour, is not selected as this criterion is last in the order of priority.\n\nPlease see this note for a deep-dive on the default termination policy:\n\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\n\nIncorrect options:\n\nInstance A\n\nInstance C\n\nInstance D\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q431",
    "questionText": "A financial services company wants to identify any sensitive data stored on its Amazon S3 buckets. The company also wants to monitor and protect all data stored on Amazon S3 against any malicious activity.\n\nAs a solutions architect, which of the following solutions would you recommend to help address the given requirements?",
    "options": [
      {
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3",
        "isCorrect": true
      },
      {
        "text": "Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use Amazon Macie to identify any sensitive data stored on Amazon S3\n\nAmazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.\n\nHow Amazon GuardDuty works:\n\nvia - https://aws.amazon.com/guardduty/\n\nAmazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data on Amazon S3. Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3.\n\nHow Amazon Macie works:\n\nvia - https://aws.amazon.com/macie/\n\nIncorrect options:\n\nUse Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3\n\nUse Amazon Macie to monitor any malicious activity on data stored in Amazon S3 as well as to identify any sensitive data stored on Amazon S3\n\nUse Amazon Macie to monitor any malicious activity on data stored in Amazon S3. Use Amazon GuardDuty to identify any sensitive data stored on Amazon S3\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/guardduty/\n\nhttps://aws.amazon.com/macie/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q432",
    "questionText": "The engineering team at an e-commerce company wants to migrate from Amazon Simple Queue Service (Amazon SQS) Standard queues to FIFO (First-In-First-Out) queues with batching.\n\nAs a solutions architect, which of the following steps would you have in the migration checklist? (Select three)",
    "options": [
      {
        "text": "Delete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue",
        "isCorrect": true
      },
      {
        "text": "Convert the existing standard queue into a FIFO (First-In-First-Out) queue",
        "isCorrect": false
      },
      {
        "text": "Make sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix",
        "isCorrect": true
      },
      {
        "text": "Make sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue",
        "isCorrect": false
      },
      {
        "text": "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second",
        "isCorrect": true
      },
      {
        "text": "Make sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nDelete the existing standard queue and recreate it as a FIFO (First-In-First-Out) queue\n\nMake sure that the name of the FIFO (First-In-First-Out) queue ends with the .fifo suffix\n\nMake sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 3,000 messages per second\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.\n\nAmazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\nBy default, FIFO queues support up to 3,000 messages per second with batching, or up to 300 messages per second (300 send, receive, or delete operations per second) without batching. Therefore, using batching you can meet a throughput requirement of upto 3,000 messages per second.\n\nThe name of a FIFO queue must end with the .fifo suffix. The suffix counts towards the 80-character queue name limit. To determine whether a queue is FIFO, you can check whether the queue name ends with the suffix.\n\nIf you have an existing application that uses standard queues and you want to take advantage of the ordering or exactly-once processing features of FIFO queues, you need to configure the queue and your application correctly. You can't convert an existing standard queue into a FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.\n\nIncorrect options:\n\nConvert the existing standard queue into a FIFO (First-In-First-Out) queue\n\nMake sure that the name of the FIFO (First-In-First-Out) queue is the same as the standard queue - The name of a FIFO queue must end with the .fifo suffix.\n\nMake sure that the throughput for the target FIFO (First-In-First-Out) queue does not exceed 300 messages per second - By default, FIFO queues support up to 3,000 messages per second with batching.\n\nReferences:\n\nhttps://aws.amazon.com/sqs/faqs/\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html",
    "awsService": "SQS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q433",
    "questionText": "A global pharmaceutical company wants to move most of the on-premises data into Amazon S3, Amazon Elastic File System (Amazon EFS), and Amazon FSx for Windows File Server easily, quickly, and cost-effectively.\n\nAs a solutions architect, which of the following solutions would you recommend as the BEST fit to automate and accelerate online data transfers to these AWS storage services?",
    "options": [
      {
        "text": "Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services",
        "isCorrect": true
      },
      {
        "text": "Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services",
        "isCorrect": false
      },
      {
        "text": "Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services",
        "isCorrect": false
      },
      {
        "text": "Use File Gateway to automate and accelerate online data transfers to the given AWS storage services",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS DataSync to automate and accelerate online data transfers to the given AWS storage services\n\nAWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect.\n\nAWS DataSync fully automates and accelerates moving large active datasets to AWS, up to 10 times faster than command-line tools. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage services, as well as detailed monitoring of the transfer.\n\nAWS DataSync uses a purpose-built network protocol and scale out architecture to transfer data. A single DataSync agent is capable of saturating a 10 Gbps network link.\n\nAWS DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the DataSync API and Console, and Amazon CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. AWS DataSync performs data integrity verification both during the transfer and at the end of the transfer.\n\nHow AWS DataSync Works:\n\nvia - https://aws.amazon.com/datasync/\n\nIncorrect options:\n\nUse AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space.\n\nAWS Snowball Edge is suitable for offline data transfers, for customers who are bandwidth constrained or transferring data from remote, disconnected, or austere environments. Therefore, it cannot support automated and accelerated online data transfers.\n\nUse AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services - The AWS Transfer Family provides fully managed support for file transfers directly into and out of Amazon S3 and Amazon EFS. Therefore, it cannot support migration into the other AWS storage services mentioned in the given use-case (Amazon FSx for Windows File Server).\n\nUse File Gateway to automate and accelerate online data transfers to the given AWS storage services - AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. It can be used for on-premises applications, and for Amazon EC2-based applications that need file protocol access to S3 object storage. Therefore, it cannot support migration into the other AWS storage services mentioned in the given use-case (such as EFS and Amazon FSx for Windows File Server).\n\nReferences:\n\nhttps://aws.amazon.com/datasync/faqs/\n\nhttps://aws.amazon.com/storagegateway/file/\n\nhttps://aws.amazon.com/aws-transfer-family/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q434",
    "questionText": "A company has a license-based, expensive, legacy commercial database solution deployed at its on-premises data center. The company wants to migrate this database to a more efficient, open-source, and cost-effective option on AWS Cloud. The CTO at the company wants a solution that can handle complex database configurations such as secondary indexes, foreign keys, and stored procedures.\n\nAs a solutions architect, which of the following AWS services should be combined to handle this use-case? (Select two)",
    "options": [
      {
        "text": "AWS Snowball Edge",
        "isCorrect": false
      },
      {
        "text": "AWS Schema Conversion Tool (AWS SCT)",
        "isCorrect": true
      },
      {
        "text": "AWS Database Migration Service (AWS DMS)",
        "isCorrect": true
      },
      {
        "text": "AWS Glue",
        "isCorrect": false
      },
      {
        "text": "Basic Schema Copy",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nAWS Schema Conversion Tool (AWS SCT)\n\nAWS Database Migration Service (AWS DMS)\n\nAWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora.\n\nGiven the use-case where the CTO at the company wants to move away from license-based, expensive, legacy commercial database solutions deployed at the on-premises data center to more efficient, open-source, and cost-effective options on AWS Cloud, this is an example of heterogeneous database migrations.\n\nFor such a scenario, the source and target databases engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts.\n\nThat makes heterogeneous migrations a two-step process. First use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database. All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located on your on-premises environment outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS.\n\nHeterogeneous Database Migrations:\n\nvia - https://aws.amazon.com/dms/\n\nIncorrect options:\n\nAWS Snowball Edge - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and AWS Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space. AWS Snowball Edge cannot be used for database migrations.\n\nAWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Therefore, it cannot be used for database migrations.\n\nBasic Schema Copy - To quickly migrate a database schema to your target instance you can rely on the Basic Schema Copy feature of AWS Database Migration Service. Basic Schema Copy will automatically create tables and primary keys in the target instance if the target does not already contain tables with the same names. Basic Schema Copy is great for doing a test migration, or when you are migrating databases heterogeneously e.g. Oracle to MySQL or SQL Server to Oracle. Basic Schema Copy will not migrate secondary indexes, foreign keys or stored procedures. When you need to use a more customizable schema migration process (e.g. when you are migrating your production database and need to move your stored procedures and secondary database objects), you must use the AWS Schema Conversion Tool.\n\nReferences:\n\nhttps://aws.amazon.com/dms/\n\nhttps://aws.amazon.com/dms/faqs/\n\nhttps://aws.amazon.com/dms/schema-conversion-tool/",
    "awsService": "General",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q435",
    "questionText": "A financial services company is modernizing its analytics platform on AWS. Their legacy data processing scripts, built for both Windows and Linux environments, require shared access to a file system that supports Windows ACLs and SMB protocol for compatibility with existing Windows workloads. At the same time, their Linux-based applications need to read from and write to the same shared storage to maintain cross-platform consistency. The solutions architect needs to design a storage solution that allows both Windows and Linux EC2 instances to access the shared file system simultaneously, while preserving Windows-specific features like NTFS permissions and Active Directory (AD) integration.\n\nWhich solution will best meet these requirements?",
    "options": [
      {
        "text": "Deploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances",
        "isCorrect": false
      },
      {
        "text": "Deploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms",
        "isCorrect": false
      },
      {
        "text": "Create an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDeploy Amazon FSx for Windows File Server and mount it using the SMB protocol from both Windows and Linux EC2 instances\n\nAmazon FSx for Windows File Server is the best choice for enabling shared file access between both Windows and Linux EC2 instances while preserving Windows-native features like SMB protocol support, NTFS file system permissions, and Active Directory (AD) integration. Windows instances can seamlessly mount the file system using SMB, and Linux instances can also connect using SMB clients (e.g., via cifs-utils). This allows Linux applications to access and interact with the same file system used by Windows applications, maintaining consistent permissions and supporting cross-platform collaboration. FSx for Windows is purpose-built for such hybrid environments, making it a fully managed and feature-rich solution that aligns with enterprise-grade security and interoperability requirements.\n\nIncorrect options:\n\nUse Amazon EFS with the Standard storage class and mount the file system using NFS from both Windows and Linux instances - Amazon EFS is a POSIX-compliant file system that uses the NFS protocol, which is not natively supported by Windows. While EFS works well for Linux-based workloads, it cannot be accessed by Windows EC2 instances without complex workarounds or third-party tools. Additionally, EFS does not support Windows-specific features like NTFS permissions, SMB, or Active Directory integration, making it unsuitable for hybrid Windows-Linux environments that need shared access and Windows ACLs.\n\nDeploy Amazon FSx for Lustre and mount the file system using a POSIX-compliant client from both platforms - Amazon FSx for Lustre is optimized for high-performance computing (HPC) and throughput-heavy workloads, particularly for integration with S3. It supports POSIX-compliant access via Lustre clients, which are compatible with Linux, but not natively supported on Windows. It lacks Windows ACLs, SMB support, and Active Directory integration, making it unsuitable for mixed environments where Windows compatibility is essential.\n\nCreate an S3 bucket and mount it on EC2 instances using Mountpoint for Amazon S3, managing access through IAM policies to support both Windows and Linux workloads - Although Mountpoint for Amazon S3 is an AWS-supported open-source tool that allows Linux EC2 instances to access S3 buckets via a file system interface, it is not a shared file system and does not support Windows clients. It lacks features such as file locking, concurrent file access control, and NTFS permissions, which are essential for shared access across platforms. Additionally, S3 is object storage, not a POSIX-compliant file system or SMB-compatible storage layer, making it unsuitable for use cases that involve cross-platform file sharing and Windows ACLs.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/storage/access-file-shares-on-amazon-fsx-for-windows-file-server-from-a-linux-environment/\n\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\n\nhttps://aws.amazon.com/s3/features/mountpoint/\n\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q436",
    "questionText": "A software company manages a fleet of Amazon EC2 instances that support internal analytics applications. These instances use an IAM role with custom policies to connect to Amazon RDS and AWS Secrets Manager for secure access to credentials and database endpoints. The IT operations team wants to implement a centralized patch management solution that simplifies compliance and security tasks. Their goal is to automate OS patching across EC2 instances without disrupting the running applications.\n\nWhich approach will allow the company to meet these goals with the least administrative overhead?",
    "options": [
      {
        "text": "Enable Default Host Management Configuration in AWS Systems Manager Quick Setup",
        "isCorrect": true
      },
      {
        "text": "Create a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation",
        "isCorrect": false
      },
      {
        "text": "Manually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts",
        "isCorrect": false
      },
      {
        "text": "Detach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nEnable Default Host Management Configuration in AWS Systems Manager Quick Setup\n\nThis is the simplest and most reliable solution that aligns with the company's requirement for zero disruption and minimal administrative effort. AWS Systems Managerâ€™s Default Host Management Configuration (part of Quick Setup) automatically applies the necessary Systems Manager permissions, activates inventory collection, and enables patching without needing to alter existing IAM roles manually. It simplifies onboarding by using AWS best practices and auto-configures EC2 instances with the required SSM settings behind the scenes.\n\n\nvia - https://aws.amazon.com/blogs/mt/enable-management-of-your-amazon-ec2-instances-in-aws-systems-manager-using-default-host-management-configuration/\n\nIncorrect options:\n\nCreate a second IAM role with the AmazonSSMManagedInstanceCore policy and attach both the new and the existing IAM roles to each EC2 instance using Systems Manager Hybrid Activation - Hybrid Activations are designed for non-EC2 resources or on-premises servers, not for managing standard EC2 instances within an AWS account. EC2 instances can only be attached to one IAM role at a time, making the dual-role approach unfeasible. This method adds unnecessary complexity and does not align with standard AWS EC2 management practices.\n\nManually install the Systems Manager Agent (SSM Agent) on each EC2 instance. Schedule daily patch jobs using cron scripts - Manually scripting patching operations using cron jobs introduces high operational overhead and does not leverage native AWS integrations. This method lacks scalability, centralized compliance reporting, and the benefit of automated remediation provided by Systems Manager.\n\nDetach the existing IAM role from all EC2 instances. Replace it with a new role that has both the original permissions and the AmazonSSMManagedInstanceCore policy to enable Systems Manager features - Detaching and replacing the existing IAM role risks disrupting application functionality that depends on the roleâ€™s current policies (e.g., RDS access). This introduces downtime and manual changes to launch templates or running instances, violating the requirement to avoid disruption.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/mt/enable-management-of-your-amazon-ec2-instances-in-aws-systems-manager-using-default-host-management-configuration/\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/quick-setup-host-management.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q437",
    "questionText": "The engineering team at a company wants to use Amazon Simple Queue Service (Amazon SQS) to decouple components of the underlying application architecture. However, the team is concerned about the VPC-bound components accessing Amazon Simple Queue Service (Amazon SQS) over the public internet.\n\nAs a solutions architect, which of the following solutions would you recommend to address this use-case?",
    "options": [
      {
        "text": "Use VPC endpoint to access Amazon SQS",
        "isCorrect": true
      },
      {
        "text": "Use Internet Gateway to access Amazon SQS",
        "isCorrect": false
      },
      {
        "text": "Use Network Address Translation (NAT) instance to access Amazon SQS",
        "isCorrect": false
      },
      {
        "text": "Use VPN connection to access Amazon SQS",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse VPC endpoint to access Amazon SQS\n\nAWS customers can access Amazon Simple Queue Service (Amazon SQS) from their Amazon Virtual Private Cloud (Amazon VPC) using VPC endpoints, without using public IPs, and without needing to traverse the public internet. VPC endpoints for Amazon SQS are powered by AWS PrivateLink, a highly available, scalable technology that enables you to privately connect your VPC to supported AWS services.\n\nAmazon VPC endpoints are easy to configure. They also provide reliable connectivity to Amazon SQS without requiring an internet gateway, Network Address Translation (NAT) instance, VPN connection, or AWS Direct Connect connection. With VPC endpoints, the data between your Amazon VPC and Amazon SQS queue is transferred within the Amazon network, helping protect your instances from internet traffic.\n\nAWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify the network architecture.\n\nIncorrect options:\n\nUse Internet Gateway to access Amazon SQS - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. This option is ruled out as the team does not want to use the public internet to access Amazon SQS.\n\nUse VPN connection to access Amazon SQS - AWS Site-to-Site VPN (aka VPN Connection) enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. As the existing infrastructure is within AWS Cloud, therefore a VPN connection is not required.\n\nUse Network Address Translation (NAT) instance to access Amazon SQS - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. Amazon provides Amazon Linux AMIs that are configured to run as NAT instances. These AMIs include the string amzn-ami-vpc-nat in their names, so you can search for them in the Amazon EC2 console. This option is ruled out because NAT instances are used to provide internet access to any instances in a private subnet.\n\nReferences:\n\nhttps://aws.amazon.com/privatelink/\n\nhttps://aws.amazon.com/about-aws/whats-new/2018/12/amazon-sqs-vpc-endpoints-aws-privatelink/",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q438",
    "questionText": "A media company wants a low-latency way to distribute live sports results which are delivered via a proprietary application using UDP protocol.\n\nAs a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for this use case?",
    "options": [
      {
        "text": "Use Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudFront to provide a low latency way to distribute live sports results",
        "isCorrect": false
      },
      {
        "text": "Use AWS Global Accelerator to provide a low latency way to distribute live sports results",
        "isCorrect": true
      },
      {
        "text": "Use Auto Scaling group to provide a low latency way to distribute live sports results",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Global Accelerator to provide a low latency way to distribute live sports results\n\nAWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. AWS Global Accelerator is easy to set up, configure, and manage. It provides static IP addresses that provide a fixed entry point to your applications and eliminate the complexity of managing specific IP addresses for different AWS Regions and Availability Zones (AZs). AWS Global Accelerator always routes user traffic to the optimal endpoint based on performance, reacting instantly to changes in application health, your userâ€™s location, and policies that you configure. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP. Therefore, this option is correct.\n\nHow AWS Global Accelerator Works:\n\nvia - https://aws.amazon.com/global-accelerator/\n\nIncorrect options:\n\nUse Amazon CloudFront to provide a low latency way to distribute live sports results - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\n\nAmazon CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. Amazon CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity. CloudFront supports HTTP/RTMP protocol based requests, therefore this option is incorrect.\n\nUse Elastic Load Balancing (ELB) to provide a low latency way to distribute live sports results - Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancer cannot help with decreasing latency of incoming traffic from the source.\n\nUse Auto Scaling group to provide a low latency way to distribute live sports results - Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of Amazon EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. Auto Scaling group cannot help with decreasing latency of incoming traffic from the source.\n\nExam Alert:\n\nPlease note the differences between the capabilities of AWS Global Accelerator and Amazon CloudFront -\n\nAWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. Amazon CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). AWS Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions.\n\nAWS Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.\n\nReferences:\n\nhttps://aws.amazon.com/global-accelerator/\n\nhttps://aws.amazon.com/cloudfront/faqs/",
    "awsService": "ELB",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q439",
    "questionText": "The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an Amazon EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a Network Address Translation (NAT) instance (N1) in the subnet S1.\n\nUnder the given infrastructure setup, which of the following entities is doing the Network Address Translation for the Amazon EC2 instance E1?",
    "options": [
      {
        "text": "Network Address Translation (NAT) instance (N1)",
        "isCorrect": false
      },
      {
        "text": "Internet Gateway (I1)",
        "isCorrect": true
      },
      {
        "text": "Subnet (S1)",
        "isCorrect": false
      },
      {
        "text": "Route Table (R1)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nInternet Gateway (I1)\n\nAn Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet.\n\nAn Internet Gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Therefore, for instance E1, the  Network Address Translation is done by Internet Gateway I1.\n\nAdditionally, an Internet Gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic.\n\nTo enable access to or from the internet for instances in a subnet in a VPC, you must do the following:\n\nAttach an Internet gateway to your VPC.\n\nAdd a route to your subnet's route table that directs internet-bound traffic to the internet gateway. If a subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet. If a subnet is associated with a route table that does not have a route to an internet gateway, it's known as a private subnet.\n\nEnsure that instances in your subnet have a globally unique IP address (public IPv4 address, Elastic IP address, or IPv6 address).\n\nEnsure that your network access control lists and security group rules allow the relevant traffic to flow to and from your instance.\n\nInternet Gateway Overview:\n\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\n\nIncorrect options:\n\nNetwork Address Translation (NAT) instance (N1) - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. As the instance E1 is in a public subnet, therefore this option is not correct.\n\nSubnet (S1)\n\nRoute Table (R1)\n\nA virtual private cloud (VPC) is a virtual network dedicated to your AWS account. A subnet is a range of IP addresses in your VPC. A route table contains a set of rules, called routes, that are used to determine where network traffic is directed. Therefore neither Subnet nor Route Table can be used for Network Address Translation.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q440",
    "questionText": "A company has hired you as an AWS Certified Solutions Architect â€“ Associate to help with redesigning a real-time data processor. The company wants to build custom applications that process and analyze the streaming data for its specialized needs.\n\nWhich solution will you recommend to address this use-case?",
    "options": [
      {
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "isCorrect": true
      },
      {
        "text": "Use Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon Kinesis Data Streams to process the data streams as well as decouple the producers and consumers for the real-time data processor\n\nAmazon Kinesis Data Streams is useful for rapidly moving data off data producers and then continuously processing the data, be it to transform the data before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing. Kinesis data streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKinesis Data Streams Overview:\n\nvia - https://aws.amazon.com/kinesis/data-streams/\n\nIncorrect options:\n\nUse Amazon Simple Notification Service (Amazon SNS) to process the data streams as well as decouple the producers and consumers for the real-time data processor - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. SNS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use-case.\n\nUse Amazon Simple Queue Service (Amazon SQS) to process the data streams as well as decouple the producers and consumers for the real-time data processor - Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. SQS cannot be used to decouple the producers and consumers for the real-time data processor as described in the given use-case.\n\nUse Amazon Kinesis Data Firehose to process the data streams as well as decouple the producers and consumers for the real-time data processor - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. Kinesis Firehose cannot be used to process and analyze the streaming data in custom applications. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics.\n\nAmazon Kinesis Data Firehose Overview\n\nvia - https://aws.amazon.com/kinesis/data-firehose/\n\nReferences:\n\nhttps://aws.amazon.com/kinesis/data-streams/\n\nhttps://aws.amazon.com/kinesis/data-firehose/",
    "awsService": "SNS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q441",
    "questionText": "An engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 terabyte.\n\nAs a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?",
    "options": [
      {
        "text": "Use Amazon EFS with Provisioned Throughput mode",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EFS with Bursting Throughput mode",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB table that is accessible by all ECS cluster instances",
        "isCorrect": false
      },
      {
        "text": "Use an Amazon EBS volume mounted to the Amazon ECS cluster instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale. It also enables massively parallel access from compute instances, including Amazon EC2, Amazon ECS, and AWS Lambda, to your data.\n\nUse Amazon EFS with Provisioned Throughput mode\n\nProvisioned Throughput mode is available for applications with high throughput to storage (MiB/s per TiB) ratios, or with requirements greater than those allowed by the Bursting Throughput mode. For example, say you're using Amazon EFS for development tools, web serving, or content management applications where the amount of data in your file system is low relative to throughput demands. Your file system can now get the high levels of throughput your applications require without having to pad your file system.\n\nIf your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease. Additionally, you can change between Provisioned Throughput mode and the default Bursting Throughput mode as long as itâ€™s been more than 24 hours since the last throughput mode change.\n\n\nvia - https://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nIncorrect options:\n\nUse Amazon EFS with Bursting Throughput mode - With Bursting Throughput mode, a file system's throughput scales as the amount of data stored in the standard storage class grows. File-based workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput levels for periods of time. By default, AWS recommends that you run your application in the Bursting Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider switching to Provisioned Throughput mode.\n\nThe use-case mentions that the solution should be optimized for high-frequency reading and writing even when the old outputs are archived, therefore Provisioned Throughput mode is a better fit as it guarantees high levels of throughput your applications require without having to pad your file system.\n\nUse an Amazon EBS volume mounted to the Amazon ECS cluster instances - Amazon EFS has a higher throughput than Amazon EBS. In addition, Amazon EBS can be attached to multiple Amazon EC2 instances when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use-case does not provide any such details, so this option is ruled out.\n\nUse Amazon DynamoDB table that is accessible by all ECS cluster instances - Amazon DynamoDB is not a fit for this scenario as each task output is 20 MB but the storage limit for each item in a Amazon DynamoDB table is 400 KB. You could write custom code to split the task output data into multiple items but it is not an optimal solution compared to using Amazon EFS in Provisioned Throughput mode.\n\nReferences:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q442",
    "questionText": "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a Network Address Translation (NAT) instance or a Network Address Translation (NAT) gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the Network Address Translation (NAT) instance and the Network Address Translation (NAT) gateway.\n\nAs a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
    "options": [
      {
        "text": "NAT gateway supports port forwarding",
        "isCorrect": false
      },
      {
        "text": "Security Groups can be associated with a NAT gateway",
        "isCorrect": false
      },
      {
        "text": "NAT gateway can be used as a bastion server",
        "isCorrect": false
      },
      {
        "text": "NAT instance can be used as a bastion server",
        "isCorrect": true
      },
      {
        "text": "Security Groups can be associated with a NAT instance",
        "isCorrect": true
      },
      {
        "text": "NAT instance supports port forwarding",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nNAT instance can be used as a bastion server\n\nSecurity Groups can be associated with a NAT instance\n\nNAT instance supports port forwarding\n\nA NAT instance or a NAT Gateway can be used in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet.\n\nHow NAT Gateway works:\n\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nHow NAT Instance works:\n\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\n\nPlease see this high-level summary of the differences between NAT instances and NAT gateways relevant to the options described in the question:\n\n\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\n\nIncorrect options:\n\nNAT gateway supports port forwarding\n\nSecurity Groups can be associated with a NAT gateway\n\nNAT gateway can be used as a bastion server\n\nThese three options contradict the details provided in the explanation above, so these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q443",
    "questionText": "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB.\n\nAs a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
    "options": [
      {
        "text": "Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance",
        "isCorrect": true
      },
      {
        "text": "Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance",
        "isCorrect": false
      },
      {
        "text": "Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance",
        "isCorrect": false
      },
      {
        "text": "Traffic is routed to instances using the instance ID specified in the primary network interface for the instance",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nTraffic is routed to instances using the primary private IP address specified in the primary network interface for the instance\n\nA Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.\n\nRequest Routing and IP Addresses -\n\nIf you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. The load balancer rewrites the destination IP address from the data packet before forwarding it to the target instance.\n\nIf you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port. Note that each network interface can have its security group. The load balancer rewrites the destination IP address before forwarding it to the target.\n\nIncorrect options:\n\nTraffic is routed to instances using the primary public IP address specified in the primary network interface for the instance - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. So public IP address cannot be used to route the traffic to the instance.\n\nTraffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. So elastic IP address cannot be used to route the traffic to the instance.\n\nTraffic is routed to instances using the instance ID specified in the primary network interface for the instance - You cannot use instance ID to route traffic to the instance. This option is just added as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html",
    "awsService": "Network Load Balancer",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q444",
    "questionText": "An e-commerce company is using Elastic Load Balancing (ELB) for its fleet of Amazon EC2 instances spread across two Availability Zones (AZs), with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled.\n\nAs a solutions architect, which of the following traffic distribution outcomes would you identify as correct?",
    "options": [
      {
        "text": "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each",
        "isCorrect": true
      },
      {
        "text": "With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each",
        "isCorrect": false
      },
      {
        "text": "With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each",
        "isCorrect": false
      },
      {
        "text": "With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nWith cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each\n\nThe nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. Therefore, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. Therefore, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each.\n\nConsider the following diagrams (the scenario illustrated in the diagrams involves 10 target instances split across 2 AZs) to understand the effect of cross-zone load balancing.\n\nIf cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets.\n\n\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\n\nIf cross-zone load balancing is disabled:\n\nEach of the two targets in Availability Zone A receives 25% of the traffic.\n\nEach of the eight targets in Availability Zone B receives 6.25% of the traffic.\n\nThis is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone\n\n\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\n\nIncorrect options:\n\nWith cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each\n\nWith cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each\n\nWith cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each\n\nThese three options contradict the details provided in the explanation above, so these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q445",
    "questionText": "A healthcare company has deployed its web application on Amazon Elastic Container Service (Amazon ECS) container instances running behind an Application Load Balancer. The website slows down when the traffic spikes and the website availability is also reduced. The development team has configured Amazon CloudWatch alarms to receive notifications whenever there is an availability constraint so the team can scale out resources. The company wants an automated solution to respond to such events.\n\nWhich of the following addresses the given use case?",
    "options": [
      {
        "text": "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold",
        "isCorrect": true
      },
      {
        "text": "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold",
        "isCorrect": false
      },
      {
        "text": "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold",
        "isCorrect": false
      },
      {
        "text": "Configure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure AWS Auto Scaling to scale out the Amazon ECS cluster when the ECS service's CPU utilization rises above a threshold\n\nYou use the Amazon ECS first-run wizard to create a cluster and a service that runs behind an Elastic Load Balancing load balancer. Then you can configure a target tracking scaling policy that scales your service automatically based on the current application load as measured by the service's CPU utilization (from the ECS, ClusterName, and ServiceName category in CloudWatch).\n\nWhen the average CPU utilization of your service rises above 75% (meaning that more than 75% of the CPU that is reserved for the service is being used), a scale out alarm triggers Service Auto Scaling to add another task to your service to help out with the increased load. Conversely, when the average CPU utilization of your service drops below the target utilization for a sustained period, a scale-in alarm triggers a decrease in the service's desired count to free up those cluster resources for other tasks and services.\n\n\nvia - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-configure-auto-scaling.html\n\nIncorrect options:\n\nConfigure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's target group's CPU utilization rises above a threshold\n\nConfigure AWS Auto Scaling to scale out the Amazon ECS cluster when the Application Load Balancer's CPU utilization rises above a threshold\n\nConfigure AWS Auto Scaling to scale out the Amazon ECS cluster when the CloudWatch alarm's CPU utilization rises above a threshold\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-autoscaling-targettracking.html\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-configure-auto-scaling.html",
    "awsService": "Auto Scaling",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q446",
    "questionText": "A national logistics company has a dedicated AWS Direct Connect connection from its corporate data center to AWS. Within its AWS account, the company operates 25 Amazon VPCs in the same Region, each supporting different regional distribution services. The VPCs were configured with non-overlapping CIDR blocks and currently use private VIFs for Direct Connect access to on-premises resources. As the architecture scales, the company wants to enable communication across all VPCs and the on-premises environment. The solution must scale efficiently, support full-mesh connectivity, and reduce the complexity of maintaining separate private VIFs for each VPC.\n\nWhich combination of solutions will best fulfill these requirements with the least amount of operational overhead? (Select two)",
    "options": [
      {
        "text": "Create an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing",
        "isCorrect": true
      },
      {
        "text": "Create a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway",
        "isCorrect": true
      },
      {
        "text": "Create individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing",
        "isCorrect": false
      },
      {
        "text": "Reconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services",
        "isCorrect": false
      },
      {
        "text": "Convert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nCreate an AWS Transit Gateway and attach all 25 VPCs to it. Enable route propagation for each attachment to automatically manage inter-VPC routing\n\nAttaching all VPCs to a single AWS Transit Gateway centralizes routing and allows for transitive communication between the VPCs. Enabling route propagation eliminates the need to manually update route tables for every VPC. This design supports scalable, full-mesh VPC connectivity with low operational burden.\n\n\nvia - https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-vgw-multi-regions-and-aws-public-peering.html\n\nCreate a transit virtual interface (VIF) from the Direct Connect connection and associate it with the transit gateway\n\nUsing a transit VIF from the Direct Connect connection to the transit gateway enables on-premises-to-VPC connectivity through a single, centralized path. This eliminates the need to maintain multiple private VIFs and simplifies BGP management and route aggregation. Combined with Option A, this forms a highly efficient hub-and-spoke architecture.\n\nIncorrect options:\n\nCreate individual Site-to-Site VPN connections from the data center to each VPC. Set up BGP route propagation for every tunnel to facilitate on-premises-to-VPC routing - Creating 25 individual VPN connections adds significant operational complexity, introduces higher latency, and does not make use of the existing high-throughput Direct Connect connection. While VPNs can provide connectivity, they are inferior in performance and scalability for this use case.\n\nReconfigure each VPC to connect through AWS PrivateLink endpoints to a central networking service VPC. Share the service with other VPCs using VPC endpoint services - AWS PrivateLink is used to expose services privately, not to enable full VPC-to-VPC or on-premises routing. It supports one-way access to a specific service, and cannot be used to facilitate general network-level communication between all VPCs and the data center.\n\nConvert each existing private VIF into a new Direct Connect gateway association by attaching a virtual private gateway (VGW) to each VPC. Manually configure routing between VGWs - While a Direct Connect gateway allows centralized on-premises access, it does not support transitive routing between VPCs. You would also need to deploy and manage 25 separate virtual private gateways (VGWs), which increases complexity and operational burden. Inter-VPC communication would still require peering or another layer of routing.\n\nReferences:\n\nhttps://aws.amazon.com/transit-gateway/\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html\n\nhttps://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-vgw-multi-regions-and-aws-public-peering.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q447",
    "questionText": "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up AWS Organizations to manage several departments running their AWS accounts and using resources such as Amazon EC2 instances and Amazon RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity.\n\nAs a solutions architect, which of the following options would you choose to facilitate this use-case?",
    "options": [
      {
        "text": "Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "isCorrect": true
      },
      {
        "text": "Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "isCorrect": false
      },
      {
        "text": "Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "isCorrect": false
      },
      {
        "text": "Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\n\nVPC sharing (part of Resource Access Manager) allows multiple AWS accounts to create their application resources such as Amazon EC2 instances, Amazon RDS databases, Amazon Redshift clusters, and AWS Lambda functions, into shared and centrally-managed Amazon Virtual Private Clouds (VPCs). To set this up, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner.\n\nYou can share Amazon VPCs to leverage the implicit routing within a VPC for applications that require a high degree of interconnectivity and are within the same trust boundaries. This reduces the number of VPCs that you create and manage while using separate accounts for billing and access control.\n\nIncorrect options:\n\nUse VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations - Using VPC sharing, an account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. The owner account cannot share the VPC itself. Therefore this option is incorrect.\n\nUse VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. VPC peering does not facilitate centrally managed VPCs. Therefore this option is incorrect.\n\nUse VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. VPC peering does not facilitate centrally managed VPCs. Moreover, an AWS owner account cannot share the VPC itself with another AWS account. Therefore this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q448",
    "questionText": "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet.\n\nWhich of the following represents the correct configuration for the IPSec VPN Connection?",
    "options": [
      {
        "text": "Create a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN",
        "isCorrect": false
      },
      {
        "text": "Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN",
        "isCorrect": false
      },
      {
        "text": "Create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN",
        "isCorrect": true
      },
      {
        "text": "Create a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN\n\nAmazon VPC provides the facility to create an IPsec VPN connection (also known as AWS site-to-site VPN) between remote customer networks and their Amazon VPC over the internet. The following are the key concepts for a site-to-site VPN:\n\nVirtual private gateway: A virtual private gateway (VGW), also known as a VPN Gateway is the endpoint on the AWS VPC side of your VPN connection.\n\nVPN connection: A secure connection between your on-premises equipment and your VPCs.\n\nVPN tunnel: An encrypted link where data can pass from the customer network to or from AWS.\n\nCustomer Gateway: An AWS resource that provides information to AWS about your Customer Gateway device.\n\nCustomer Gateway device: A physical device or software application on the customer side of the Site-to-Site VPN connection.\n\nAWS Managed IPSec VPN\n\nvia - https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html\n\nIncorrect options:\n\nCreate a virtual private gateway (VGW) on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN - You need to create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong.\n\nCreate a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN - You need to create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong.\n\nCreate a virtual private gateway (VGW) on both the AWS side of the VPN as well as the on-premises side of the VPN - You need to create a virtual private gateway (VGW) on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong.\n\nReferences:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q449",
    "questionText": "A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Amazon Route 53. The web development team would like to create Amazon Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com.\n\nAs a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?",
    "options": [
      {
        "text": "Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com",
        "isCorrect": false
      },
      {
        "text": "Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com",
        "isCorrect": true
      },
      {
        "text": "Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com",
        "isCorrect": false
      },
      {
        "text": "Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an alias record for covid19survey.com that routes traffic to www.covid19survey.com\n\nAlias records provide Amazon Route 53â€“specific extension to DNS functionality. Alias records let you route traffic to selected AWS resources, such as Amazon CloudFront distributions and Amazon S3 buckets.\n\nYou can create an alias record at the top node of a DNS namespace, also known as the zone apex, however, you cannot create a CNAME record for the top node of the DNS namespace. So, if you register the DNS name covid19survey.com, the zone apex is covid19survey.com. You can't create a CNAME record for covid19survey.com, but you can create an alias record for covid19survey.com that routes traffic to www.covid19survey.com.\n\nExam Alert:\n\nYou should also note that Amazon Route 53 doesn't charge for alias queries to AWS resources but Route 53 does charge for CNAME queries. Additionally, an alias record can only redirect queries to selected AWS resources such as Amazon S3 buckets, Amazon CloudFront distributions, and another record in the same Amazon Route 53 hosted zone; however a CNAME record can redirect DNS queries to any DNS record. So, you can create a CNAME record that redirects queries from app.covid19survey.com to app.covid19survey.net.\n\nIncorrect options:\n\nCreate a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com - You cannot create a CNAME record for the top node of the DNS namespace, so this option is incorrect.\n\nCreate an MX record for covid19survey.com that routes traffic to www.covid19survey.com - An MX record specifies the names of your mail servers and, if you have two or more mail servers, the priority order. It cannot be used to create Amazon Route 53 record to route traffic for the top node of the DNS namespace, so this option is incorrect.\n\nCreate an NS record for covid19survey.com that routes traffic to www.covid19survey.com - An NS record identifies the name servers for the hosted zone. It cannot be used to create Amazon Route 53 record to route traffic for the top node of the DNS namespace, so this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html",
    "awsService": "Route 53",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q450",
    "questionText": "A healthcare startup is modernizing its monolithic Python-based analytics application by transitioning to a microservices architecture on AWS. As a pilot, the team wants to refactor one module into a standalone microservice that can handle hundreds of requests per second. They are seeking an AWS-native solution that supports Python, scales automatically with traffic, and requires minimal infrastructure management and operational overhead to build, test, and deploy the service efficiently.\n\nWhich AWS solution best meets these requirements?",
    "options": [
      {
        "text": "Deploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics",
        "isCorrect": false
      },
      {
        "text": "Use AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup",
        "isCorrect": false
      },
      {
        "text": "Use AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Lambda to run the Python-based microservice. Integrate it with Amazon API Gateway for HTTP access and enable provisioned concurrency for performance during peak loads\n\nAWS Lambda is a fully managed serverless compute service that natively supports Python runtimes. It is ideal for event-driven and API-based microservices. Lambda automatically scales based on request volume, requires no server provisioning or patching, and integrates easily with API Gateway for RESTful access. The company can also enable provisioned concurrency to reduce cold start latency for high-throughput workloads. This approach aligns perfectly with the requirement for minimal infrastructure and high scalability.\n\n\nvia - https://aws.amazon.com/blogs/compute/creating-low-latency-high-volume-apis-with-provisioned-concurrency/\n\n\nvia - https://aws.amazon.com/blogs/compute/creating-low-latency-high-volume-apis-with-provisioned-concurrency/\n\nIncorrect options:\n\nDeploy the microservice in an AWS Fargate task using Amazon ECS. Package the code in a Docker container image with a Python runtime and configure ECS Service Auto Scaling to respond to CPU utilization metrics - AWS Fargate is a serverless container service that works with ECS and supports auto scaling, but it requires containerization of the application, Dockerfile management, and more setup complexity than Lambda. Although suitable for microservices, it involves more operational overhead, especially during initial development and testing phases.\n\nUse AWS App Runner to build and deploy the Python application directly from a GitHub repository. Allow App Runner to manage traffic scaling and deployments - AWS App Runner provides simple deployment from source code or container images and supports Python apps via containerized runtimes. However, App Runner is more suitable for long-running web applications rather than event-driven functions. It also incurs costs during idle time and requires setting up build and deployment configurations, which may not be ideal for rapid experimentation.\n\nUse Amazon EC2 Spot Instances in an Auto Scaling group. Launch the Python application as a background service and install all required dependencies at instance startup - While Spot Instances are cost-effective, they are not reliable for production workloads due to the risk of interruption. Additionally, managing EC2 Auto Scaling groups and instance health checks introduces high operational overhead, which contradicts the goal of minimal infrastructure and low maintenance.\n\nReferences:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\n\nhttps://aws.amazon.com/blogs/compute/creating-low-latency-high-volume-apis-with-provisioned-concurrency/\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\n\nhttps://docs.aws.amazon.com/apprunner/latest/dg/what-is-apprunner.html\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q451",
    "questionText": "A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group. The Auto Scaling group uses a Launch Template (LT1) with \"dedicated\" instance tenancy but the VPC (V1) used by the Launch Template LT1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Template (LT2) with shared (default) instance tenancy but the VPC (V2) used by the Launch Template LT2 has the instance tenancy set to dedicated.\n\nWhich of the following is correct regarding the instances launched via Launch Template LT1 and Launch Template LT2?",
    "options": [
      {
        "text": "The instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy",
        "isCorrect": false
      },
      {
        "text": "The instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy",
        "isCorrect": false
      },
      {
        "text": "The instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy",
        "isCorrect": false
      },
      {
        "text": "The instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nThe instances launched by both Launch Template LT1 and Launch Template LT2 will have dedicated instance tenancy\n\nA launch template specifies instance configuration information. It includes the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and other parameters used to launch EC2 instances. If you've launched an EC2 instance before, you specified the same information to launch the instance.\n\nWhen you create a Launch Template, the default value for the instance tenancy is shared and the instance tenancy is controlled by the tenancy attribute of the VPC.  If you set the Launch Template Tenancy to shared (default) and the VPC Tenancy is set to dedicated, then the instances have dedicated tenancy. If you set the Launch Template Tenancy to dedicated and the VPC Tenancy is set to default, then again the instances have dedicated tenancy.\n\nAmazon EC2 provides three options for the tenancy of your EC2 instances:\n\nShared (Shared) â€“ Multiple AWS accounts may share the same physical hardware. This is the default tenancy option when launching an instance.\n\nDedicated instances (Dedicated) â€“ Your instance runs on single-tenant hardware. No other AWS customer shares the same physical server.\n\nDedicated Hosts (Dedicated host) â€“ The instance runs on a physical server that is dedicated to your use. Using Dedicated Hosts makes it easier to bring your own licenses (BYOL) that have dedicated hardware requirements to EC2 and meet compliance use cases. If you choose this option, you must provide a host resource group for Tenancy host resource group.\n\nIncorrect options:\n\nThe instances launched by Launch Template LT1 will have dedicated instance tenancy while the instances launched by the Launch Template LT2 will have shared (default) instance tenancy - If either Launch Template Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect.\n\nThe instances launched by Launch Template LT1 will have default instance tenancy while the instances launched by the Launch Template LT2 will have dedicated instance tenancy - If either Launch Template Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect.\n\nThe instances launched by both Launch Template LT1 and Launch Template LT2 will have default instance tenancy - If either Launch Template Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/advanced-settings-for-your-launch-template.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q452",
    "questionText": "An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in the us-west-2 region. The IT consultant is trying to launch an Amazon EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone (AZ) of the us-west-2 region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones (AZs) are still different.\n\nAs a solutions architect, which of the following would you suggest resolving this issue?",
    "options": [
      {
        "text": "Reach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts",
        "isCorrect": false
      },
      {
        "text": "Use Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts",
        "isCorrect": true
      },
      {
        "text": "Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts",
        "isCorrect": false
      },
      {
        "text": "Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Availability Zone (AZ) ID to uniquely identify the Availability Zones across the two AWS Accounts\n\nAn Availability Zone is represented by a region code followed by a letter identifier; for example, us-east-1a. To ensure that resources are distributed across the Availability Zones for a region, AWS maps Availability Zones to names for each AWS account. For example, the Availability Zone us-west-2a for one AWS account might not be the same location as us-west-2a for another AWS account.\n\nTo coordinate Availability Zones across accounts, you must use the AZ ID, which is a unique and consistent identifier for an Availability Zone. For example, usw2-az2 is an AZ ID for the us-west-2 region and it has the same location in every AWS account.\n\nViewing AZ IDs enables you to determine the location of resources in one account relative to the resources in another account. For example, if you share a subnet in the Availability Zone with the AZ ID usw2-az2 with another account, this subnet is available to that account in the Availability Zone whose AZ ID is also usw2-az2.\n\nYou can view the AZ IDs by going to the service health section of the Amazon EC2 Dashboard via your AWS Management Console.\n\nAvailability Zone (AZ) IDs for Availability Zones:\n\n\nIncorrect options:\n\nUse the default VPC to uniquely identify the Availability Zones across the two AWS Accounts - A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. Since a VPC spans an AWS region, it cannot be used to uniquely identify an Availability Zone. Therefore, this option is incorrect.\n\nUse the default subnet to uniquely identify the Availability Zones across the two AWS Accounts - A subnet is a range of IP addresses in your VPC. A subnet spans an Availability Zone of an AWS region. The default subnet representing the Availability Zone us-west-2a for one AWS account might not be the same location as us-west-2a for another AWS account. Therefore, this option is incorrect.\n\nReach out to AWS Support for creating the Amazon EC2 instances in the same Availability Zone (AZ) across the two AWS accounts - Since the AZ ID is a unique and consistent identifier for an Availability Zone, there is no need to contact AWS Support. Therefore, this option is incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q453",
    "questionText": "An application running on an Amazon EC2 instance needs to access a Amazon DynamoDB table in the same AWS account.\n\nWhich of the following solutions should a solutions architect configure for the necessary permissions?",
    "options": [
      {
        "text": "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly",
        "isCorrect": false
      },
      {
        "text": "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly",
        "isCorrect": false
      },
      {
        "text": "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role",
        "isCorrect": false
      },
      {
        "text": "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nSet up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance\n\nA service role is an IAM role that a service assumes to perform actions on your behalf. Service roles provide access only within your account and cannot be used to grant access to services in other accounts. An IAM administrator can create, modify, and delete a service role from within IAM. When you create the service role, you define the trusted entity in the definition.\n\nIf you are going to use the role with Amazon EC2 or another AWS service that uses Amazon EC2, you must store the role in an instance profile. An instance profile is a container for a role that can be attached to an Amazon EC2 instance when launched. An instance profile can contain only one role, and that limit cannot be increased. If you create the role using the AWS Management Console, the instance profile is created for you with the same name as the role.\n\nIncorrect options:\n\nSet up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in an Amazon S3 bucket and read them from within the application code directly\n\nSet up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly\n\nYou should never store the IAM access credentials for a user in Amazon S3 or local storage or a database. It's a security bad practice. It is always recommended to use IAM roles to configure access to other AWS resources from Amazon EC2 instances. Therefore both these options are incorrect.\n\nSet up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can assume the role - There is no need for this option because when you create an IAM service role for Amazon EC2, the role automatically has Amazon EC2 identified as a trusted entity. Therefore this option is not correct.\n\nConfiguring a Service Role:\n\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q454",
    "questionText": "A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the company has transitioned from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the company is now looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data.\n\nHow will you implement this requirement without adding the overhead of splitting the data into logical groups?",
    "options": [
      {
        "text": "Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data",
        "isCorrect": false
      },
      {
        "text": "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data",
        "isCorrect": true
      },
      {
        "text": "Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data",
        "isCorrect": false
      },
      {
        "text": "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data\n\nServer-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates.\n\nNote: Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance.\n\nIncorrect options:\n\nStore the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement.\n\nUse Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect.\n\nConfigure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q455",
    "questionText": "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the Network Access Control List (Network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance.\n\nAs a solutions architect, how will you fix this issue?",
    "options": [
      {
        "text": "Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic",
        "isCorrect": false
      },
      {
        "text": "IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs",
        "isCorrect": false
      },
      {
        "text": "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic",
        "isCorrect": true
      },
      {
        "text": "Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSecurity Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic\n\nSecurity groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.\n\nTo enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.\n\nThe designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.\n\nBy default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.\n\nIf you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway (VGW).\n\nIncorrect options:\n\nNetwork ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic - This is incorrect as already discussed.\n\nIAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs - This is a made-up option and just added as a distractor.\n\nRules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool.\n\nReference:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q456",
    "questionText": "A company is transferring a significant volume of data from on-site storage to AWS, where it will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Part of this data will be accessed regularly, while the rest will be accessed less frequently. The company requires a hosting solution for this data that minimizes operational overhead.\n\nWhat solution would best meet these requirements?",
    "options": [
      {
        "text": "Set up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume",
        "isCorrect": true
      },
      {
        "text": "Set up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up an Amazon FSx for ONTAP instance. Configure an FSx for ONTAP file system on the root volume and migrate the data to the FSx for ONTAP volume\n\nAmazon FSx for NetApp ONTAP is a storage service that allows customers to launch and run fully managed ONTAP file systems in the cloud. ONTAP is NetAppâ€™s file system technology that provides a widely adopted set of data access and data management capabilities.\n\nAmazon FSx for NetApp ONTAP Overview\nvia - https://aws.amazon.com/fsx/netapp-ontap/\n\nThe given use case mandates that the storage on AWS will be accessed by Windows, Mac, and Linux-based Amazon EC2 instances within the same AWS region using both SMB and NFS protocols. Amongst the Amazon FSx family, FSx for ONTAP is the only file system that supports this key requirement.\n\n\nvia - https://aws.amazon.com/fsx/when-to-choose-fsx/\n\nIncorrect options:\n\nSet up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume\n\nSet up an Amazon Elastic File System (Amazon EFS) volume that uses EFS Infrequent Access. Use AWS DataSync to migrate the data to the EFS volume\n\nAmazon EFS is not supported on Windows instances. So, both these options are incorrect.\n\nSet up an Amazon FSx for OpenZFS instance. Configure an FSx for OpenZFS file ystem on the root volume and migrate the data to the FSx for OpenZFS volume - Amazon FSx for OpenZFS is a fully managed file storage service that lets you launch, run, and scale fully managed file systems built on the open-source OpenZFS file system. FSx for OpenZFS makes it easy to migrate your on-premises file servers without changing your applications or how you manage data, and to build new high-performance, data-intensive applications on the cloud. FSx for OpenZFS is compatible with Windows, Linux, macOS clients. It supports NFS 3, 4.0, 4.1, 4.2 protocols, however, it does NOT support the SMB protocol.\n\nReferences:\n\nhttps://aws.amazon.com/fsx/netapp-ontap/\n\nhttps://aws.amazon.com/fsx/when-to-choose-fsx/\n\nhttps://aws.amazon.com/fsx/openzfs/faqs/\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q457",
    "questionText": "A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an Amazon EC2 instance and needs to support up to 25,000 IOPS per volume.\n\nAs a solutions architect, which of the following Amazon Elastic Block Store (Amazon EBS) volume types would you recommend for this use-case?",
    "options": [
      {
        "text": "General Purpose SSD (gp2)",
        "isCorrect": false
      },
      {
        "text": "Cold HDD (sc1)",
        "isCorrect": false
      },
      {
        "text": "Provisioned IOPS SSD (io1)",
        "isCorrect": true
      },
      {
        "text": "Throughput Optimized HDD (st1)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nProvisioned IOPS SSD (io1)\n\nProvisioned IOPS SSD (io1) is backed by solid-state drives (SSDs) and is a high-performance Amazon EBS storage option designed for critical, I/O intensive database and application workloads, as well as throughput-intensive database workloads. io1 is designed to deliver a consistent baseline performance of up to 50 IOPS/GB to a maximum of 64,000 IOPS and provide up to 1,000 MB/s of throughput per volume. Therefore, the io1 volume type would be able to meet the requirement of 25,000 IOPS per volume for the given use-case.\n\nIncorrect options:\n\nGeneral Purpose SSD (gp2) - gp2 is backed by solid-state drives (SSDs) and is suitable for a broad range of transactional workloads, including dev/test environments, low-latency interactive applications, and boot volumes. It supports max IOPS/Volume of 16,000.\n\nCold HDD (sc1) - sc1 is backed by hard disk drives (HDDs). It is ideal for less frequently accessed workloads with large, cold datasets. It supports max IOPS/Volume of 250.\n\nThroughput Optimized HDD (st1) - st1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput-intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. It supports max IOPS/Volume of 500.\n\nReference:\n\nhttps://aws.amazon.com/ebs/volume-types/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q458",
    "questionText": "An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain.\n\nAs a solutions architect, which of the following AWS services would you recommend for this use-case?",
    "options": [
      {
        "text": "Active Directory Connector",
        "isCorrect": false
      },
      {
        "text": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "isCorrect": true
      },
      {
        "text": "Simple Active Directory (Simple AD)",
        "isCorrect": false
      },
      {
        "text": "Amazon Cloud Directory",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nAWS Directory Service provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services.\n\nAWS Directory Service for Microsoft Active Directory (aka AWS Managed Microsoft AD) is powered by an actual Microsoft Windows Server Active Directory (AD), managed by AWS. With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud such as SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO).\n\nIncorrect options:\n\nActive Directory Connector - Use AD Connector if you only need to allow your on-premises users to log in to AWS applications and services with their Active Directory credentials. AD Connector simply connects your existing on-premises Active Directory to AWS. You cannot use it to run directory-aware workloads on AWS, hence this option is not correct.\n\nSimple Active Directory (Simple AD) - Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. Simple AD is a standalone managed directory that is powered by a Samba 4 Active Directory Compatible Server.  Simple AD does not support features such as trust relationships with other domains. Therefore, this option is not correct.\n\nAmazon Cloud Directory - Amazon Cloud Directory is a cloud-native directory that can store hundreds of millions of application-specific objects with multiple relationships and schemas. Use Amazon Cloud Directory if you need a highly scalable directory store for your applicationâ€™s hierarchical data. You cannot use it to\nestablish trust relationships with other domains on the on-premises infrastructure. Therefore, this option is not correct.\n\nExam Alert:\n\nYou may see questions on choosing \"AWS Managed Microsoft AD\" vs \"AD Connector\" vs \"Simple AD\" on the exam. Just remember that you should use AD Connector if you only need to allow your on-premises users to log in to AWS applications with their Active Directory credentials. AWS Managed Microsoft AD would also allow you to run directory-aware workloads in the AWS Cloud.  AWS Managed Microsoft AD is your best choice if you have more than 5,000 users and need a trust relationship set up between an AWS hosted directory and your on-premises directories. Simple AD is the least expensive option and your best choice if you have 5,000 or fewer users and donâ€™t need the more advanced Microsoft Active Directory features such as trust relationships with other domains.\n\nReference:\n\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html",
    "awsService": "General",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q459",
    "questionText": "A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack.\n\nAs a solutions architect, which of the following solutions would you recommend to the team?",
    "options": [
      {
        "text": "Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "isCorrect": true
      },
      {
        "text": "Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "isCorrect": false
      },
      {
        "text": "Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "isCorrect": false
      },
      {
        "text": "Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\n\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - â€œWhat did my AWS resource look like at xyz point in time?â€\n\nHow AWS Config Works:\n\nvia - https://aws.amazon.com/config/\n\nIncorrect options:\n\nUse Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes - AWS CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. You cannot use Amazon CloudWatch to maintain a history of resource configuration changes.\n\nUse AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes - With AWS CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - â€œWho made an API call to modify this resource?â€. AWS CloudTrail provides an event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use AWS CloudTrail to maintain a history of resource configuration changes.\n\nUse AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. You cannot use AWS Systems Manager to maintain a history of resource configuration changes.\n\nExam Alert:\n\nYou may see scenario-based questions asking you to select one of Amazon CloudWatch vs AWS CloudTrail vs AWS Config. Just remember this thumb rule -\n\nThink resource performance monitoring, events, and alerts; think Amazon CloudWatch.\n\nThink account-specific activity and audit; think AWS CloudTrail.\n\nThink resource-specific history, audit, and compliance; think AWS Config.\n\nReferences:\n\nhttps://aws.amazon.com/config/\n\nhttps://aws.amazon.com/cloudwatch/\n\nhttps://aws.amazon.com/cloudtrail/\n\nhttps://aws.amazon.com/systems-manager/",
    "awsService": "CloudWatch",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q460",
    "questionText": "A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past weekâ€™s data from the production database to keep the database small while still keeping its data accessible.\n\nAs a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?",
    "options": [
      {
        "text": "Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script",
        "isCorrect": false
      },
      {
        "text": "Schedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job",
        "isCorrect": true
      },
      {
        "text": "Provision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression",
        "isCorrect": false
      },
      {
        "text": "Provision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSchedule a weekly Amazon EventBridge event cron expression to invoke an AWS Lambda function that runs the database rollover job\n\nAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. AWS Lambda supports standard rate and cron expressions for frequencies of up to once per minute.\n\nSchedule expressions using rate or cron:\n\n\nIncorrect options:\n\nCreate a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing and it's not the right fit for running a database rollover script. Although AWS Glue is also serverless, AWS Lambda is a more cost-effective option compared to AWS Glue.\n\nProvision an Amazon EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly (up to 90% off the On-Demand price).\nAs the Spot Instance runs whenever capacity is available, there is no guarantee that the weekly job will be executed during the defined time window. Additionally, the given use-case requires a serverless solution, therefore this option is incorrect.\n\nProvision an Amazon EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression - Scheduled Reserved Instances run on a part-time basis. Scheduled Reserved Instances option allows you to use reserve capacity on a recurring daily, weekly, and monthly schedules. Scheduled Reserved Instances are available for one-year terms at 5-10% below On-Demand rates. As the given use-case requires a serverless solution, therefore this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/lambda/\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents-expressions.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q461",
    "questionText": "A company has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3.\n\nAs a solutions architect, which of the following solutions would you recommend for this use-case?",
    "options": [
      {
        "text": "Use AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket",
        "isCorrect": false
      },
      {
        "text": "Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket",
        "isCorrect": false
      },
      {
        "text": "Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket",
        "isCorrect": false
      },
      {
        "text": "Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket\n\nAWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways â€“ Tape Gateway, File Gateway, and Volume Gateway â€“ that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access.\nWith cached volumes, the AWS Volume Gateway stores the full volume in its Amazon S3 service bucket, and just the recently accessed data is retained in the gatewayâ€™s local cache for low-latency access.\n\nIncorrect options:\n\nUse AWS Direct Connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. AWS Direct connect cannot be used to store the most frequently accessed logs locally for low-latency access.\n\nUse AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket - With stored volumes, your entire data volume is available locally in the gateway, for fast read access. Volume Gateway also maintains an asynchronous copy of your stored volume in the serviceâ€™s Amazon S3 bucket. This does not fit the requirements per the given use-case, hence this option is not correct.\n\nUse AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket - You can use Snowball Edge Storage Optimized device to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. Snowball Edge Storage Optimized device cannot be used to store the most frequently accessed logs locally for low-latency access.\n\nReference:\n\nhttps://aws.amazon.com/storagegateway/volume/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q462",
    "questionText": "A financial services firm runs a containerized risk analytics tool in its on-premises data center using Docker. The tool depends on persistent data storage for maintaining customer simulation results and operates on a single host machine where the volume is locally mounted. The infrastructure team is looking to replatform the tool to AWS using a fully managed service because they want to avoid managing EC2 instances, volumes, or underlying servers.\n\nWhich AWS solution best meets these requirements?",
    "options": [
      {
        "text": "Use Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually",
        "isCorrect": false
      },
      {
        "text": "Use Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage",
        "isCorrect": false
      },
      {
        "text": "Use AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon ECS with Fargate launch type. Provision an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS volume inside the container at runtime to provide persistent storage access\n\nThis is the most suitable and fully managed solution. ECS with Fargate allows you to run containers without managing EC2 infrastructure, and it integrates natively with Amazon EFS to support persistent, shared storage for stateful applications. You define the EFS volume in the task definition and mount it to the container, allowing multiple tasks to access shared data. This setup requires no server or storage lifecycle management and aligns perfectly with the company's requirements.\n\nIncorrect options:\n\nUse Amazon EKS with managed node groups. Provision an Amazon EBS volume and mount it inside the container by creating a Kubernetes persistent volume and claim. Manage storage lifecycle manually - Although EKS supports Kubernetes-native persistent storage with EBS volumes, this approach still requires provisioning and managing EC2 instances and storage volumes, which contradicts the requirement to avoid managing infrastructure.\n\nUse Amazon ECS with Fargate launch type. Attach an Amazon S3 bucket using a shared access script that mounts the S3 bucket into the container for data storage - Amazon S3 is object storage, not a block or file system, and cannot be mounted directly into a container like a file system volume. While applications can access S3 via API, this does not fulfill the requirement for a persistent storage volume that functions like a mounted file system.\n\nUse AWS Lambda with a container image runtime. Store stateful data in temporary local storage (/tmp) and sync with Amazon S3 periodically - AWS Lambda supports custom container images and provides ephemeral /tmp storage during execution, but this storage is limited in size (512 MB) and not persistent across invocations. Syncing to S3 would require custom logic, and this setup is inappropriate for long-running or stateful microservices. Lambda is best suited for stateless, event-driven functions, not for applications requiring mounted persistent volumes.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/efs-volumes.html\n\nhttps://aws.amazon.com/blogs/containers/developers-guide-to-using-amazon-efs-with-amazon-ecs-and-aws-fargate-part-1/\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q463",
    "questionText": "A company wants to improve its gaming application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's performance metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable user data for the community of users. The leaderboard would be accessed by millions of users simultaneously.\n\nWhich of the following options support the case for using Amazon ElastiCache to meet the given requirements? (Select two)",
    "options": [
      {
        "text": "Use Amazon ElastiCache to improve latency and throughput for read-heavy application workloads",
        "isCorrect": true
      },
      {
        "text": "Use Amazon ElastiCache to improve latency and throughput for write-heavy application workloads",
        "isCorrect": false
      },
      {
        "text": "Use Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads",
        "isCorrect": false
      },
      {
        "text": "Use Amazon ElastiCache to improve the performance of compute-intensive workloads",
        "isCorrect": true
      },
      {
        "text": "Use Amazon ElastiCache to run highly complex JOIN queries",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon ElastiCache to improve latency and throughput for read-heavy application workloads\n\nUse Amazon ElastiCache to improve the performance of compute-intensive workloads\n\nAmazon ElastiCache allows you to run in-memory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.\n\n\nvia - https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\n\nAmazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, leaderboard, and Q&A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache.\n\nOverview of Amazon ElastiCache features:\n\nvia - https://aws.amazon.com/elasticache/features/\n\nIncorrect options:\n\nUse Amazon ElastiCache to improve latency and throughput for write-heavy application workloads - As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads. Caching is not a good fit for write-heavy applications as the cache goes stale at a very fast rate.\n\nUse Amazon ElastiCache to improve the performance of Extract-Transform-Load (ETL) workloads - ETL workloads involve reading and transforming high-volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads.\n\nUse Amazon ElastiCache to run highly complex JOIN queries - Complex JSON queries can be run on relational databases such as Amazon RDS or Amazon Aurora. Amazon ElastiCache is not a good fit for this use case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\n\nhttps://aws.amazon.com/elasticache/features/",
    "awsService": "ElastiCache",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q464",
    "questionText": "A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon Simple Queue Service (Amazon SQS). The development team at the company wants to minimize the costs of using Amazon SQS.\n\nAs a solutions architect, which of the following options would you recommend for the given use-case?",
    "options": [
      {
        "text": "Use SQS short polling to retrieve messages from your Amazon SQS queues",
        "isCorrect": false
      },
      {
        "text": "Use SQS visibility timeout to retrieve messages from your Amazon SQS queues",
        "isCorrect": false
      },
      {
        "text": "Use SQS long polling to retrieve messages from your Amazon SQS queues",
        "isCorrect": true
      },
      {
        "text": "Use SQS message timer to retrieve messages from your Amazon SQS queues",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse SQS long polling to retrieve messages from your Amazon SQS queues\n\nAmazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.\n\nAmazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\n\nLong polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Using long polling can reduce the cost of using SQS because you can reduce the number of empty receives.\n\nShort Polling vs Long Polling:\n\nvia - https://aws.amazon.com/sqs/faqs/\n\nIncorrect options:\n\nUse SQS short polling to retrieve messages from your Amazon SQS queues - With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives.\n\nUse SQS visibility timeout to retrieve messages from your Amazon SQS queues - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.\n\nUse SQS message timer to retrieve messages from your Amazon SQS queues - You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\n\nhttps://aws.amazon.com/sqs/faqs/",
    "awsService": "SQS",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q465",
    "questionText": "A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve Domain Name System (DNS) queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network.\n\nAs a solutions architect, which of the following solutions can be combined to address the given use case? (Select two)",
    "options": [
      {
        "text": "Create an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint",
        "isCorrect": false
      },
      {
        "text": "Create an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint",
        "isCorrect": false
      },
      {
        "text": "Create a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint",
        "isCorrect": false
      },
      {
        "text": "Create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint",
        "isCorrect": true
      },
      {
        "text": "Create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nCreate an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint\n\nCreate an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint\n\nAmazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS â€“ such as Amazon EC2 instances â€“ and can also be used to route users to infrastructure outside of AWS. By default, Amazon Route 53 Resolver automatically answers DNS queries for local VPC domain names for Amazon EC2 instances. You can integrate DNS resolution between Resolver and DNS resolvers on your on-premises network by configuring forwarding rules.\n\nTo resolve any DNS queries for resources in the AWS VPC from the on-premises network, you can create an inbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint.\n\nResolver Inbound Endpoint:\n\nvia - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html\n\nTo resolve DNS queries for any resources in the on-premises network from the AWS VPC, you can create an outbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint. To conditionally forward queries, you need to create Resolver rules that specify the domain names for the DNS queries that you want to forward (such as example.com) and the IP addresses of the DNS resolvers on the on-premises network that you want to forward the queries to.\n\nResolver Outbound Endpoint:\n\nvia - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html\n\nIncorrect options:\n\nCreate an outbound endpoint on Amazon Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via this endpoint - DNS resolvers on the on-premises network can forward DNS queries to Amazon Route 53 Resolver via an inbound endpoint. Hence, this option is incorrect.\n\nCreate an inbound endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint - Amazon Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via an outbound endpoint. Hence, this option is incorrect.\n\nCreate a universal endpoint on Amazon Route 53 Resolver and then Amazon Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint - There is no such thing as a universal endpoint on Amazon Route 53 Resolver. This option has been added as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-getting-started.html\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q466",
    "questionText": "A company has set up AWS Organizations to manage several departments running their own AWS accounts. The departments operate from different countries and are spread across various AWS Regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of Amazon EC2 instances, specific IAM roles for AWS Lambda functions, etc.\n\nAs a solutions architect, which of the following options would you recommend for this use-case?",
    "options": [
      {
        "text": "Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions",
        "isCorrect": true
      },
      {
        "text": "Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions",
        "isCorrect": false
      },
      {
        "text": "Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions",
        "isCorrect": false
      },
      {
        "text": "Use AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions\n\nAWS CloudFormation StackSet extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. Using an administrator account of an \"AWS Organization\", you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts of an \"AWS Organization\" across specified regions.\n\nAWS CloudFormation StackSets:\n\nvia - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\n\nIncorrect options:\n\nUse AWS CloudFormation templates to deploy the same template across AWS accounts and regions - AWS Cloudformation template is a JSON or YAML-format, text-based file that describes all the AWS resources you need to deploy to run your application. A template acts as a blueprint for a stack. AWS CloudFormation templates cannot be used to deploy the same template across AWS accounts and regions.\n\nUse AWS CloudFormation stacks to deploy the same template across AWS accounts and regions - AWS CloudFormation stack is a set of AWS resources that are created and managed as a single unit when AWS CloudFormation instantiates a template. A stack cannot be used to deploy the same template across AWS accounts and regions.\n\nUse AWS Resource Access Manager (AWS RAM) to deploy the same template across AWS accounts and regions - AWS Resource Access Manager (AWS RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. Resource Access Manager cannot be used to deploy the same template across AWS accounts and regions.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q467",
    "questionText": "A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world to upload and download video files that can reach a maximum size of 10 gigabytes. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience.\n\nAs a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?",
    "options": [
      {
        "text": "Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon S3 for hosting the web application and use Amazon S3 Transfer Acceleration (Amazon S3TA) to reduce the latency that geographically dispersed users might face\n\nAmazon S3 Transfer Acceleration (S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. S3 Transfer Acceleration (S3TA) reduces the variability in Internet routing, congestion, and speeds that can affect transfers, and logically shortens the distance to S3 for remote applications.\n\nS3TA improves transfer performance by routing traffic through Amazon CloudFrontâ€™s globally distributed Edge Locations and over AWS backbone networks, and by using network protocol optimizations.\n\nFor applications interacting with your Amazon S3 buckets through the S3 API from outside of your bucketâ€™s region, S3TA helps avoid the variability in Internet routing and congestion. It does this by routing your uploads and downloads over the AWS global network infrastructure, so you get the benefit of AWS network optimizations.\n\nIncorrect options:\n\nUse Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users - Amazon S3 with Amazon CloudFront is a very powerful way of distributing static content to geographically dispersed users with low latency speeds. If you have objects that are smaller than 1GB or if the data set is less than 1GB in size, you should consider using Amazon CloudFront's PUT/POST commands for optimal performance. The given use case has data larger than 1GB and hence S3 Transfer Acceleration is a better option.\n\n\nvia - https://aws.amazon.com/s3/faqs/\n\nUse Amazon EC2 with AWS Global Accelerator for faster distribution of content, while using Amazon S3 as storage service- AWS Global Accelerator is a networking service that sends your userâ€™s traffic through Amazon Web Serviceâ€™s global network infrastructure, improving your internet user performance by up to 60%. With AWS Global Accelerator, you are provided two global static customer-facing IPs to simplify traffic management. On the back end, add or remove your AWS application origins, such as Network Load Balancers, Application Load Balancers, Elastic IPs, and Amazon EC2 Instances, without making user-facing changes. As discussed, AWS Global Accelerator is meant for a different use case and is not meant for increasing the speed of Amazon S3 uploads or downloads.\n\nUse Amazon EC2 with Amazon ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Amazon S3 Transfer Acceleration is a better performing option than opting for Amazon EC2 with Amazon ElastiCache, which is not meant to address the given use-case.\n\nReference:\n\n[https://aws.amazon.com/s3/transfer-acceleration/](https://aws.amazon.com/s3/transfer-acceleration\n\nhttps://aws.amazon.com/s3/faqs/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q468",
    "questionText": "A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single Amazon EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes.\n\nIn case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?",
    "options": [
      {
        "text": "Configure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails",
        "isCorrect": false
      },
      {
        "text": "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes",
        "isCorrect": false
      },
      {
        "text": "Configure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume",
        "isCorrect": true
      },
      {
        "text": "Configure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance, however, should only be configured with an Amazon EBS volume\n\nIf your instance fails a system status check, you can use Amazon CloudWatch alarm actions to automatically recover it. The recover option is available for over 90% of deployed customer Amazon EC2 instances. The Amazon CloudWatch recovery option works only for system check failures, not for instance status check failures. Also, if you terminate your instance, then it can't be recovered.\n\nYou can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group.\n\nThe automatic recovery process attempts to recover your instance for up to three separate failures per day. Your instance may subsequently be retired if automatic recovery fails and a hardware degradation is determined to be the root cause for the original system status check failure.\n\nIncorrect options:\n\nConfigure Amazon EventBridge events that can trigger the recovery of the Amazon EC2 instance, in case the instance or the application fails - You cannot use Amazon EventBridge events to directly trigger the recovery of the Amazon EC2 instance.\n\nConfigure an Amazon CloudWatch alarm that triggers the recovery of the Amazon EC2 instance, in case the instance fails. The instance can be configured with Amazon Elastic Block Store (Amazon EBS) or with instance store volumes - The recover action is supported only on instances that have Amazon EBS volumes configured on them, instance store volumes are not supported for automatic recovery by Amazon CloudWatch alarms.\n\nConfigure AWS Trusted Advisor to monitor the health check of Amazon EC2 instance and provide a remedial action in case an unhealthy flag is detected - You can use Amazon EventBridge events to detect and react to changes in the status of AWS Trusted Advisor checks. This support is only available with AWS Business Support and AWS Enterprise Support. AWS Trusted Advisor by itself does not support health checks of Amazon EC2 instances or their recovery.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q469",
    "questionText": "A legacy application is built using a tightly-coupled monolithic architecture. Due to a sharp increase in the number of users, the application performance has degraded. The company now wants to decouple the architecture and adopt AWS microservices architecture. Some of these microservices need to handle fast running processes whereas other microservices need to handle slower processes.\n\nWhich of these options would you identify as the right way of connecting these microservices?",
    "options": [
      {
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones",
        "isCorrect": false
      },
      {
        "text": "Configure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones",
        "isCorrect": false
      },
      {
        "text": "Add Amazon EventBridge to decouple the complex architecture",
        "isCorrect": false
      },
      {
        "text": "Configure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nConfigure Amazon Simple Queue Service (Amazon SQS) queue to decouple microservices running faster processes from the microservices running slower ones\n\nAmazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.\n\nUse Amazon SQS to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available. Amazon SQS lets you decouple application components so that they run and fail independently, increasing the overall fault tolerance of the system. Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed. Being able to store the messages and replay them is a very important feature in decoupling the system architecture, as is needed in the current use case.\n\nIncorrect options:\n\nUse Amazon Simple Notification Service (Amazon SNS) to decouple microservices running faster processes from the microservices running slower ones - Amazon SNS follows the \"publish-subscribe\" (pub-sub) messaging paradigm, with notifications being delivered to clients using a \"push\" mechanism. This is an important difference between Amazon SNS and Amazon SQS. Whereas Amazon SQS is a polling mechanism, that gives applications the chance to poll at their own comfort, the push mechanism assumes the other applications are present. For the current requirement, we need messages to be stored till they are processed by the downstream applications. Hence, Amazon SQS is the right choice.\n\nConfigure Amazon Kinesis Data Streams to decouple microservices running faster processes from the microservices running slower ones - Amazon Kinesis Data Streams are used for streaming real-time high-volume data. Amazon Kinesis is a publish-subscribe model, used when publisher applications need to publish the same data to different consumers in parallel. Amazon SQS is the right fit for the current use case.\n\nAdd Amazon EventBridge to decouple the complex architecture - This event-based service is extremely useful for connecting non-AWS SaaS (Software as a Service) services to AWS services. With Amazon Eventbridge, the downstream application would need to immediately process the events whenever they arrive, thereby making it a tightly coupled scenario. Hence, this option is not correct.\n\nReferences:\n\nhttps://aws.amazon.com/sqs/",
    "awsService": "SNS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q470",
    "questionText": "A small business has been running its IT systems on the on-premises infrastructure but the business now plans to migrate to AWS Cloud for operational efficiencies.\n\nAs a Solutions Architect, can you suggest a cost-effective serverless solution for its flagship application that has both static and dynamic content?",
    "options": [
      {
        "text": "Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries",
        "isCorrect": false
      },
      {
        "text": "Host the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions",
        "isCorrect": true
      },
      {
        "text": "Host the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy",
        "isCorrect": false
      },
      {
        "text": "Host both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nHost the static content on Amazon S3 and use AWS Lambda with Amazon DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of AWS Lambda for distribution across diverse regions\n\nAWS Lambda with Amazon DynamoDB is the right answer for a serverless solution. Amazon CloudFront will help in enhancing user experience by delivering content, across different geographic locations with low latency. Amazon S3 is a cost-effective and faster way of distributing static content for web applications.\n\nIncorrect options:\n\nHost both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries - Amazon S3 is not the right fit for hosting Dynamic content, so this option is incorrect.\n\nHost the static content on Amazon S3 and use Amazon EC2 with Amazon RDS for generating the dynamic content. Amazon CloudFront can be configured in front of Amazon EC2 instance, to make global distribution easy - The company is looking for a serverless solution, and Amazon EC2 is not a serverless service as the Amazon EC2 instances have to be managed by AWS customers.\n\nHost both the static and dynamic content of the web application on Amazon EC2 with Amazon RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions - This is a possible solution, but not a cost-effective or optimal one. Since static content can be cost-effectively managed on Amazon S3 and can be accessed and distributed faster when compared to fetching the content from the Amazon EC2 server.\n\nReference:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q471",
    "questionText": "The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the Amazon RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity.\n\nCan you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?",
    "options": [
      {
        "text": "Increase the size of Amazon RDS instance",
        "isCorrect": false
      },
      {
        "text": "Migrate from General Purpose SSD to magnetic storage to enhance IOPS",
        "isCorrect": false
      },
      {
        "text": "Create a read replica and connect the report generation tool/application to it",
        "isCorrect": true
      },
      {
        "text": "Configure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a read replica and connect the report generation tool/application to it\n\nAmazon RDS Read Replicas provide enhanced performance and durability for Amazon RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances.\n\nThere are a variety of scenarios where deploying one or more read replicas for a given source DB instance may make sense. Common reasons for deploying a read replica include:\n\n\nScaling beyond the compute or I/O capacity of a single DB instance for read-heavy database workloads. This excess read traffic can be directed to one or more read replicas.\nServing read traffic while the source DB instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your read replica(s). For this use case, keep in mind that the data on the read replica may be â€œstaleâ€ since the source DB Instance is unavailable.\nBusiness reporting or data warehousing scenarios; you may want business reporting queries to run against a read replica, rather than your primary, production DB Instance.\nYou may use a read replica for disaster recovery of the source DB instance, either in the same AWS Region or in another Region.\n\n\nComparing Read Replicas with Multi-AZ and Multi-Region Amazon RDS deployments:\n\nvia - https://aws.amazon.com/rds/features/read-replicas/\n\nIncorrect options:\n\nIncrease the size of Amazon RDS instance - This will not help as it's mentioned that the CPU, memory, and storage are running at only 50% of the total capacity.\n\nMigrate from General Purpose SSD to magnetic storage to enhance IOPS - This is incorrect. Amazon RDS supports magnetic storage for backward compatibility only. AWS recommends that you use General Purpose SSD or Provisioned IOPS for any storage needs.\n\nConfigure the Amazon RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. However, you cannot read from the standby database, making multi-AZ, an incorrect option for the given scenario.\n\nReference:\n\nhttps://aws.amazon.com/rds/features/read-replicas/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q472",
    "questionText": "An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to cope up with the traffic and application freezes on most of the pages.\n\nWhich of the following is a cost-optimal solution that does not need provisioning of infrastructure?",
    "options": [
      {
        "text": "Configure AWS Lambda with an Amazon RDS database to provide a serverless architecture",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets",
        "isCorrect": false
      },
      {
        "text": "Use AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon CloudFront with Amazon S3 as the storage solution for the static assets\n\nWhen you put your content in an Amazon S3 bucket in the cloud, a lot of things become much easier. First, you donâ€™t need to plan for and allocate a specific amount of storage space because Amazon S3 buckets scale automatically. As Amazon S3 is a serverless service, you donâ€™t need to manage or patch servers that store files yourself; you just put and get your content. Finally, even if you require a server for your application (for example, because you have a dynamic application), the server can be smaller because it doesnâ€™t have to handle requests for static content.\n\nAmazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from Amazon S3 directly to your users. Amazon CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located.\n\nWhen a user requests content that you serve with Amazon CloudFront, their request is routed to a nearby Edge Location. If Amazon CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file theyâ€™ve requested isnâ€™t yet cached, CloudFront retrieves it from your origin â€“ for example, the Amazon S3 bucket where youâ€™ve stored your content. Then, for the next local request for the same content, itâ€™s already cached nearby and can be served immediately.\n\nBy caching your content in Edge Locations, Amazon CloudFront reduces the load on your Amazon S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using Amazon CloudFront is often more cost-effective than serving files directly from Amazon S3, and there is no data transfer fee from Amazon S3 to Amazon CloudFront. You only pay for what is delivered to the internet from Amazon CloudFront, plus request fees.\n\nIncorrect options:\n\nConfigure AWS Lambda with an Amazon RDS database to provide a serverless architecture - Amazon RDS is not the right choice for the current scenario because of the overhead of a database management system, as the given use-case can be addressed by using Amazon S3 storage solution.\n\nUse Amazon CloudFront with Amazon DynamoDB for greater speed and low latency access to static assets - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. But, Amazon DynamoDB is overkill for the given use-case and will prove to be a very costly solution.\n\nUse AWS Lambda with Amazon ElastiCache and Amazon RDS for serving static assets at high speed and low latency - As discussed above, Amazon RDS is not needed for this use case where web application needs to display static pages and facilitate downloads of historic data. Amazon S3 is much better suited for this requirement.\n\nReference:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q473",
    "questionText": "An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain.\n\nWhich of the following solutions addresses these requirements with the minimal integration effort?",
    "options": [
      {
        "text": "Use Amazon FSx for Windows File Server as a shared storage solution",
        "isCorrect": true
      },
      {
        "text": "Use File Gateway of AWS Storage Gateway to create a hybrid storage solution",
        "isCorrect": false
      },
      {
        "text": "Use Amazon FSx for Lustre as a shared storage solution with millisecond latencies",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Elastic File System (Amazon EFS) as a shared storage solution",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon FSx for Windows File Server as a shared storage solution\n\nAmazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. It offers single-AZ and multi-AZ deployment options, fully managed backups, and encryption of data at rest and in transit. You can optimize cost and performance for your workload needs with SSD and HDD storage options; and you can scale storage and change the throughput performance of your file system at any time.\n\nWith Amazon FSx, you get highly available and durable file storage starting from $0.013 per GB-month. Data deduplication enables you to optimize costs even further by removing redundant data. You can increase your file system storage and scale throughput capacity at any time, making it easy to respond to changing business needs. There are no upfront costs or licensing fees.\n\nHow Amazon FSx for Windows File Server works:\n\nvia - https://aws.amazon.com/fsx/windows/\n\nIncorrect options:\n\nUse File Gateway of AWS Storage Gateway to create a hybrid storage solution - AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration between your on-premises IT environment and the AWS storage infrastructure. Storage Gateway uses Amazon S3 to store data on AWS Cloud and from here the on-premises data can seamlessly integrate with Cloud services. It is not suited to be used as a shared storage space that multiple applications can access in parallel.\n\nUse Amazon FSx for Lustre as a shared storage solution with millisecond latencies - Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for compute workloads. Many workloads such as machine learning, high performance computing (HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data through high-performance shared storage. Lustre is Linux based, hence it is not the right choice since the use case is about Windows-based applications.\n\nUse Amazon Elastic File System (Amazon EFS) as a shared storage solution - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS is a powerful, shared storage solution that would have been the right answer if the customer systems were Linux based. Amazon EFS is compatible with only Linux-based AMIs for Amazon EC2.\n\nReference:\n\nhttps://aws.amazon.com/fsx/windows/",
    "awsService": "EFS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q474",
    "questionText": "A company has its application servers in the public subnet that connect to the database instances in the private subnet. For regular maintenance, the database instances need patch fixes that need to be downloaded from the internet.\n\nConsidering the company uses only IPv4 addressing and is looking for a fully managed service, which of the following would you suggest as an optimal solution?",
    "options": [
      {
        "text": "Configure a Network Address Translation instance (NAT instance) in the public subnet of the VPC",
        "isCorrect": false
      },
      {
        "text": "Configure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC",
        "isCorrect": true
      },
      {
        "text": "Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables",
        "isCorrect": false
      },
      {
        "text": "Configure an Egress-only internet gateway for the resources in the private subnet of the VPC",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure a Network Address Translation gateway (NAT gateway) in the public subnet of the VPC\n\nYou can use a Network Address Translation gateway (NAT gateway) to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside.\n\nYou must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed after you associate it with the NAT Gateway. After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet. If you no longer need a NAT gateway, you can delete it. Deleting a NAT gateway disassociates its Elastic IP address, but does not release the address from your account.\n\nVPC architecture with NAT:\n\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nIncorrect options:\n\nConfigure an Egress-only internet gateway for the resources in the private subnet of the VPC - An Egress-only internet gateway is an Internet Gateway that supports IPv6 traffic, so this option is not correct for the given use-case.\n\nConfigure a Network Address Translation instance (NAT instance) in the public subnet of the VPC - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the internet. NAT instances are not a managed service, it has to be managed and maintained by the customer.\n\nConfigure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables - Internet Gateway cannot be used directly with a private subnet. It is not possible to set up this configuration, without a NAT instance or a NAT gateway in the public subnet.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q475",
    "questionText": "A gaming company uses Application Load Balancers in front of Amazon EC2 instances for different services and microservices. The architecture has now become complex with too many Application Load Balancers in multiple AWS Regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations.\n\nThe company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?",
    "options": [
      {
        "text": "Configure Elastic IPs for each of the Application Load Balancers in each Region",
        "isCorrect": false
      },
      {
        "text": "Set up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Launch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints",
        "isCorrect": true
      },
      {
        "text": "Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nLaunch AWS Global Accelerator and create endpoints for all the Regions. Register the Application Load Balancers of each Region to the corresponding endpoints\n\nAWS Global Accelerator is a networking service that sends your userâ€™s traffic through Amazon Web Serviceâ€™s global network infrastructure, improving your internet user performance by up to 60%. When the internet is congested, Global Acceleratorâ€™s automatic routing optimizations will help keep your packet loss, jitter, and latency consistently low.\n\nWith AWS Global Accelerator, you are provided two global static customer-facing IPs to simplify traffic management. On the back end, add or remove your AWS application origins, such as Network Load Balancers, Application Load Balancers, elastic IP address (EIP), and Amazon EC2 Instances, without making user-facing changes. To mitigate endpoint failure, AWS Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint.\n\nSimplified and resilient traffic routing for multi-Region applications:\n\nvia - https://aws.amazon.com/global-accelerator/\n\nIncorrect options:\n\nConfigure Elastic IPs for each of the Application Load Balancers in each Region - An Application Load Balancer cannot be assigned an Elastic IP address (static IP address).\n\nSet up a Network Load Balancer with elastic IP address. Register the private IPs of all the Application Load Balancers as targets of this Network Load Balancer - A Network Load Balancer can be configured to take an Elastic IP address. However, with hundreds of Application Load Balancers and Network Load Balancers, the solution will be equally cumbersome to manage.\n\nAssign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 instances to run behind the Auto Scaling Groups, for each of the Regions - You cannot assign an elastic IP address to an Auto Scaling Group (ASG), since ASG just manages a collection of Amazon EC2 instances.\n\nReferences:\n\nhttps://aws.amazon.com/global-accelerator/\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q476",
    "questionText": "A health care application processes the real-time health data of the patients into an analytics workflow. With a sharp increase in the number of users, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead.\n\nWhich of the following would you recommend as a scalable alternative to the current solution?",
    "options": [
      {
        "text": "Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing",
        "isCorrect": false
      },
      {
        "text": "Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon Kinesis Data Streams to ingest the data, process it using AWS Lambda or run analytics using Amazon Kinesis Data Analytics\n\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service with support for retry mechanism. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\n\nKDS makes sure your streaming data is available to multiple real-time analytics applications, to Amazon S3, or AWS Lambda within 70 milliseconds of the data being collected. Amazon Kinesis data streams scale from megabytes to terabytes per hour and scale from thousands to millions of PUT records per second. You can dynamically adjust the throughput of your stream at any time based on the volume of your input data.\n\nHow Data Streams work:\n\nvia - https://aws.amazon.com/kinesis/data-streams/?nc=sn&loc=2&dn=2\n\nIncorrect options:\n\nUse Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing - Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. Amazon SNS is a push mechanism that does not support robust retry mechanisms, as is needed in the current use case.\n\nUse Amazon Simple Queue Service (Amazon SQS) for data ingestion and configure AWS Lambda to trigger logic for downstream processing - Amazon Simple Queue Service (Amazon SQS) is a messaging service that helps in decoupling systems and reducing the complexity of architecture. Amazon SQS can still work but Amazon Kinesis Data streams is custom made for streaming real-time data.\n\nUse Amazon API Gateway with the existing REST-based interface to create a high performing architecture - Amazon API Gateway is not meant for handling real-time streaming data.\n\nReference:\n\nhttps://aws.amazon.com/kinesis/data-streams/?nc=sn&loc=2&dn=2",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q477",
    "questionText": "An e-commerce company has deployed its application on several Amazon EC2 instances that are configured in a private subnet using IPv4. These Amazon EC2 instances read and write a huge volume of data to and from Amazon S3 in the same AWS region. The company has set up subnet routing to direct all the internet-bound traffic through a Network Address Translation gateway (NAT gateway). The company wants to build the most cost-optimal solution without impacting the application's ability to communicate with Amazon S3 or the internet.\n\nAs an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
    "options": [
      {
        "text": "Set up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint",
        "isCorrect": true
      },
      {
        "text": "Provision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic",
        "isCorrect": false
      },
      {
        "text": "Set up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic",
        "isCorrect": false
      },
      {
        "text": "Set up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up a VPC gateway endpoint for Amazon S3. Attach an endpoint policy to the endpoint. Update the route table to direct the S3-bound traffic to the VPC endpoint\n\nGateway endpoints provide reliable connectivity to Amazon S3 without requiring an internet gateway or a NAT device for your VPC. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to Amazon S3. There is no additional charge for using gateway endpoints.\n\nThe VPC endpoint policy for the gateway endpoint controls access to Amazon S3 from the VPC through the endpoint. The default policy allows full access.\n\n\nvia - https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html\n\nUsing the VPC gateway endpoint allows the Amazon EC2 instances to reach Amazon S3 without using the public internet. Since the data transfer remains within the same AWS region, so there is no data transfer costs for ingress as well as egress traffic. Hence this is the most cost-optimal solution.\n\nIncorrect options:\n\nProvision an internet gateway. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL (NACL) to allow the S3-bound traffic - If a subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet. If a subnet is associated with a route table that does not have a route to an internet gateway, it's known as a private subnet. This option has been added as a distractor as adding a route to the internet gateway in the route table associated with the private subnet would make the subnet public. This would also make the internet-bound routing to the NAT gateway redundant. This option has been added as a distractor.\n\nSet up an egress-only internet gateway in the public subnet. Update the route table in the private subnet to route traffic to the internet gateway. Update the network ACL to allow the S3-bound traffic - An egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating an IPv6 connection with your instances. Since the use case talks about only IPv4 traffic, so this option is incorrect.\n\nSet up a Gateway Load Balancer (GWLB) endpoint for Amazon S3. Update the route table in the private subnet to direct the S3-bound traffic via the Gateway Load Balancer (GWLB) endpoint - Gateway Load Balancers use Gateway Load Balancer endpoints to securely exchange traffic across VPC boundaries. A Gateway Load Balancer endpoint is a VPC endpoint that provides private connectivity between virtual appliances in the service provider VPC and application servers in the service consumer VPC. You cannot set up a gateway load balancer endpoint to access Amazon S3. This option has been added as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/vpc-reduce-nat-gateway-transfer-costs/\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway-load-balancer.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q478",
    "questionText": "An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the Amazon S3 outbound data costs.\n\nAs a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?",
    "options": [
      {
        "text": "To reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data",
        "isCorrect": false
      },
      {
        "text": "Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively",
        "isCorrect": true
      },
      {
        "text": "Configure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively\n\nStoring content with Amazon S3 provides a lot of advantages. But to help optimize your applicationâ€™s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your Amazon S3 bucket to serve and protect the content.\n\nAmazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from Amazon S3 directly to your users.\n\nAmazon CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located. Amazon CloudFront has edge servers in locations all around the world.\n\nWhen a user requests content that you serve with Amazon CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file theyâ€™ve requested isnâ€™t yet cached, Amazon CloudFront retrieves it from your origin â€“ for example, the S3 bucket where youâ€™ve stored your content. Then, for the next local request for the same content, itâ€™s already cached nearby and can be served immediately.\n\nBy caching your content in Edge Locations, Amazon CloudFront reduces the load on your Amazon S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from Amazon S3, and there is no data transfer fee from Amazon S3 to CloudFront. You only pay for what is delivered to the internet from Amazon CloudFront, plus request fees.\n\nIncorrect options:\n\nTo reduce Amazon S3 cost, the data can be saved on an Amazon EBS volume connected to an Amazon EC2 instance that can host the application - Amazon EBS volumes are fast and are relatively cheap (though Amazon S3 is still a cheaper alternative). But, Amazon EBS volumes are accessible only through Amazon EC2 instances and are bound to a specific region.\n\nUse Amazon EFS service, as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data - Amazon EFS is a shareable file system that can be mounted onto Amazon EC2 instances. Amazon EFS is costlier than Amazon EBS and not a solution if the company is looking at reducing costs.\n\nConfigure Amazon S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to Amazon S3 buckets - This statement is incorrect and given only as a distractor. You can use Amazon S3 Batch Operations to perform large-scale batch operations on Amazon S3 objects, and it has nothing to do with content distribution.\n\nReference:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q479",
    "questionText": "A data analytics company manages an application that stores user data in a Amazon DynamoDB table. The development team has observed that once in a while, the application writes corrupted data in the Amazon DynamoDB table. As soon as the issue is detected, the team needs to remove the corrupted data at the earliest.\n\nWhat do you recommend?",
    "options": [
      {
        "text": "Configure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon DynamoDB point in time recovery to restore the table to the state just before corrupted data was written\n\nAmazon DynamoDB enables you to back up your table data continuously by using point-in-time recovery (PITR). When you enable PITR, DynamoDB backs up your table data automatically with per-second granularity so that you can restore to any given second in the preceding 35 days.\n\nPITR helps protect you against accidental writes and deletes. For example, if a test script writes accidentally to a production DynamoDB table or someone mistakenly issues a \"DeleteItem\" call, PITR has you covered.\n\nIncorrect options:\n\nUse Amazon DynamoDB on-demand backup to restore the table to the state just before corrupted data was written - The on-demand backup and restore process scales without degrading the performance or availability of your applications. It uses a new and unique distributed technology that lets you complete backups in seconds regardless of table size. You can create backups that are consistent within seconds across thousands of partitions without worrying about schedules or long-running backup processes. All on-demand backups are cataloged, discoverable, and retained until they are explicitly deleted.\n\nOn-demand backup is created upon request. So this option is not correct since an on-demand backup cannot be created pre-emptively to handle data corruption issues that happen once in a while.\n\nConfigure the Amazon DynamoDB table as a global table and point the application to use the table from another AWS region that has no corrupted data - Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications.\n\nGlobal tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region.\n\nAny changes made to any item in any replica table are replicated to all the other replicas within the same global table. In a global table, a newly written item is usually propagated to all replica tables within a second. With a global table, each replica table stores the same set of data items. Amazon DynamoDB does not support partial replication of only some of the items. If applications update the same item in different Regions at about the same time, conflicts can arise. To help ensure eventual consistency, Amazon DynamoDB global tables use a last-writer-wins reconciliation between concurrent updates, in which DynamoDB makes its best effort to determine the last writer. With this conflict resolution mechanism, all replicas agree on the latest update and converge toward a state in which they all have identical data.\n\nGlobal tables replicate your Amazon DynamoDB tables automatically across your choice of AWS Regions. This option has been added as a distractor since you cannot point the application to use the table from another AWS region, since there is no \"other\" table in another region. It's just a single logical Global table.\n\nUse Amazon DynamoDB Streams to restore the table to the state just before corrupted data was written - Amazon DynamoDB Streams captures a time-ordered sequence of item-level modifications in any Amazon DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time. A DynamoDB stream is an ordered flow of information about changes to items in a Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.\n\nAmazon DynamoDB Streams writes stream records in near-real time so that you can build applications that consume these streams and take action based on the contents. It will take considerable effort and custom coding to reliably rebuild table data to the state just before any corrupted data was written. So this option is not the best fit.\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery_Howitworks.html\n\nhttps://aws.amazon.com/dynamodb/global-tables/\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
    "awsService": "DynamoDB",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q480",
    "questionText": "The database backend for a retail company's website is hosted on Amazon RDS for MySQL having a primary instance and three read replicas to support read scalability. The company has mandated that the read replicas should lag no more than 1 second behind the primary instance to provide the best possible user experience. The read replicas are falling further behind during periods of peak traffic spikes, resulting in a bad user experience as the searches produce inconsistent results.\n\nYou have been hired as an AWS Certified Solutions Architect - Associate to reduce the replication lag as much as possible with minimal changes to the application code or the effort required to manage the underlying resources.\n\nWhich of the following will you recommend?",
    "options": [
      {
        "text": "Host the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas",
        "isCorrect": false
      },
      {
        "text": "Set up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling",
        "isCorrect": false
      },
      {
        "text": "Set up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nSet up database migration from Amazon RDS MySQL to Amazon Aurora MySQL. Swap out the MySQL read replicas with Aurora Replicas. Configure Aurora Auto Scaling\n\nAurora features a distributed, fault-tolerant, and self-healing storage system that is decoupled from compute resources and auto-scales up to 128 TiB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon Simple Storage Service (Amazon S3), and replication across three Availability Zones (AZs).\n\nSince Amazon Aurora Replicas share the same data volume as the primary instance in the same AWS Region, there is virtually no replication lag. The replica lag times are in the 10s of milliseconds (compared to the replication lag of seconds in the case of MySQL read replicas). Therefore, this is the right option to ensure that the read replicas lag no more than 1 second behind the primary instance.\n\nAurora Replicas:\n\nvia - https://aws.amazon.com/rds/aurora/faqs/\n\nIncorrect options:\n\nHost the MySQL primary database on a memory-optimized Amazon EC2 instance. Spin up additional compute-optimized Amazon EC2 instances to host the read replicas - Hosting the MySQL primary database and the read replicas on the Amazon EC2 instances would result in significant overhead to manage the underlying resources such as OS patching, database patching, etc. So this option is incorrect.\n\nSet up an Amazon ElastiCache for Redis cluster in front of the MySQL database. Update the website to check the cache before querying the read replicas - Introducing a caching layer would result in significant changes to the application code, so this option is incorrect.\n\nSet up database migration from Amazon RDS MySQL to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput and enable Auto-Scaling - Introducing a NoSQL database, such as Amazon DynamoDB, would result in significant changes to the application code since the database queries would have to be re-written for Amazon DynamoDB. Therefore, this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/rds/aurora/faqs/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q481",
    "questionText": "The development team at a retail company wants to optimize the cost of Amazon EC2 instances. The team wants to move certain nightly batch jobs to spot instances. The team has hired you as a solutions architect to provide the initial guidance.\n\nWhich of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)",
    "options": [
      {
        "text": "When you cancel an active spot request, it terminates the associated instance as well",
        "isCorrect": false
      },
      {
        "text": "If a spot request is persistent, then it is opened again after your Spot Instance is interrupted",
        "isCorrect": true
      },
      {
        "text": "If a spot request is persistent, then it is opened again after you stop the Spot Instance",
        "isCorrect": false
      },
      {
        "text": "Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated",
        "isCorrect": true
      },
      {
        "text": "When you cancel an active spot request, it does not terminate the associated instance",
        "isCorrect": true
      },
      {
        "text": "Spot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nIf a spot request is persistent, then it is opened again after your Spot Instance is interrupted\n\nA Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances.\n\nA Spot Instance request is either one-time or persistent. If the spot request is persistent, the request is opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance.\n\nHow Spot requests work:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html\n\nSpot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated\n\nA Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that is launched based on criteria that you specify. The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated. You can submit a Spot Fleet as a one-time request, which does not persist after the instances have been terminated. You can include On-Demand Instance requests in a Spot Fleet request.\n\nWhen you cancel an active spot request, it does not terminate the associated instance\n\nIf your Spot Instance request is active and has an associated running Spot Instance, or your Spot Instance request is disabled and has an associated stopped Spot Instance, canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. Moreover, to cancel a persistent Spot request and terminate its Spot Instances, you must cancel the Spot request first and then terminate the Spot Instances.\n\nIncorrect options:\n\nWhen you cancel an active spot request, it terminates the associated instance as well - If your Spot Instance request is active and has an associated running Spot Instance, then canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. So, this option is incorrect.\n\nIf a spot request is persistent, then it is opened again after you stop the Spot Instance - If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. So, this option is incorrect.\n\nSpot Fleets cannot maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated - As mentioned above, Spot Fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q482",
    "questionText": "A startup is building a serverless microservices architecture where client applications (web and mobile) authenticate users via a third-party OIDC-compliant identity provider. The backend APIs must validate JSON Web Tokens (JWTs) issued by this provider, enforce scope-based access control, and be cost-effective with minimal latency. The development team wants to use a fully managed service that supports JWT validation natively, without writing custom authentication logic.\n\nWhich solution should the team implement to meet these requirements?",
    "options": [
      {
        "text": "Use Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider",
        "isCorrect": true
      },
      {
        "text": "Use Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens",
        "isCorrect": false
      },
      {
        "text": "Use Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer",
        "isCorrect": false
      },
      {
        "text": "Deploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon API Gateway HTTP API with a native JWT authorizer configured to validate tokens from the OIDC provider\n\nAmazon API Gateway HTTP APIs support native JWT authorizers, allowing developers to configure the API to automatically validate JWT tokens issued by an OIDC-compliant identity provider, such as Auth0, Okta, or Amazon Cognito. This eliminates the need for custom authentication logic in Lambda functions and reduces both latency and cost. HTTP APIs are optimized for low-latency, high-performance workloads and are more cost-effective than REST APIs, making them ideal for modern, serverless applications that require standard JWT validation and claim-based access control.\n\nIncorrect options:\n\nUse Amazon API Gateway REST API with a Lambda function that manually validates JWT tokens - Although API Gateway REST APIs support custom Lambda authorizers, using them to validate JWT tokens requires writing, deploying, and maintaining custom authentication logic in Lambda functions. This introduces additional latency, increases costs, and violates the requirement for a native, no-code solution. Moreover, REST APIs are more expensive and designed for full-featured API management use cases, making them less optimal for lightweight JWT validation scenarios compared to HTTP APIs.\n\nUse Amazon API Gateway WebSocket API with JWT claims validated by a Lambda authorizer - WebSocket APIs in API Gateway are designed for bidirectional communication, not stateless HTTP request-response models. While a Lambda authorizer can inspect tokens during the $connect phase, WebSocket APIs do not natively support JWT authorizers, and authorization is not enforced per message. This approach lacks the stateless, per-request token validation required by the startupâ€™s microservices. It also adds complexity and is not aligned with standard RESTful JWT use cases.\n\nDeploy a gRPC backend on Amazon ECS Fargate and expose it through AWS App Runner, handling JWT validation inside the containerized services - Deploying a gRPC backend on ECS Fargate and handling JWT validation within the container logic shifts the responsibility of authentication to the application layer, which increases complexity and violates the requirement for a fully managed JWT validation solution.\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-jwt-authorizer.html\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api-lambda-auth.html",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q483",
    "questionText": "A media streaming startup is building a set of backend APIs that will be consumed by external mobile applications. To prevent API abuse, protect downstream resources, and ensure fair usage across clients, the architecture must enforce rate limiting and throttling on a per-client basis. The team also wants to define usage quotas and apply different limits to different API consumers.\n\nWhich solution should the team implement to enforce rate limiting and usage quotas at the API layer?",
    "options": [
      {
        "text": "Use an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits",
        "isCorrect": false
      },
      {
        "text": "Use Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client",
        "isCorrect": true
      },
      {
        "text": "Use a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances",
        "isCorrect": false
      },
      {
        "text": "Use a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon API Gateway and configure usage plans with API keys to apply rate limits and quotas per client\n\nAmazon API Gateway natively supports rate limiting and throttling through the use of usage plans and API keys. By assigning API keys to clients and associating them with usage plans, developers can control the number of requests per second (rate) and the total number of requests over a period (quota). This enables fine-grained traffic control and ensures fair usage across clients, making it ideal for public-facing APIs. Throttling settings are enforced at the API Gateway layer, protecting downstream resources such as Lambda functions or backend services from overload.\n\nIncorrect options:\n\nUse an Application Load Balancer (ALB) with path-based routing and configure listener rules to enforce request limits - Application Load Balancers (ALBs) offer path-based and host-based routing, support WebSocket and HTTP/HTTPS traffic, and can perform request-level routing. However, ALBs do not provide built-in rate limiting or throttling capabilities. Any rate-limiting logic must be implemented at the backend or using WAF rules, which lack per-client usage plans or quota control. ALBs are optimized for load balancing, not API rate control.\n\nUse a Gateway Load Balancer to inspect and control incoming HTTP traffic and throttle requests by integrating with third-party firewall appliances - Gateway Load Balancers (GWLB) are designed to integrate with third-party security appliances, such as intrusion prevention systems or firewalls. While they can be used for traffic inspection and filtering, GWLBs do not provide application-layer rate limiting and are not intended for managing client usage quotas or request throttling. They operate at Layer 3/4 and are not suited for fine-grained API access control.\n\nUse a Network Load Balancer (NLB) to terminate TLS and apply rate-limiting logic within backend EC2 instances - Network Load Balancers (NLBs) are optimized for ultra-low latency and high-throughput TCP/UDP traffic, and they operate at Layer 4 (Transport Layer). NLBs do not have built-in capabilities to throttle or limit requests. Any rate-limiting would need to be implemented at the application level on the target EC2 instances, which defeats the purpose of centralized API rate limiting and adds unnecessary complexity to backend services.\n\nReferences:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/introduction.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q484",
    "questionText": "An e-commerce company runs its web application on Amazon EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an Amazon Simple Queue Service (Amazon SQS) queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received.\n\nAs a solutions architect, which of the following solutions would you recommend to address this use-case?",
    "options": [
      {
        "text": "Use a target tracking scaling policy based on a custom Amazon SQS queue metric",
        "isCorrect": true
      },
      {
        "text": "Use a simple scaling policy based on a custom Amazon SQS queue metric",
        "isCorrect": false
      },
      {
        "text": "Use a step scaling policy based on a custom Amazon SQS queue metric",
        "isCorrect": false
      },
      {
        "text": "Use a scheduled scaling policy based on a custom Amazon SQS queue metric",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse a target tracking scaling policy based on a custom Amazon SQS queue metric\n\nIf you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively. You may use an existing CloudWatch Amazon SQS metric like ApproximateNumberOfMessagesVisible for target tracking but you could still face an issue so that the number of messages in the queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. The solution is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain.\n\nTo calculate your backlog per instance, divide the ApproximateNumberOfMessages queue attribute by the number of instances in the InService state for the Auto Scaling group. Then set a target value for the Acceptable backlog per instance.\n\nTo illustrate with an example, let's say that the current ApproximateNumberOfMessages is 1500 and the fleet's running capacity is 10. If the average processing time is 0.1 seconds for each message and the longest acceptable latency is 10 seconds, then the acceptable backlog per instance is 10 / 0.1, which equals 100. This means that 100 is the target value for your target tracking policy. If the backlog per instance is currently at 150 (1500 / 10), your fleet scales out, and it scales out by five instances to maintain proportion to the target value.\n\nScaling Based on Amazon SQS:\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\n\nIncorrect options:\n\nUse a simple scaling policy based on a custom Amazon SQS queue metric - With simple scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process. The main issue with simple scaling is that after a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms. This implies that the application would not be able to react quickly to sudden spikes in orders.\n\nUse a step scaling policy based on a custom Amazon SQS queue metric - With step scaling, you choose scaling metrics and threshold values for the Amazon CloudWatch alarms that trigger the scaling process. When step adjustments are applied, they increase or decrease the current capacity of your Auto Scaling group, and the adjustments vary based on the size of the alarm breach. For the given use-case, step scaling would try to approximate the correct number of instances by increasing/decreasing the steps as per the policy. This is not as efficient as the target tracking policy where you can calculate the exact number of instances required to handle the spike in orders.\n\nUse a scheduled scaling policy based on a custom Amazon SQS queue metric - Scheduled scaling allows you to set your scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date. You cannot use scheduled scaling policies to address the sudden spike in orders.\n\nReference:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q485",
    "questionText": "The DevOps team at a multi-national company is helping its subsidiaries standardize Amazon EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project.\n\nWhich of the following would you identify as CORRECT regarding the capabilities of an Amazon Machine Image (AMI)? (Select three)",
    "options": [
      {
        "text": "You cannot copy an Amazon Machine Image (AMI) across AWS Regions",
        "isCorrect": false
      },
      {
        "text": "You cannot share an Amazon Machine Image (AMI) with another AWS account",
        "isCorrect": false
      },
      {
        "text": "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot",
        "isCorrect": false
      },
      {
        "text": "You can copy an Amazon Machine Image (AMI) across AWS Regions",
        "isCorrect": true
      },
      {
        "text": "You can share an Amazon Machine Image (AMI) with another AWS account",
        "isCorrect": true
      },
      {
        "text": "Copying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nYou can copy an Amazon Machine Image (AMI) across AWS Regions\n\nYou can share an Amazon Machine Image (AMI) with another AWS account\n\nCopying an Amazon Machine Image (AMI) backed by an encrypted snapshot cannot result in an unencrypted target snapshot\n\nAn Amazon Machine Image (AMI) provides the information required to launch an instance. An AMI includes the following:\n\nOne or more Amazon EBS snapshots, or, for instance-store-backed AMIs, a template for the root volume of the instance.\n\nLaunch permissions that control which AWS accounts can use the AMI to launch instances.\n\nA block device mapping that specifies the volumes to attach to the instance when it's launched.\n\nYou can copy an AMI within or across AWS Regions using the AWS Management Console, the AWS Command Line Interface or SDKs, or the Amazon EC2 API, all of which support the CopyImage action. You can copy both Amazon EBS-backed AMIs and instance-store-backed AMIs. You can copy AMIs with encrypted snapshots and also change encryption status during the copy process. Therefore, the option - \"You can copy an AMI across AWS Regions\" - is correct.\n\nCopying AMIs across regions:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\n\nThe following table shows encryption support for various AMI-copying scenarios. While it is possible to copy an unencrypted snapshot to yield an encrypted snapshot, you cannot copy an encrypted snapshot to yield an unencrypted one. Therefore, the option - \"Copying an AMI backed by an encrypted snapshot cannot result in an unencrypted target snapshot\" is correct.\n\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\n\nYou can share an AMI with another AWS account. To copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either the associated Amazon EBS snapshot (for an Amazon EBS-backed AMI) or an associated S3 bucket (for an instance store-backed AMI). Therefore, the option - \"You can share an AMI with another AWS account\" - is correct.\n\nIncorrect options:\n\nYou cannot copy an Amazon Machine Image (AMI) across AWS Regions\n\nYou cannot share an Amazon Machine Image (AMI) with another AWS account\n\nCopying an Amazon Machine Image (AMI) backed by an encrypted snapshot results in an unencrypted target snapshot\n\nThese three options contradict the details provided in the explanation above.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q486",
    "questionText": "A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)",
    "options": [
      {
        "text": "Amazon Elastic File System (Amazon EFS)",
        "isCorrect": false
      },
      {
        "text": "Amazon Elastic Block Store (Amazon EBS)",
        "isCorrect": false
      },
      {
        "text": "Amazon FSx for Windows File Server",
        "isCorrect": true
      },
      {
        "text": "Amazon Simple Storage Service (Amazon S3)",
        "isCorrect": false
      },
      {
        "text": "File Gateway Configuration of AWS Storage Gateway",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nAmazon FSx for Windows File Server\n\nAmazon FSx for Windows File Server is a fully managed, highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.\n\nFile Gateway Configuration of AWS Storage Gateway\n\nDepending on the use case, AWS Storage Gateway provides 3 types of storage interfaces for on-premises applications: File, Volume, and Tape. The File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as Network File System (NFS) and Server Message Block (SMB).\n\nIncorrect options:\n\nAmazon Elastic File System (Amazon EFS) - Amazon EFS is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics, and concurrently-accessible storage for up to thousands of Amazon EC2 instances. Amazon EFS uses the Network File System protocol. EFS does not support SMB protocol.\n\nAmazon Elastic Block Store (Amazon EBS) - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS does not support SMB protocol.\n\nAmazon Simple Storage Service (Amazon S3) - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 provides a simple, standards-based REST web services interface that is designed to work with any Internet-development toolkit. S3 does not support SMB protocol.\n\nReferences:\n\nhttps://aws.amazon.com/fsx/windows/\n\nhttps://aws.amazon.com/storagegateway/file/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q487",
    "questionText": "The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover Amazon EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise.\n\nAs a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)",
    "options": [
      {
        "text": "Terminated Amazon EC2 instances can be recovered if they are configured at the launch of instance",
        "isCorrect": false
      },
      {
        "text": "A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata",
        "isCorrect": true
      },
      {
        "text": "If your instance has a public IPv4 address, it retains the public IPv4 address after recovery",
        "isCorrect": true
      },
      {
        "text": "During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained",
        "isCorrect": false
      },
      {
        "text": "If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nA recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata\n\nIf your instance has a public IPv4 address, it retains the public IPv4 address after recovery\n\nYou can create an Amazon CloudWatch alarm to automatically recover the Amazon EC2 instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group. If your instance has a public IPv4 address, it retains the public IPv4 address after recovery. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost.\n\nIncorrect options:\n\nTerminated Amazon EC2 instances can be recovered if they are configured at the launch of instance - This is incorrect as terminated instances cannot be recovered.\n\nDuring instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained - As mentioned above, during instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost.\n\nIf your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery - As mentioned above, if your instance has a public IPv4 address, it retains the public IPv4 address after recovery.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q488",
    "questionText": "A regional transportation authority operates a high-traffic public information portal hosted on AWS. The backend consists of Amazon EC2 instances behind an Application Load Balancer (ALB). In recent weeks, the operations team has observed intermittent slowdowns and performance issues. After investigation, the team suspect the application is being targeted by distributed denial-of-service (DDoS) attacks coming from a wide range of IP addresses. The team needs a solution that provides DDoS mitigation, detailed logs for audit purposes, and requires minimal changes to the existing architecture.\n\nWhich solution best addresses these needs?",
    "options": [
      {
        "text": "Deploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups",
        "isCorrect": false
      },
      {
        "text": "Enable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly",
        "isCorrect": false
      },
      {
        "text": "Subscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events",
        "isCorrect": true
      },
      {
        "text": "Create an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nSubscribe to AWS Shield Advanced to gain proactive DDoS protection. Engage the AWS DDoS Response Team (DRT) to analyze traffic patterns and apply mitigations. Use the built-in logging and reporting to maintain an audit trail of detected events\n\nAWS Shield Advanced provides advanced, always-on DDoS protection for resources like ALBs, CloudFront, and Route 53. It includes detailed telemetry, real-time detection, automated application-layer DDoS mitigation, and most importantly, access to the AWS DDoS Response Team (DRT). The DRT can assist with rule creation and response during active events. Shield Advanced also supports detailed logging and event reports through AWS CloudWatch and AWS WAF logs, enabling complete auditing of DDoS attempts.\n\nIncorrect options:\n\nDeploy Amazon GuardDuty and integrate with the EC2 environment. Use GuardDuty findings to manually block suspected IP addresses at the ALB level or within EC2 security groups - While Amazon GuardDuty can detect anomalies and suspicious activity across your AWS environment, it is not designed for real-time DDoS mitigation. It identifies threats such as reconnaissance or credential exfiltration, not volume-based attacks. It also lacks native integration with ALB to automatically block high-volume attacks. Any mitigation must be manually scripted or reactive, which increases operational complexity.\n\nEnable Amazon Inspector for the EC2 instances. Use its vulnerability findings to detect potential DDoS attack vectors and patch the EC2 environments accordingly - Amazon Inspector focuses on vulnerability scanning and application security assessment for EC2 instances and container workloads. It does not detect or prevent network-layer attacks like DDoS. Although it helps improve overall security posture, it does nothing to address volumetric or bot-based attacks, making it irrelevant for this scenario.\n\nCreate an Amazon CloudFront distribution in front of the ALB. Enable AWS WAF on the distribution and configure custom rules to filter traffic from known malicious IP ranges and geographies. Use CloudFront access logs for analysis - Placing Amazon CloudFront in front of the ALB and using AWS WAF for traffic filtering can improve latency and offer some DDoS protection, especially at Layer 7. However, this introduces architectural changes that may require reworking DNS, caching logic, and content delivery behavior. It also does not offer access to the DDoS Response Team (DRT) or the automated event-based analytics provided by Shield Advanced. This option increases operational complexity and does not meet the goal of minimal architectural disruption.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/category/aws-shield/\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html\n\nhttps://docs.aws.amazon.com/guardduty/latest/ug/what-is-guardduty.html\n\nhttps://docs.aws.amazon.com/inspector/latest/user/what-is-inspector.html\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q489",
    "questionText": "An AWS Organization is using Service Control Policies (SCPs) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organizationâ€™s access control guidelines.\n\nWhich of the given scenarios are correct regarding the permissions described below? (Select three)",
    "options": [
      {
        "text": "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action",
        "isCorrect": true
      },
      {
        "text": "If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action",
        "isCorrect": false
      },
      {
        "text": "Service control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts",
        "isCorrect": true
      },
      {
        "text": "Service control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts",
        "isCorrect": false
      },
      {
        "text": "Service control policy (SCP) affects service-linked roles",
        "isCorrect": false
      },
      {
        "text": "Service control policy (SCP) does not affect service-linked role",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nIf a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action\n\nService control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts\n\nService control policy (SCP) does not affect service-linked role\n\nService control policy (SCP) are one type of policy that can be used to manage your organization. Service control policy (SCP) offers central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organizationâ€™s access control guidelines.\n\nIn service control policy (SCP), you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access. You can also define conditions for when to restrict access to AWS services, resources, and API actions. These restrictions even override the administrators of member accounts in the organization.\n\nPlease note the following effects on permissions vis-a-vis the service control policy (SCP):\n\nIf a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can't perform that action.\n\nService control policy (SCP) affects all users and roles in the member accounts, including root user of the member accounts.\n\nService control policy (SCP) does not affect any service-linked role.\n\nIncorrect options:\n\nIf a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable service control policy (SCP), the user or role can still perform that action\n\nService control policy (SCP) affects all users and roles in the member accounts, excluding root user of the member accounts\n\nService control policy (SCP) affects service-linked roles\n\nThese three options contradict the details provided in the explanation above.\n\nReference:\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
    "awsService": "IAM",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q490",
    "questionText": "The engineering team at a company is moving the static content from the company's logistics website hosted on Amazon EC2 instances to an Amazon S3 bucket. The team wants to use an Amazon CloudFront distribution to deliver the static content. The security group used by the Amazon EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post-migration to Amazon CloudFront, access to the static content should only be allowed from the aforementioned IP addresses.\n\nWhich options would you combine to build a solution to meet these requirements? (Select two)",
    "options": [
      {
        "text": "Create an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy",
        "isCorrect": false
      },
      {
        "text": "Create a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution",
        "isCorrect": false
      },
      {
        "text": "Create a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution",
        "isCorrect": false
      },
      {
        "text": "Configure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects",
        "isCorrect": true
      },
      {
        "text": "Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nConfigure an origin access identity (OAI) and associate it with the Amazon CloudFront distribution. Set up the permissions in the Amazon S3 bucket policy so that only the OAI can read the objects\n\nWhen you use Amazon CloudFront with an Amazon S3 bucket as the origin, you can configure Amazon CloudFront and Amazon S3 in a way that provides the following benefits:\n\nRestricts access to the Amazon S3 bucket so that it's not publicly accessible\n\nMakes sure that viewers (users) can access the content in the bucket only through the specified Amazon CloudFront distributionâ€”that is, prevents them from accessing the content directly from the bucket, or through an unintended CloudFront distribution.\n\nTo do this, configure Amazon CloudFront to send authenticated requests to Amazon S3, and configure Amazon S3 to only allow access to authenticated requests from Amazon CloudFront. Amazon CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI).\n\nExam Alert:\n\nPlease note that AWS recommends using OAC because it supports:\n\nAll Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022\n\nAmazon S3 server-side encryption with AWS KMS (SSE-KMS)\n\nDynamic requests (POST, PUT, etc.) to Amazon S3\n\nOAI doesn't work for the scenarios in the preceding list, or it requires extra workarounds in those scenarios. However, you will continue to see answers enlisting OAI as the preferred option in the actual exam as it takes about 6 months/1 year for a new feature to appear in the exam.\n\nCreate an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon CloudFront distribution\n\nAWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to your protected web application resources. You can protect the following resource types:\n\nAmazon CloudFront distribution\n\nAmazon API Gateway REST API\n\nApplication Load Balancer\n\nAWS AppSync GraphQL API\n\nAmazon Cognito user pool\n\nAWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response.\n\nIf you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions via your AWS WAF. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from.\n\nFor the given use case, you should add those IP addresses that are allowed in the Amazon EC2 security group into the IP match condition.\n\nIncorrect options:\n\nCreate an AWS Web Application Firewall (AWS WAF) ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the Amazon EC2 security group. Associate this new AWS WAF ACL with the Amazon S3 bucket policy - You cannot associate an AWS WAF ACL with an Amazon S3 bucket policy.\n\nCreate a new NACL that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new NACL with the Amazon CloudFront distribution - NACL is associated with a subnet within a VPC. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a NACL cannot be associated with a Amazon CloudFront distribution.\n\nCreate a new security group that allows traffic from the same IPs as specified in the current Amazon EC2 security group. Associate this new security group with the Amazon CloudFront distribution - A security group acts as a virtual firewall for your Amazon EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. Amazon CloudFront delivers your content through a worldwide network of data centers called edge locations. So a security group cannot be associated with Amazon CloudFront distribution.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q491",
    "questionText": "A geospatial analytics firm recently completed a large-scale seafloor imaging project and used AWS Snowball Edge Storage Optimized devices to collect and transfer over 150 TB of raw sonar data. The firm uses a high-performance computing (HPC) cluster on AWS to process and analyze massive datasets to identify natural resource formations. The processing workloads require sub-millisecond latency, high-throughput, fast and parallel file access to the imported dataset once the Snowball Edge devices are returned to AWS and the data is ingested.\n\nWhich solution best meets these requirements?",
    "options": [
      {
        "text": "Create an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data",
        "isCorrect": true
      },
      {
        "text": "Import the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency",
        "isCorrect": false
      },
      {
        "text": "Import the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access",
        "isCorrect": false
      },
      {
        "text": "Copy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an Amazon FSx for Lustre file system and import the 150 TB of data directly into the Lustre file system. Mount the FSx file system on the HPC cluster instances to enable low-latency, high-throughput access to the data\n\nThis solution is purpose-built for high-performance computing (HPC) use cases and is the best choice when consistent sub-millisecond latency and high-throughput access to large datasets are required. Amazon FSx for Lustre is a fully managed, POSIX-compliant file system designed to support data-intensive workloads like seismic analysis, machine learning, financial modeling, and genomics.\n\nIn this scenario, the company is returning AWS Snowball Edge Storage Optimized devices, which contain 150 TB of raw ocean survey data. The data is imported directly into FSx for Lustre during the Snowball data ingestion process.  Once the data is in the FSx for Lustre file system, it can be mounted across all HPC cluster nodes using standard Linux mount commands over NFS or Lustre clients, enabling shared, parallel access at massive scale. FSx for Lustre offers hundreds of GBps of throughput, millions of IOPS, and sub-millisecond latencies, meeting the performance demands of the compute-intensive analytics workloads used to identify oil and gas formations.\n\nAdditionally, FSx for Lustre scales automatically with demand, integrates with AWS Identity and Access Management (IAM) for security, and removes the operational burden of provisioning and tuning file systems.\n\nIncorrect options:\n\nImport the data into an Amazon S3 bucket. Create an Amazon FSx for Lustre file system, and link it to the S3 bucket. Mount the FSx for Lustre file system on the HPC nodes to access the data with high throughput and low latency - While Amazon FSx for Lustre integrated with Amazon S3 is a powerful solution for many high-throughput use cases, it involves an additional integration step where the Lustre file system imports metadata from the linked S3 bucket. The actual data transfer from S3 to Lustre occurs on demand, meaning the first access to each file may experience slightly higher latency compared to data that is preloaded or natively present on the file system. This approach is excellent when working with existing S3-based data lakes, but in this scenario, the data is coming directly from Snowball Edge, and uploading it to S3 introduces an unnecessary intermediate step. Instead, importing the data directly into FSx for Lustre, without routing through S3, avoids this overhead and provides immediate, consistent sub-millisecond latency required for HPC workloads.\n\nImport the data into Amazon S3. Set up an Amazon FSx for NetApp ONTAP file system and configure the FSx volume to sync with the S3 bucket. Mount the FSx volume on the HPC nodes for shared access - While FSx for NetApp ONTAP is a powerful, POSIX-compatible shared file system, it is better suited for enterprise applications, file sharing, and backups rather than HPC workloads. It supports tiering to S3 but does not offer the extremely high throughput or parallel access patterns needed for HPC. FSx for Lustre is specifically optimized for this use case.\n\nCopy the data into Amazon S3. Transfer the contents to an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the HPC cluster nodes to access the data in parallel - Amazon EFS is designed for general-purpose workloads such as web apps, analytics dashboards, and home directories, not for extremely performance-sensitive HPC applications. Its throughput and latency profile cannot match FSx for Lustre, and the additional manual data transfer from S3 to EFS introduces operational complexity and delay.\n\nReferences:\n\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html\n\nhttps://aws.amazon.com/blogs/storage/unlock-higher-performance-for-file-system-workloads-with-scalable-metadata-performance-on-amazon-fsx-for-lustre/\n\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q492",
    "questionText": "An engineering lead is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates.\n\nWhich of the following options represents the correct solution to set up internet access for the private subnets?",
    "options": [
      {
        "text": "Set up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ",
        "isCorrect": false
      },
      {
        "text": "Set up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ",
        "isCorrect": false
      },
      {
        "text": "Set up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ",
        "isCorrect": true
      },
      {
        "text": "Set up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up three NAT gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ\n\nYou can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances.\n\nTo create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed after you associate it with the NAT Gateway. After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet.\n\nEach NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone.\n\nIf you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gatewayâ€™s Availability Zone is down, resources in the other Availability Zones lose internet access. To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.\n\nHow NAT gateway works:\n\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nIncorrect options:\n\nSet up three NAT gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the NAT gateway in its AZ - NAT gateways need to be set up in public subnets, so this option is incorrect.\n\nSet up three Internet gateways, one in each private subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the Internet gateway in its AZ - Internet gateways cannot be provisioned in private subnets of a VPC.\n\nSet up three egress-only internet gateways, one in each public subnet in each AZ. Create a custom route table for each AZ that forwards non-local traffic to the egress-only internet gateway in its AZ - An Egress-only Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating an IPv6 connection with your instances. The given use-case is for IPv4 traffic, hence an Egress-only Internet gateway is not an option.\n\nReference:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q493",
    "questionText": "A company recently experienced a database outage in its on-premises data center. The company now wants to migrate to a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes.\n\nWhich of the following solutions meets these requirements?",
    "options": [
      {
        "text": "Set up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data",
        "isCorrect": true
      },
      {
        "text": "Set up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data\n\nWhen you provision an RDS Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption. In the event of a planned or unplanned outage of your DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if you have enabled Multi-AZ. The time it takes for the failover to complete depends on the database activity and other conditions at the time the primary DB instance became unavailable. Failover times are typically 60â€“120 seconds.\n\n\nvia - https://aws.amazon.com/rds/features/multi-az/\n\nIncorrect options:\n\nSet up an Amazon RDS MySQL DB instance and then create a read replica in another Availability Zone that synchronously replicates the data\n\nSet up an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data\n\nAmazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica. Using read replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\n\nBoth these options talk about creating a read replica that synchronously replicates the data, but in reality, any updates made to the primary DB instance are asynchronously copied to the read replica. So both these options are incorrect.\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\nSet up an Amazon EC2 instance with a MySQL DB engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance - Setting up a database on an Amazon EC2 instance would not be reliable as you would have to monitor and manage the underlying Amazon EC2 instance for any issues or outages. In addition, using AWS Lambda to replicate the data from EC2 based MySQL DB to an Amazon RDS MySQL DB would make the solution really complex since the same functionality can be achieved out-of-the-box using RDS Multi-AZ configuration.\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/multi-az/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q494",
    "questionText": "Your application is hosted by a provider on yourapp.provider.com. You would like to have your users access your application using www.your-domain.com, which you own and manage under Amazon Route 53.\n\nWhich Amazon Route 53 record should you create?",
    "options": [
      {
        "text": "Create an A record",
        "isCorrect": false
      },
      {
        "text": "Create a CNAME record",
        "isCorrect": true
      },
      {
        "text": "Create a PTR record",
        "isCorrect": false
      },
      {
        "text": "Create an Alias Record",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a CNAME record\n\nA CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).\n\nCNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.\n\nPlease review the major differences between CNAME and Alias Records:\n\nvia - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\n\nIncorrect options:\n\nCreate an A record - Used to point a domain or subdomain to an IP address. 'A record' cannot be used to map one domain name to another.\n\nCreate a PTR record - A Pointer (PTR) record resolves an IP address to a fully-qualified domain name (FQDN) as an opposite to what A record does. PTR records are also called Reverse DNS records. 'PTR record' cannot be used to map one domain name to another.\n\nCreate an Alias Record - Alias records let you route traffic to selected AWS resources, such as Amazon CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. 3rd party websites do not qualify for these as we have no control over those. 'Alias record' cannot be used to map one domain name to another.\n\nReference:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html",
    "awsService": "Route 53",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q495",
    "questionText": "The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group.\n\nAs a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?",
    "options": [
      {
        "text": "You can use a security group as the custom source for the inbound rule",
        "isCorrect": false
      },
      {
        "text": "You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule",
        "isCorrect": false
      },
      {
        "text": "You can use an IP address as the custom source for the inbound rule",
        "isCorrect": false
      },
      {
        "text": "You can use an Internet Gateway ID as the custom source for the inbound rule",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nYou can use an Internet Gateway ID as the custom source for the inbound rule\n\nA security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, you can use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.\n\nPlease see this list of allowed source or destination for security group rules:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\n\nTherefore, you cannot use an Internet Gateway ID as the custom source for the inbound rule.\n\nIncorrect options:\n\nYou can use a security group as the custom source for the inbound rule\n\nYou can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule\n\nYou can use an IP address as the custom source for the inbound rule\n\nAs described in the list of allowed sources or destinations for security group rules, the above options are supported.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
    "awsService": "General",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q496",
    "questionText": "Your company is deploying a website running on AWS Elastic Beanstalk. The website takes over 45 minutes for the installation and contains both static as well as dynamic files that must be generated during the installation process.\n\nAs a Solutions Architect, you would like to bring the time to create a new instance in your AWS Elastic Beanstalk deployment to be less than 2 minutes. Which of the following options should be combined to build a solution for this requirement? (Select two)",
    "options": [
      {
        "text": "Use Amazon EC2 user data to customize the dynamic installation parts at boot time",
        "isCorrect": true
      },
      {
        "text": "Create a Golden Amazon Machine Image (AMI) with the static installation components already setup",
        "isCorrect": true
      },
      {
        "text": "Store the installation files in Amazon S3 so they can be quickly retrieved",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 user data to install the application at boot time",
        "isCorrect": false
      },
      {
        "text": "Use AWS Elastic Beanstalk deployment caching feature",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\n\nYou can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.\n\nWhen you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn't included in the standard AMIs.\n\nCreate a Golden Amazon Machine Image (AMI) with the static installation components already setup\n\nA Golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can have the static installation components already setup via the golden AMI.\n\nUse Amazon EC2 user data to customize the dynamic installation parts at boot time\n\nAmazon EC2 instance user data is the data that you specified in the form of a configuration script while launching your instance. You can use Amazon EC2 user data to customize the dynamic installation parts at boot time, rather than installing the application itself at boot time.\n\nIncorrect options:\n\nStore the installation files in Amazon S3 so they can be quickly retrieved - Amazon S3 bucket can be used as a storage location for your source code, logs, and other artifacts that are created when you use AWS Elastic Beanstalk. It cannot be used to run or generate dynamic files since Amazon S3 is not an environment but a storage service.\n\nUse Amazon EC2 user data to install the application at boot time - User data of an instance can be used to perform common automated configuration tasks or run scripts after the instance starts. User data, cannot, however, be used to install the application since it takes over 45 minutes for the installation which contains static as well as dynamic files that must be generated during the installation process.\n\nUse AWS Elastic Beanstalk deployment caching feature - AWS Elastic Beanstalk deployment caching is a made-up option. It is just added as a distractor.\n\nReferences:\n\nhttps://aws.amazon.com/elasticbeanstalk/\n\nhttps://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.S3.html\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q497",
    "questionText": "A digital media platform is preparing to launch a new interactive content service that is expected to receive sudden spikes in user engagement, especially during live events and media releases. The backend uses an Amazon Aurora PostgreSQL Serverless v2 cluster to handle dynamic workloads. The architecture must be capable of scaling both compute and storage performance to maintain low latency and avoid bottlenecks under load. The engineering team is evaluating storage configuration options and wants a solution that will scale automatically with traffic, optimize I/O performance, and remain cost-effective without manual provisioning or tuning.\n\nWhich configuration will best meet these requirements?",
    "options": [
      {
        "text": "Configure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks",
        "isCorrect": false
      },
      {
        "text": "Configure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges",
        "isCorrect": true
      },
      {
        "text": "Select Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage",
        "isCorrect": false
      },
      {
        "text": "Configure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Auroraâ€™s autoscaling to handle demand spikes",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure the Aurora cluster to use Aurora I/O-Optimized storage. This configuration delivers high throughput and low-latency I/O performance with predictable pricing and no I/O-based charges\n\nAurora I/O-Optimized is a purpose-built storage configuration for Aurora that eliminates I/O-based pricing and provides consistent high-throughput and low-latency performance, especially under high workloads. It's ideal for use cases with frequent read/write operations, such as media platforms and real-time applications. With I/O-Optimized, customers pay a flat storage rate and benefit from enhanced performance without needing to manage or provision IOPS manually. It's designed to be both cost-effective and performant in I/O-heavy environments.\n\nIncorrect options:\n\nConfigure the Aurora cluster to use General Purpose SSD (gp2) storage. Increase performance by scaling database compute capacity to reduce IOPS bottlenecks - While General Purpose (gp2) SSD storage can provide reasonable baseline performance, it uses a burst-bucket model for IOPS that is not ideal under sustained heavy load. It cannot scale IOPS independently of storage size, and relying on compute scaling to compensate for storage I/O limitations can be inefficient and more expensive.\n\nSelect Provisioned IOPS (io1) as the storage type for the Aurora cluster. Manually adjust IOPS based on expected traffic during peak usage - Provisioned IOPS allows precise tuning of IOPS levels, but it is not supported directly as a configurable storage type in Aurora. Instead, Aurora abstracts much of the IOPS provisioning logic. Even if it were supported, manually adjusting IOPS would increase operational complexity and cost more compared to Auroraâ€™s I/O-Optimized option.\n\nConfigure the cluster with Magnetic (Standard) storage to minimize baseline storage costs and rely on Auroraâ€™s autoscaling to handle demand spikes - Magnetic storage is a legacy, low-cost option that is not suitable for high-performance production workloads. It suffers from high latency and limited throughput, making it a poor choice for applications with spiky or sustained I/O demand. Moreover, magnetic storage is not supported in newer Aurora configurations.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/new-amazon-aurora-i-o-optimized-cluster-configuration-with-up-to-40-cost-savings-for-i-o-intensive-applications/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html",
    "awsService": "Auto Scaling",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q498",
    "questionText": "A company has migrated its application from a monolith architecture to a microservices based architecture. The development team has updated the Amazon Route 53 simple record to point \"myapp.mydomain.com\" from the old Load Balancer to the new one.\n\nThe users are still not redirected to the new Load Balancer. What has gone wrong in the configuration?",
    "options": [
      {
        "text": "The Time To Live (TTL) is still in effect",
        "isCorrect": true
      },
      {
        "text": "The health checks are failing",
        "isCorrect": false
      },
      {
        "text": "The Alias Record is misconfigured",
        "isCorrect": false
      },
      {
        "text": "The CNAME Record is misconfigured",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS â€“ such as Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets â€“ and can also be used to route users to infrastructure outside of AWS.\n\nYou can use Amazon Route 53 to configure DNS health checks to route traffic to healthy endpoints or to independently monitor the health of your application and its endpoints. Amazon Route 53 Traffic Flow makes it easy for you to manage traffic globally through a variety of routing types, including Latency Based Routing, Geo DNS, Geoproximity, and Weighted Round Robinâ€”all of which can be combined with DNS Failover to enable a variety of low-latency, fault-tolerant architectures.\n\nThe Time To Live (TTL) is still in effect\n\nTTL (time to live), is the amount of time, in seconds, that you want DNS recursive resolvers to cache information about a record. If you specify a longer value (for example, 172800 seconds, or two days), you reduce the number of calls that DNS recursive resolvers must make to Amazon Route 53 to get the latest information for the record. This has the effect of reducing latency and reducing your bill for Route 53 service.\n\nHowever, if you specify a longer value for TTL, it takes longer for changes to the record (for example, a new IP address) to take effect because recursive resolvers use the values in their cache for longer periods before they ask Route 53 for the latest information. If you're changing settings for a domain or subdomain that's already in use, AWS recommends that you initially specify a shorter value, such as 300 seconds, and increase the value after you confirm that the new settings are correct.\n\nFor this use-case, the most likely issue is that the TTL is still in effect so you have to wait until it expires for the new request to perform another DNS query and get the value for the new Load Balancer.\n\nIncorrect options:\n\nThe CNAME Record is misconfigured - A CNAME record can redirect DNS queries to any DNS record. For example, you can create a CNAME record that redirects queries from acme.example.com to zenith.example.com or to acme.example.org. You don't need to use Amazon Route 53 as the DNS service for the domain that you're redirecting queries to.\n\nThe Alias Record is misconfigured - Amazon Route 53 also offers alias records, which are an Amazon Route 53-specific extension to DNS. Alias records let you route traffic to selected AWS resources, such as Amazon CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record.\nUnlike a CNAME record, you can create an alias record at the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You can't create a CNAME record for example.com, but you can create an alias record for example.com that routes traffic to www.example.com.\n\nThe health checks are failing - Simple Records do not have health checks, so this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/route53/\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-basic.html\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html",
    "awsService": "Route 53",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q499",
    "questionText": "A global insurance company is modernizing its infrastructure by migrating multiple line-of-business applications from its on-premises data centers to AWS. These applications will be deployed across several AWS accounts, all governed under a centralized AWS Organizations structure. The company manages all user identities, groups, and access policies within its on-premises Microsoft Active Directory and wants to continue doing so. The goal is to enable seamless single sign-in across all AWS accounts without duplicating user identity stores or manually provisioning accounts.\n\nWhich solution best meets these requirements in the most operationally efficient manner?",
    "options": [
      {
        "text": "Deploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts",
        "isCorrect": true
      },
      {
        "text": "Enable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies",
        "isCorrect": false
      },
      {
        "text": "Deploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDeploy AWS IAM Identity Center and configure it to use AWS Directory Service for Microsoft Active Directory (Enterprise Edition). Establish a two-way trust relationship between the managed directory and the on-premises Active Directory to enable federated authentication across all AWS accounts\n\nThis solution meets all key requirements: centralized SSO across multiple AWS accounts, integration with AWS Organizations, and continued use of the on-premises Active Directory. AWS Directory Service for Microsoft Active Directory (Enterprise Edition) supports two-way forest trust relationships with the companyâ€™s self-managed Active Directory. Once the trust is established, AWS IAM Identity Center can be configured to use the AWS Managed AD as its identity source, allowing federated login with on-prem AD users and groups. IAM Identity Center then centrally manages SSO access and permission sets across all AWS accountsâ€”without duplicating identities.\n\nHow AWS IAM Identity Center Active Directory sync enhances AWS application experiences:\n\nvia - https://aws.amazon.com/blogs/security/how-aws-sso-active-directory-sync-enhances-aws-application-experiences/\n\nIncorrect options:\n\nEnable AWS IAM Identity Center and manually create user accounts and groups within it. Assign these users permission sets in each AWS account. Manage synchronization with on-premises Active Directory using custom PowerShell scripts - Manually creating and managing users within IAM Identity Center introduces operational burden and violates the requirement to continue managing users in on-prem AD. PowerShell scripts are not a supported or scalable way to synchronize identity sources. IAM Identity Center is designed to integrate with external identity providers, not replace one entirely via scripting.\n\nUse Amazon Cognito as the primary identity store and create a custom OpenID Connect (OIDC) federation with the on-premises Active Directory. Assign IAM roles using Cognito identity pools and propagate access to multiple AWS accounts using resource policies - Amazon Cognito is optimized for web and mobile app authentication, not enterprise SSO across multiple AWS accounts. While OIDC federation can be used to connect to Active Directory via intermediate identity providers (e.g., Azure AD), Cognito does not natively integrate with IAM Identity Center or manage account-level SSO access across AWS Organizations. This approach is not suitable for centralized AWS console access control.\n\nDeploy an OpenLDAP server on Amazon EC2, sync it with the on-premises Active Directory, and integrate it with each AWS account by creating IAM roles that trust the EC2-hosted LDAP server as a SAML provider - Deploying and managing a self-hosted OpenLDAP directory on EC2 increases operational complexity, security risks, and does not natively support integration with AWS IAM Identity Center. Additionally, IAM roles in AWS require a trusted SAML identity provider, and running a SAML IdP on EC2 means managing high-availability, patching, and scaling manually. This is contrary to the low-maintenance goals of the company.\n\nReferences:\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-identity-source-ad.html\n\nhttps://aws.amazon.com/blogs/security/how-aws-sso-active-directory-sync-enhances-aws-application-experiences/\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-identity-source.html\n\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q500",
    "questionText": "The engineering team at a global e-commerce company is currently reviewing their disaster recovery strategy. The team has outlined that they need to be able to quickly recover their application stack with a Recovery Time Objective (RTO) of 5 minutes, in all of the AWS Regions that the application runs. The application stack currently takes over 45 minutes to install on a Linux system.\n\nAs a Solutions architect, which of the following options would you recommend as the disaster recovery strategy?",
    "options": [
      {
        "text": "Store the installation files in Amazon S3 for quicker retrieval",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 user data to speed up the installation process",
        "isCorrect": false
      },
      {
        "text": "Create an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions",
        "isCorrect": false
      },
      {
        "text": "Create an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nCreate an Amazon Machine Image (AMI) after installing the software and copy the AMI across all Regions. Use this Region-specific AMI to run the recovery process in the respective Regions\n\nAn Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need instances with different configurations.\n\nFor the current use case, you need to create an AMI such that the application stack is already set up. But AMIs are bound to the Region they are created in. So, you need to copy the AMI across Regions for disaster recovery readiness.\n\nCopying a source AMI results in an identical but distinct target AMI with its own unique identifier. In the case of an Amazon EBS-backed AMI, each of its backing snapshots is, by default, copied to an identical but distinct target snapshot. (The sole exceptions are when you choose to encrypt or re-encrypt the snapshot.) You can change or deregister the source AMI with no effect on the target AMI. The reverse is also true.\nThere are no charges for copying an AMI. However, standard storage and data transfer rates apply. If you copy an Amazon EBS-backed AMI, you will incur charges for the storage of any additional Amazon EBS snapshots.\n\nAWS does not copy launch permissions, user-defined tags, or Amazon S3 bucket permissions from the source AMI to the new AMI. After the copy operation is complete, you can apply launch permissions, user-defined tags, and Amazon S3 bucket permissions to the new AMI.\n\nAMIs Cross-Region copying:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\n\nIncorrect options:\n\nStore the installation files in Amazon S3 for quicker retrieval - Amazon Simple Storage Service (Amazon S3) is an object storage service from AWS. It will not help with speeding up the installation since Amazon S3 is a storage service. You will still need an Amazon EC2 instance to have the necessary installation environment.\n\nUse Amazon EC2 user data to speed up the installation process - User data of an Amazon EC2 instance can be used to perform common automated configuration tasks or run scripts after the instance starts. User data, cannot, however, be used to install the application. Amazon EC2 user data would not help as it would run the same installation script for the same duration of 45 minutes.\n\nCreate an Amazon Machine Image (AMI) after installing the software and use this AMI to run the recovery process in other Regions - As discussed above, AMIs are Region-specific and need to be copied to all Regions they are intended to be used in.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q501",
    "questionText": "An e-commerce analytics company is preparing to archive several years of transaction records and customer analytics reports in Amazon S3 for long-term storage. To meet compliance requirements, the archived data must be encrypted at rest using AWS-managed encryption keys. Additionally, the solution must be cost-effective and ensure that key rotation occurs automatically every 12 months to comply with the companyâ€™s internal data governance policy.\n\nWhich solution will meet these requirements with the least operational overhead?",
    "options": [
      {
        "text": "Use Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3",
        "isCorrect": false
      },
      {
        "text": "Encrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings",
        "isCorrect": false
      },
      {
        "text": "Use AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow",
        "isCorrect": false
      },
      {
        "text": "Use AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucketâ€™s default encryption to use the customer managed key. Migrate the data to the S3 bucket",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Key Management Service (KMS) to create a customer managed key with automatic rotation enabled. Configure the S3 bucketâ€™s default encryption to use the customer managed key. Migrate the data to the S3 bucket\n\nThis solution provides the lowest operational overhead while meeting both encryption and automatic annual key rotation requirements. By creating a customer managed KMS key and enabling automatic key rotation, AWS KMS will handle rotation every year without manual action. Configuring the S3 bucket's default encryption to use this KMS key ensures that all new objects are automatically encrypted at rest, regardless of upload method. This is a fully managed, secure, and compliant approach.\n\n\nvia - https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\n\nIncorrect options:\n\nUse Amazon S3 server-side encryption with S3-managed keys (SSE-S3). Upload data with default encryption enabled. Rely on the built-in key management and rotation behavior of SSE-S3 - While SSE-S3 provides at-rest encryption with no additional configuration, it uses Amazon-managed keys, and key rotation is not user-configurable or trackable. SSE-S3 keys are rotated irregularly behind the scenes, which does not satisfy compliance requirements that call for annual, auditable key rotation.\n\nEncrypt the data locally using client-side encryption libraries and upload the encrypted files to S3. Create a KMS key with imported key material and configure key rotation settings - This option adds unnecessary operational complexity. When using imported key material in KMS, automatic rotation is disabledâ€”the customer is responsible for manually rotating and re-importing the key material. This defeats the goal of minimizing operational overhead and complicates compliance validation.\n\nUse AWS CloudHSM to generate encryption keys. Configure S3 to use these custom encryption keys via client-side encryption and rotate the keys annually using an on-premises key management workflow - AWS CloudHSM is designed for organizations with strict hardware key control requirements, but it requires manual infrastructure management, including key generation, backup, and lifecycle rotation. It also introduces complexity when combined with client-side encryption, making this the highest-maintenance optionâ€”unsuitable for a team prioritizing cost-efficiency and automation.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html\n\nhttps://docs.aws.amazon.com/cloudhsm/latest/userguide/introduction.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q502",
    "questionText": "A global pharmaceutical company operates a hybrid cloud network. Its primary AWS workloads run in the us-west-2 Region, connected to its on-premises data center via an AWS Direct Connect connection. After acquiring a biotech firm headquartered in Europe, the company must integrate the biotechâ€™s workloads, which are hosted in several VPCs in the eu-central-1 Region and connected to the biotech's on-premises facility through a separate Direct Connect link. All CIDR blocks are non-overlapping, and the business requires full connectivity between both data centers and all VPCs across the two Regions. The company also wants a scalable solution that minimizes manual network configuration and long-term operational overhead.\n\nWhich solution will best meet these requirements?",
    "options": [
      {
        "text": "Establish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication",
        "isCorrect": false
      },
      {
        "text": "Connect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions",
        "isCorrect": true
      },
      {
        "text": "Deploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions",
        "isCorrect": false
      },
      {
        "text": "Create private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConnect both Direct Connect links to a shared Direct Connect gateway. Attach each Region's virtual private gateway (VGW) to the Direct Connect gateway, enabling transitive routing between the VPCs and the on-premises networks across Regions\n\nThis is the best solution for interconnecting multiple VPCs across Regions and between on-premises data centers, while maintaining low operational overhead. A Direct Connect gateway is a global resource that allows VPCs in any AWS Region (except China) to connect to Direct Connect via virtual private gateways (VGWs). By connecting both Direct Connect links to the same DX gateway and associating the VGWs of all relevant VPCs, the company can enable transitive routing across Regions and between on-premises locations and VPCs â€” without setting up complex peering or custom VPN appliances.\n\n\nvia - https://aws.amazon.com/blogs/networking-and-content-delivery/hybrid-cloud-architectures-using-aws-direct-connect-gateway/\n\nIncorrect options:\n\nEstablish inter-Region VPC peering between each VPC in the us-west-2 and eu-central-1 Regions. Use static routing tables in each VPC to define peer relationships and enable cross-Region communication - While inter-Region VPC peering does support direct communication between two VPCs, it does not provide transitive routing. That means each VPC must be peered individually, and on-prem connectivity cannot be transited through VPCs. Additionally, managing dozens of peering connections and route tables is not scalable as the number of VPCs grows.\n\nDeploy EC2-based VPN appliances in each VPC. Configure a full mesh VPN topology between all VPCs and data centers using CloudHub-style routing across Regions - While possible, deploying a full mesh VPN using EC2 instances or third-party VPN appliances introduces high operational complexity, including manual configuration, key management, and failure handling. CloudHub is only recommended when you have Site-to-Site VPNs connected to a single virtual private gateway, which is not scalable or efficient across multiple VPCs and Regions.\n\nCreate private VIFs (virtual interfaces) in each Region and associate them directly with foreign-region VPCs using routing table entries and BGP. Use VPC endpoints in each account to forward cross-Region traffic - Private VIFs are used to connect a Direct Connect to a VGW or a transit gateway in the same Region. They cannot be directly associated with a foreign-region VPC. Furthermore, VPC endpoints are for accessing AWS services within a Region and cannot route private traffic between Regions. This design does not enable global hybrid connectivity.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/hybrid-cloud-architectures-using-aws-direct-connect-gateway/\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q503",
    "questionText": "A junior developer has downloaded a sample Amazon S3 bucket policy to make changes to it based on new company-wide access policies. He has requested your help in understanding this bucket policy.\n\nAs a Solutions Architect, which of the following would you identify as the correct description for the given policy?\n\n{\n \"Version\": \"2012-10-17\",\n \"Id\": \"S3PolicyId1\",\n \"Statement\": [\n   {\n     \"Sid\": \"IPAllow\",\n     \"Effect\": \"Allow\",\n     \"Principal\": \"*\",\n     \"Action\": \"s3:*\",\n     \"Resource\": \"arn:aws:s3:::examplebucket/*\",\n     \"Condition\": {\n        \"IpAddress\": {\"aws:SourceIp\": \"54.240.143.0/24\"},\n        \"NotIpAddress\": {\"aws:SourceIp\": \"54.240.143.188/32\"}\n     }\n   }\n ]\n}",
    "options": [
      {
        "text": "It ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP",
        "isCorrect": false
      },
      {
        "text": "It authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket",
        "isCorrect": true
      },
      {
        "text": "It ensures Amazon EC2 instances that have inherited a security group can access the bucket",
        "isCorrect": false
      },
      {
        "text": "It authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nIt authorizes an entire Classless Inter-Domain Routing (CIDR) except one IP address to access the Amazon S3 bucket\n\nYou manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations SCPs, ACLs, and session policies.\n\nLet's analyze the bucket policy one step at a time:\n\nThe snippet \"Effect\": \"Allow\" implies an allow effect.\nThe snippet \"Principal\": \"*\" implies any Principal.\nThe snippet \"Action\": \"s3:*\" implies any Amazon S3 API.\nThe snippet \"Resource\": \"arn:aws:s3:::examplebucket/*\" implies that the resource can be the bucket examplebucket and its contents.\nConsider the last snippet of the given bucket policy:\n\"Condition\": {\n         \"IpAddress\": {\"aws:SourceIp\": \"54.240.143.0/24\"},\n         \"NotIpAddress\": {\"aws:SourceIp\": \"54.240.143.188/32\"}\n      }\nThis snippet implies that if the source IP is in the CIDR block \"54.240.143.0/24\" (== 54.240.143.0 - 54.240.143.255), then it is allowed to access the examplebucket and its contents.\nHowever, the source IP cannot be in the CIDR \"54.240.143.188/32\" (== 1 IP, 54.240.143.188/32), which means one IP address is explicitly blocked from accessing the examplebucket and its contents.\n\nExample Bucket policies:\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html\n\nIncorrect options:\n\nIt ensures the Amazon S3 bucket is exposing an external IP within the Classless Inter-Domain Routing (CIDR) range specified, except one IP\n\nIt ensures Amazon EC2 instances that have inherited a security group can access the bucket\n\nIt authorizes an IP address and a Classless Inter-Domain Routing (CIDR) to access the S3 bucket\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q504",
    "questionText": "You are working for a software as a service (SaaS) company as a solutions architect and help design solutions for the company's customers. One of the customers is a bank and has a requirement to whitelist a public IP when the bank is accessing external services across the internet.\n\nWhich architectural choice do you recommend to maintain high availability, support scaling-up to 10 instances and comply with the bank's requirements?",
    "options": [
      {
        "text": "Use a Network Load Balancer with an Auto Scaling Group",
        "isCorrect": true
      },
      {
        "text": "Use a Classic Load Balancer with an Auto Scaling Group",
        "isCorrect": false
      },
      {
        "text": "Use an Application Load Balancer with an Auto Scaling Group",
        "isCorrect": false
      },
      {
        "text": "Use an Auto Scaling Group with Dynamic Elastic IPs attachment",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse a Network Load Balancer with an Auto Scaling Group\n\nNetwork Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers â€“ within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second.\n\nNetwork Load Balancers expose a fixed IP to the public web, therefore allowing your application to be predictably reached using this IP, while allowing you to scale your application behind the Network Load Balancer using an ASG.\n\nIncorrect options:\n\nClassic Load Balancers and Application Load Balancers use the private IP addresses associated with their Elastic network interfaces as the source IP address for requests forwarded to your web servers.\n\nThese IP addresses can be used for various purposes, such as allowing the load balancer traffic on the web servers and for request processing. It's a best practice to use security group referencing on the web servers for whitelisting load balancer traffic from Classic Load Balancers or Application Load Balancers.\n\nHowever, because Network Load Balancers don't support security groups, based on the target group configurations, the IP addresses of the clients or the private IP addresses associated with the Network Load Balancers must be allowed on the web server's security group.\n\nUse a Classic Load Balancer with an Auto Scaling Group - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the Amazon EC2-Classic network.\n\nUse an Application Load Balancer with an Auto Scaling Group - Application Load Balancer operates at the request level (layer 7), routing traffic to targets â€“ Amazon EC2 instances, containers, IP addresses and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at the delivery of modern application architectures, including microservices and container-based applications.\n\nApplication and Classic Load Balancers expose a fixed DNS (=URL) rather than the IP address. So these are incorrect options for the given use-case.\n\nUse an Auto Scaling Group with Dynamic Elastic IPs attachment - The option \"Use an Auto Scaling Group (ASG) with Dynamic Elastic IPs attachment\" has been added as a distractor. ASG does not have a dynamic Elastic IPs attachment feature.\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/elb-find-load-balancer-IP/\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html",
    "awsService": "Auto Scaling",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q505",
    "questionText": "A ride-sharing company wants to improve the ride-tracking system that stores GPS coordinates for all rides. The engineering team at the company is looking for a NoSQL database that has single-digit millisecond latency, can scale horizontally, and is serverless, so that they can perform high-frequency lookups reliably.\n\nAs a Solutions Architect, which database do you recommend for their requirements?",
    "options": [
      {
        "text": "Amazon Neptune",
        "isCorrect": false
      },
      {
        "text": "Amazon Relational Database Service (Amazon RDS)",
        "isCorrect": false
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nAmazon DynamoDB\n\nAmazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable NoSQL database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. DynamoDB is serverless, has single-digit millisecond latency and scales horizontally. This is the correct choice for the given requirements.\n\nSample Amazon DynamoDB solution for Real time applications:\n\nvia - https://aws.amazon.com/dynamodb/\n\nIncorrect options:\n\nAmazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores, compatible with Redis or Memcached. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. The primary use-case for ElastiCache is that of a caching service and it should not be used as the main database.\n\nHow Amazon ElastiCache works:\n\nvia - https://aws.amazon.com/elasticache/\n\nAmazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. Relational databases are not NoSQL databases and these cannot provide the millisecond latency that the current use case needs, hence it's an incorrect choice.\n\nAmazon Neptune - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.\n\nExample Use cases of Amazon Neptune:\n\nvia - https://aws.amazon.com/neptune/\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/\n\nhttps://aws.amazon.com/elasticache/\n\nhttps://aws.amazon.com/neptune/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q506",
    "questionText": "The development team at a social media company wants to handle some complicated queries such as \"What are the number of likes on the videos that have been posted by friends of a user A?\".\n\nAs a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?",
    "options": [
      {
        "text": "Amazon Neptune",
        "isCorrect": true
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      },
      {
        "text": "Amazon OpenSearch Service",
        "isCorrect": false
      },
      {
        "text": "Amazon Aurora",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon Neptune\n\nAmazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.\n\nAmazon Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune is secure with support for HTTPS encrypted client connections and encryption at rest. Neptune is fully managed, so you no longer need to worry about database management tasks such as hardware provisioning, software patching, setup, configuration, or backups.\n\nAmazon Neptune can quickly and easily process large sets of user-profiles and interactions to build social networking applications. Neptune enables highly interactive graph queries with high throughput to bring social features into your applications. For example, if you are building a social feed into your application, you can use Neptune to provide results that prioritize showing your users the latest updates from their family, from friends whose updates they â€˜Like,â€™ and from friends who live close to them.\n\nSocial Networking example with Amazon Neptune:\n\nvia - https://aws.amazon.com/neptune/\n\nIdentity graphs example with Amazon Neptune:\n\nvia - https://aws.amazon.com/neptune/\n\nIncorrect options:\n\nAmazon OpenSearch Service - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. Amazon OpenSearch Service offers the latest versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), as well as visualization capabilities powered by OpenSearch Dashboards and Kibana (1.5 to 7.10 versions). Amazon OpenSearch Service currently has tens of thousands of active customers with hundreds of thousands of clusters under management processing trillions of requests per month.\n\nAmazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. The given use-case is not about data warehousing, so this is not a correct option.\n\nAmazon Aurora - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64 terabytes per database instance. Aurora is not an in-memory database. Here, we need a graph database due to the highly connected datasets and queries, therefore Neptune is the best answer.\n\nReference:\n\nhttps://aws.amazon.com/neptune/",
    "awsService": "Redshift",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q507",
    "questionText": "A transportation logistics company runs a shipment tracking application on Amazon EC2 instances with an Amazon Aurora MySQL database cluster. The application is experiencing rapid growth due to increased demand from mobile app users querying package delivery statuses. Although the compute layer (EC2) has remained stable, the Aurora DB cluster is under growing read pressure, especially from frequent repeated queries about package locations and delivery history. The company added an Aurora read replica, which temporarily alleviated the load, but read traffic continues to spike as user queries grow. The company wants to reduce the repeated reads pressure on the DB cluster.\n\nWhich solution will best meet these requirements in a cost-effective manner?",
    "options": [
      {
        "text": "Integrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database",
        "isCorrect": true
      },
      {
        "text": "Enable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint",
        "isCorrect": false
      },
      {
        "text": "Add another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas",
        "isCorrect": false
      },
      {
        "text": "Convert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nIntegrate Amazon ElastiCache for Redis between the application and Aurora. Cache frequently accessed query results in Redis to reduce the number of identical read requests hitting the database\n\nThis is the most cost-effective and high-performance solution for workloads that suffer from frequent, repeated reads of the same data. Amazon ElastiCache for Redis acts as an in-memory caching layer between the application and Aurora. When delivery status data is read repeatedly, the application can first check the cache, and only query Aurora when a cache miss occurs. This reduces Aurora read traffic, improves performance, and scales cost-efficiently without overprovisioning read replicas. Redis is ideal for low-latency access to read-heavy or idempotent workloads.\n\nIncorrect options:\n\nEnable Aurora Serverless v2 for the DB cluster to automatically scale read and write capacity in response to usage spikes. Route all traffic through the cluster endpoint - Aurora Serverless v2 provides fine-grained autoscaling, but it is designed primarily for variable and infrequent workloadsâ€”not for high-throughput, read-intensive traffic. It does not support read replicas, and read traffic must route through the cluster endpoint, limiting scalability. It is not optimal or cost-effective for workloads that already have steady or high read demand.\n\nAdd another Aurora read replica to distribute the increasing read load across more read nodes. Adjust the application to perform client-side load balancing across the read replicas - Adding more read replicas can help scale read operations linearly, but it's not the most cost-effective approach for repeated read queries. Each read replica incurs additional cost, and identical queries will still hit the database repeatedly, consuming IOPS and CPU unnecessarily. Caching would reduce this load more efficiently.\n\nConvert the Aurora MySQL DB cluster into a multi-writer setup using Aurora global database. Allow concurrent writes from multiple application nodes across Regions - Aurora multi-writer setups are designed for globally distributed write workloads, not for optimizing local repeated reads. Multi-writer configurations increase cost and add complexity with conflict resolution and write coordination. This is overkill for a workload thatâ€™s bottlenecked on read traffic, not write concurrency.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/dg/elasticache-use-cases.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q508",
    "questionText": "A company has developed a popular photo-sharing website using a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. The website is experiencing high read traffic and the AWS Lambda functions are putting an increased read load on the Amazon RDS database.\n\nThe architecture team is planning to increase the read throughput of the database, without changing the application's core logic. As a Solutions Architect, what do you recommend?",
    "options": [
      {
        "text": "Use Amazon RDS Multi-AZ feature",
        "isCorrect": false
      },
      {
        "text": "Use Amazon RDS Read Replicas",
        "isCorrect": true
      },
      {
        "text": "Use Amazon ElastiCache",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon RDS Read Replicas\n\nAmazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances.\n\nMore on Amazon RDS Read Replicas:\n\nvia - https://aws.amazon.com/rds/features/read-replicas/\n\nIncorrect options:\n\nUse Amazon RDS Multi-AZ feature - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete.\n\nUse Amazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.\n\nUse Amazon DynamoDB - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Amazon DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second.\n\nAmazon RDS Multi-AZ helps with disaster recovery in case of an AZ failure. Amazon ElastiCache would definitely help with the read load, but would require a refactor of the application's core logic. Amazon DynamoDB with DAX would also probably help with the read load, but once again it would require a refactor of the application's core logic. Here, our only option to scale reads is to use Amazon RDS Read Replicas.\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/multi-az/\n\nhttps://aws.amazon.com/rds/features/read-replicas/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q509",
    "questionText": "A digital media company needs to manage uploads of around 1 terabyte each from an application being used by a partner company.\n\nAs a Solutions Architect, how will you handle the upload of these files to Amazon S3?",
    "options": [
      {
        "text": "Use multi-part upload feature of Amazon S3",
        "isCorrect": true
      },
      {
        "text": "Use Amazon S3 Versioning",
        "isCorrect": false
      },
      {
        "text": "Use AWS Snowball",
        "isCorrect": false
      },
      {
        "text": "Use AWS Direct Connect to provide extra bandwidth",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse multi-part upload feature of Amazon S3\n\nMulti-part upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object.\n\nAWS recommends that you use multi-part uploading in the following ways:\n1. If you're uploading large objects over a stable high-bandwidth network, use multi-part uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance.\n2. If you're uploading over a spotty network, use multi-part uploading to increase resiliency to network errors by avoiding upload restarts. When using multi-part uploading, you need to retry uploading only parts that are interrupted during the upload. You don't need to restart uploading your object from the beginning.\n\nIn general, when your object size reaches 100 megabytes, you should consider using multipart uploads instead of uploading the object in a single operation. If the file is greater than 5 gigabytes in size, you must use multi-part upload to upload that file to Amazon S3.\n\nIncorrect options:\n\nUse Amazon S3 Versioning - Amazon S3 Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. When you enable versioning for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of the objects. If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.\n\nUse AWS Direct Connect to provide extra bandwidth - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.\n\nAWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. This dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs. This is a physical connection that takes at least a month to set up.\n\nUse AWS Snowball - AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 terabytes of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 gigabytes network connectivity to address large scale data transfer and pre-processing use cases.\n\n(The original AWS Snowball devices were transitioned out of service and AWS Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original AWS Snowball device had 80 terabytes of storage space).\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q510",
    "questionText": "A company has noticed that its Amazon EBS Elastic Volume (io1) accounts for 90% of the cost and the remaining 10% cost can be attributed to the Amazon EC2 instance. The Amazon CloudWatch metrics report that both the Amazon EC2 instance and the Amazon EBS volume are under-utilized. The Amazon CloudWatch metrics also show that the Amazon EBS volume has occasional I/O bursts. The entire infrastructure is managed by AWS CloudFormation.\n\nAs a Solutions Architect, what do you propose to reduce the costs?",
    "options": [
      {
        "text": "Don't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges",
        "isCorrect": false
      },
      {
        "text": "Keep the Amazon EBS volume to io1 and reduce the IOPS",
        "isCorrect": false
      },
      {
        "text": "Convert the Amazon EC2 instance EBS volume to gp2",
        "isCorrect": true
      },
      {
        "text": "Change the Amazon EC2 instance type to something much smaller",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon EBS provides the various volume types, that differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volumes types fall into two categories:\n\nSSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS.\n\nHDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS\n\nProvisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.\n\nConvert the Amazon EC2 instance EBS volume to gp2\n\nGeneral Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for an extended duration. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver a provisioned performance of 99% uptime. A gp2 volume can range in size from 1 GiB to 16 TiB.\n\nTherefore, gp2 is the right choice as it is more cost-effective than io1, and it also allows a burst in performance when needed.\n\nIncorrect options:\n\nKeep the Amazon EBS volume to io1 and reduce the IOPS - Keeping the Amazon EBS volume to io1 and reducing the IOPS may interfere with the burst of performance we need, so this option is ruled out.\n\nChange the Amazon EC2 instance type to something much smaller - Changing the Amazon EC2 instance type to something much smaller won't affect 90% of the costs that are incurred, therefore this option is also incorrect.\n\nDon't use a AWS CloudFormation template to create the database as the AWS CloudFormation service incurs greater service charges - This statement is incorrect as AWS CloudFormation is a free service to use. The resources that are invoked by CloudFormation are charged as per their utilization rates, but using AWS CloudFormation will not cost anything.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q511",
    "questionText": "As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The Auto Scaling Group spans across 2 Availability Zones (AZs). AZ-A has 3 Amazon EC2 instances and AZ-B has 4 Amazon EC2 instances. The Auto Scaling Group is about to go into a scale-in event due to the triggering of a Amazon CloudWatch alarm.\n\nWhat will happen under the default Auto Scaling Group configuration?",
    "options": [
      {
        "text": "The instance with the oldest launch template or launch configuration will be terminated in AZ-B",
        "isCorrect": true
      },
      {
        "text": "A random instance in the AZ-A will be terminated",
        "isCorrect": false
      },
      {
        "text": "An instance in the AZ-A will be created",
        "isCorrect": false
      },
      {
        "text": "A random instance will be terminated in AZ-B",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThe instance with the oldest launch template or launch configuration will be terminated in AZ-B\n\nAmazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of Amazon EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.\n\nWith each Auto Scaling group, you can control when it adds instances (referred to as scaling out) or removes instances (referred to as scaling in) from your network architecture.\n\nThe default termination policy is designed to help ensure that your instances span Availability Zones evenly for high availability. The default policy is kept generic and flexible to cover a range of scenarios.\n\nThe default termination policy behavior is as follows:\n1. Determine which Availability Zones (Azs) have the most instances and at least one instance that is not protected from scale-in.\n2. Determine which instances to terminate to align the remaining instances to the allocation strategy for the On-Demand or Spot Instance that is terminating.\n3. Determine whether any of the instances use the oldest launch template or configuration:\n    3.a. Determine whether any of the instances use the oldest launch template unless there are instances that use a launch configuration.\n    3.b. Determine whether any of the instances use the oldest launch configuration.\n4. After applying all of the above criteria, if there are multiple unprotected instances to terminate, determine which instances are closest to the next billing hour.\n\nPer the given use-case, AZs will be balanced first, then the instance with the oldest launch template or launch configuration within the applicable AZ (AZ-B) will be terminated.\n\nDefault Termination policy:\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\n\nIncorrect options:\n\nA random instance in the AZ-A will be terminated\n\nAn instance in the AZ-A will be created\n\nA random instance will be terminated in AZ-B\n\nThese three options contradict the details provided in the explanation above. Hence these are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q512",
    "questionText": "What does this AWS CloudFormation snippet do? (Select three)\n\nSecurityGroupIngress:\n     - IpProtocol: tcp\n       FromPort: 80\n       ToPort: 80\n       CidrIp: 0.0.0.0/0\n     - IpProtocol: tcp\n       FromPort: 22\n       ToPort: 22\n       CidrIp: 192.168.1.1/32",
    "options": [
      {
        "text": "It configures the inbound rules of a network access control list (network ACL)",
        "isCorrect": false
      },
      {
        "text": "It allows any IP to pass through on the HTTP port",
        "isCorrect": true
      },
      {
        "text": "It only allows the IP 0.0.0.0 to reach HTTP",
        "isCorrect": false
      },
      {
        "text": "It prevents traffic from reaching on HTTP unless from the IP 192.168.1.1",
        "isCorrect": false
      },
      {
        "text": "It configures a security group's inbound rules",
        "isCorrect": true
      },
      {
        "text": "It lets traffic flow from one IP on port 22",
        "isCorrect": true
      },
      {
        "text": "It configures a security group's outbound rules",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nIt allows any IP to pass through on the HTTP port\n\nIt configures a security group's inbound rules\n\nIt lets traffic flow from one IP on port 22\n\nA security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.\n\nThe following are the characteristics of security group rules:\n    1. By default, security groups allow all outbound traffic.\n    2. Security group rules are always permissive; you can't create rules that deny access.\n    3. Security groups are stateful\n\nAWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This gives you a single source of truth for your AWS and third-party resources.\n\nConsidering the given AWS CloudFormation snippet, 0.0.0.0/0 means any IP, not the IP 0.0.0.0. Ingress means traffic going into your instance, and Security Groups are different from NACL. Each \"-\" in our security group rule represents a different rule (YAML syntax)\n\nTherefore the AWS CloudFormation snippet creates two Security Group inbound rules that allow any IP to pass through on the HTTP port and lets traffic flow from one source IP (192.168.1.1) on port 22.\n\nIncorrect options:\n\nIt configures the inbound rules of a network access control list (network ACL) - A Network Access Control List ( Network ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups to add an additional layer of security to your VPC.\n\nIt only allows the IP 0.0.0.0 to reach HTTP\n\nIt prevents traffic from reaching on HTTP unless from the IP 192.168.1.1\n\nIt configures a security group's outbound rules\n\nThese three options contradict the description provided above. So these are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\n\nhttps://aws.amazon.com/cloudformation/",
    "awsService": "CloudFormation",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q513",
    "questionText": "A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity.\n\nWhich of the following options will maximize the VPN throughput?",
    "options": [
      {
        "text": "Use Transfer Acceleration for the VPN connection to maximize the throughput",
        "isCorrect": false
      },
      {
        "text": "Use AWS Global Accelerator for the VPN connection to maximize the throughput",
        "isCorrect": false
      },
      {
        "text": "Create a virtual private gateway with equal cost multipath routing and multiple channels",
        "isCorrect": false
      },
      {
        "text": "Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nCreate an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels\n\nVPN connection is a secure connection between your on-premises equipment and your VPCs. Each VPN connection has two VPN tunnels which you can use for high availability. A VPN tunnel is an encrypted link where data can pass from the customer network to or from AWS. The following diagram shows the high-level connectivity with virtual private gateways.\n\nWith AWS Transit Gateway, you can simplify the connectivity between multiple VPCs and also connect to any VPC attached to AWS Transit Gateway with a single VPN connection. AWS Transit Gateway also enables you to scale the IPsec VPN throughput with equal cost multi-path (ECMP) routing support over multiple VPN tunnels. A single VPN tunnel still has a maximum throughput of 1.25 Gbps. If you establish multiple VPN tunnels to an ECMP-enabled transit gateway, it can scale beyond the default maximum limit of 1.25 Gbps.  You also must enable the dynamic routing option on your transit gateway to be able to take advantage of ECMP for scalability.\n\n\nvia - https://aws.amazon.com/premiumsupport/knowledge-center/transit-gateway-ecmp-multiple-tunnels/\n\nIncorrect options:\n\nUse Transfer Acceleration for the VPN connection to maximize the throughput - Transfer Acceleration is an Amazon S3 bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets. Transfer Acceleration takes advantage of the globally distributed edge locations in Amazon CloudFront.\n\nThis option has been added as a distractor as it is not relevant to AWS VPN connections.\n\nUse AWS Global Accelerator for the VPN connection to maximize the throughput - AWS Global Accelerator is a networking service that improves the performance of your usersâ€™ traffic by up to 60% using the global network infrastructure of AWS. When the internet is congested, AWS Global Accelerator optimizes the path to your application to keep packet loss, jitter, and latency consistently low. With Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, improving availability. Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.\n\nAWS Global Accelerator can be used to optimize the network path, using the congestion-free AWS global network to route traffic to the endpoint that provides the best application performance . You can use an accelerated VPN connection to avoid network disruptions that might occur when traffic is routed over the public internet. AWS Global Accelerator will not maximize the VPN throughput, so it is not the best fit for the given use case.\n\nCreate a virtual private gateway with equal cost multipath routing and multiple channels - A virtual private gateway is the VPN endpoint on the Amazon side of your Site-to-Site VPN connection that can be attached to a single VPC. A virtual private gateway does not support equal cost multi-path (ECMP) routing, so this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/transit-gateway-ecmp-multiple-tunnels/",
    "awsService": "General",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q514",
    "questionText": "An e-commerce company wants to migrate its on-premises application to AWS. The application consists of application servers and a Microsoft SQL Server database. The solution should result in the maximum possible availability for the database layer while minimizing operational and management overhead.\n\nAs a solutions architect, which of the following would you recommend to meet the given requirements?",
    "options": [
      {
        "text": "Migrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration",
        "isCorrect": false
      },
      {
        "text": "Migrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration",
        "isCorrect": false
      },
      {
        "text": "Migrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment",
        "isCorrect": false
      },
      {
        "text": "Migrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nMigrate the data to Amazon RDS for SQL Server database in a Multi-AZ deployment\n\nAmazon RDS supports Multi-AZ deployments for Microsoft SQL Server by using either SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs). Amazon RDS monitors and maintains the health of your Multi-AZ deployment. If problems occur, Amazon RDS automatically repairs unhealthy database instances, reestablishes synchronization, and initiates failovers.\n\nMulti-AZ deployments provide increased availability, data durability, and fault tolerance for database instances. In the event of planned database maintenance or unplanned service disruption, Amazon RDS automatically fails over to the up-to-date secondary database instance. This functionality lets database operations resume quickly without manual intervention. The primary and standby instances use the same endpoint, whose physical network address transitions to the secondary replica as part of the failover process. You don't have to reconfigure your application when a failover occurs.\n\nThis option provides the maximum possible availability for the database layer while minimizing operational and management overhead.\n\nIncorrect options:\n\nMigrate the data to Amazon EC2 instance hosted SQL Server database. Deploy the Amazon EC2 instances in a Multi-AZ configuration - Hosting SQL Server database on Amazon EC2 instance involves significant operational and management overhead in terms of OS patching, database patching, etc. So this option is incorrect.\n\nMigrate the data to Amazon RDS for SQL Server database in a cross-region read-replica configuration - Amazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWS Region. Read replicas are used to enhance the read scalability of a database. You cannot use read replicas to improve the availability of a database. Therefore this option is incorrect.\n\nMigrate the data to Amazon RDS for SQL Server database in a cross-region Multi-AZ deployment - Amazon RDS Multi-AZ deployments provide enhanced availability for database instances within a single AWS Region. There is no such thing as a cross-region Multi-AZ deployment. Hence this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_SQLServerMultiAZ.html\n\nhttps://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q515",
    "questionText": "A Big Data processing company has created a distributed data processing framework that performs best if the network performance between the processing machines is high. The application has to be deployed on AWS, and the company is only looking at performance as the key measure.\n\nAs a Solutions Architect, which deployment do you recommend?",
    "options": [
      {
        "text": "Use Spot Instances",
        "isCorrect": false
      },
      {
        "text": "Use a Spread placement group",
        "isCorrect": false
      },
      {
        "text": "Optimize the Amazon EC2 kernel using EC2 User Data",
        "isCorrect": false
      },
      {
        "text": "Use a Cluster placement group",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nWhen you launch a new Amazon EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:\n\nCluster â€“ packs instances close together inside an Availability Zone (AZ). This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.\n\nPartition â€“ spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.\n\nSpread â€“ strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.\n\nThere is no charge for creating a placement group.\n\nUse a Cluster placement group\n\nA cluster placement group is a logical grouping of instances within a single Availability Zone (AZ). A cluster placement group can span peered VPCs in the same Region. Instances in the same cluster placement group enjoy a higher per-flow throughput limit of up to 10 Gbps for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network.\n\nCluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking.\n\nImage of Cluster placement group:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nImage of Partition placement group:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nImage of Spread placement group:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nIncorrect options:\n\nUse Spot Instances - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. Since performance is the key criteria, this is not the right choice.\n\nOptimize the Amazon EC2 kernel using EC2 User Data -  Optimizing the Amazon EC2 kernel won't help with network performance as it's bounded by the EC2 instance type mainly. Therefore, this option is incorrect.\n\nUse a Spread placement group - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. A spread placement group can span multiple Availability Zones (AZs) in the same Region. You can have a maximum of seven running instances per Availability Zone (AZ) per group.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q516",
    "questionText": "A streaming media company operates a high-traffic content delivery platform on AWS. The application backend is deployed on Amazon EC2 instances within an Auto Scaling group across multiple Availability Zones in a VPC. The team has observed that workloads follow predictable usage patterns, such as higher viewership on weekends and in the evenings, along with occasional real-time spikes due to viral content. To reduce cost and improve responsiveness, the team wants an automated scaling approach that can forecast future demand using historical usage patterns, scale in advance based on those predictions, and react quickly to unplanned usage surges in real time.\n\nWhich scaling strategy should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "text": "Configure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments",
        "isCorrect": false
      },
      {
        "text": "Use predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes",
        "isCorrect": true
      },
      {
        "text": "Implement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours",
        "isCorrect": false
      },
      {
        "text": "Set up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse predictive scaling for the Auto Scaling group to analyze daily and weekly patterns, and configure dynamic scaling with target tracking policies to respond to real-time traffic changes\n\nThis strategy offers the most intelligent and automated approach for the given requirements. Predictive scaling uses machine learning to analyze historical workload patterns and forecast future usage. It can schedule capacity adjustments in advance based on expected demand. By combining predictive scaling with dynamic scaling using target tracking (e.g., maintaining average CPU at 60%), the Auto Scaling group can adapt in real time to unforeseen traffic surges. This hybrid method provides cost-efficiency and responsiveness.\n\n\nvia - https://aws.amazon.com/blogs/compute/introducing-native-support-for-predictive-scaling-with-amazon-ec2-auto-scaling/\n\nIncorrect options:\n\nConfigure step scaling policies based on EC2 CPU utilization. Use CloudWatch alarms to trigger scaling actions when utilization crosses defined thresholds with incremental adjustments - Step scaling reacts only to real-time metrics, such as CPU utilization, using predefined steps. While it offers precise control, it does not provide forecasting or proactive scaling. It is not suited for workloads that show predictable, time-based patterns, since scaling happens after demand increases, not before.\n\nImplement scheduled scaling actions based on pre-defined time windows from historical traffic data. Adjust instance count manually for known high-traffic hours - Scheduled scaling can be used to pre-provision capacity at specific times. It lacks the flexibility and intelligence of automated forecasting and dynamic scaling. It also does not adapt to day-to-day variation or sudden changes in demand, making it less reliable for dynamic web applications. The manual approach for adjusting the instance count (for known high-traffic hours) does not meet the requirements of an automated solution.\n\nSet up simple scaling policies with longer cooldown periods to avoid rapid scaling. Trigger scale-out events based on average network throughput - Simple scaling policies perform one action per alarm and are limited in flexibility. Long cooldowns reduce the frequency of scaling actions but can delay necessary adjustments. It also lacks the ability to scale proactively or adapt to complex usage patterns.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/compute/introducing-native-support-for-predictive-scaling-with-amazon-ec2-auto-scaling/\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q517",
    "questionText": "An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes.\n\nWhat would you recommend as an AWS Certified Solutions Architect - Associate?",
    "options": [
      {
        "text": "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3",
        "isCorrect": true
      },
      {
        "text": "Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time",
        "isCorrect": false
      },
      {
        "text": "Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nLeverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3\n\nYou can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs.\nAmazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams.\n\nFor the given use case, you can use Amazon Kinesis Data Analytics to transform and analyze incoming streaming data from Kinesis Data Streams in real time. Kinesis Data Analytics takes care of everything required to run streaming applications continuously, and scales automatically to match the volume and throughput of your incoming data. With Amazon Kinesis Data Analytics, there are no servers to manage, no minimum fee or setup cost, and you only pay for the resources your streaming applications consume.\n\nAmazon Kinesis Data Analytics:\n\nvia - https://aws.amazon.com/kinesis/\n\nAmazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms and delivers streaming data to data lakes, data stores, and analytics services.\n\nFor the given use case, post the real-time analysis, the output feed from Kinesis Data Analytics is output into Kinesis Data Firehose which dumps the data into Amazon S3 without any data loss.\n\nAmazon Kinesis Data Firehose:\n\nvia - https://aws.amazon.com/kinesis/\n\nIncorrect options:\n\nLeverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3 - Amazon QuickSight cannot use Amazon Kinesis Data Streams as a source. In addition, Amazon QuickSight cannot be used for real-time streaming data analysis from its source. Therefore this option is incorrect.\n\nLeverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena cannot be used to analyze data in real time. Therefore this option is incorrect.\n\nLeverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances - Even though using Amazon SQS with Amazon EC2 instances can decouple the architecture, however, performing real-time analytics using a third party library on the Amazon EC2 instances is not the best fit solution for the given use case. The Amazon Kinesis family of services is the better fit for the given scenario as these services allow streaming data ingestion, real-time analysis, and reliable data delivery to the data sink.\n\nReferences:\n\nhttps://aws.amazon.com/kinesis/\n\nhttps://aws.amazon.com/quicksight/resources/faqs/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q518",
    "questionText": "For security purposes, a development team has decided to deploy the Amazon EC2 instances in a private subnet. The team plans to use VPC endpoints so that the instances can access some AWS services securely. The members of the team would like to know about the two AWS services that support Gateway Endpoints.\n\nAs a solutions architect, which of the following services would you suggest for this requirement? (Select two)",
    "options": [
      {
        "text": "Amazon Simple Queue Service (Amazon SQS)",
        "isCorrect": false
      },
      {
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB",
        "isCorrect": true
      },
      {
        "text": "Amazon S3",
        "isCorrect": true
      },
      {
        "text": "Amazon Kinesis",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nAmazon S3\n\nAmazon DynamoDB\n\nA VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.\n\nEndpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.\n\nThere are two types of VPC endpoints: Interface Endpoints and Gateway Endpoints. An Interface Endpoint is an Elastic Network Interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.\n\nA Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported: Amazon S3 and Amazon DynamoDB.\n\nYou can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints. A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway.\n\nYou must remember that these two services use a VPC gateway endpoint. The rest of the AWS services use VPC interface endpoints.\n\nGateway VPC endpoints:\n\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\n\nIncorrect options:\n\nAmazon Simple Queue Service (Amazon SQS)\n\nAmazon Simple Notification Service (Amazon SNS)\n\nAmazon Kinesis\n\nAs mentioned in the description above, these three options use interface endpoints, so these are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q519",
    "questionText": "A research organization is running a high-performance computing (HPC) workload using Amazon EC2 instances that are distributed across multiple Availability Zones (AZs) within a single AWS Region. The workload requires access to a shared file system with the lowest possible latency for frequent reads and writes.The team decides to use Amazon Elastic File System (Amazon EFS) for its scalability and simplicity. To ensure optimal performance and reduce network latency, the solution architect must design the architecture so that each EC2 instance can access the file system with the least possible delay.\n\nWhich of the following is the most appropriate solution to meet these requirements?",
    "options": [
      {
        "text": "Create EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target",
        "isCorrect": true
      },
      {
        "text": "Create mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances",
        "isCorrect": false
      },
      {
        "text": "Create a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target",
        "isCorrect": false
      },
      {
        "text": "Use Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate EFS mount targets in each AZ and mount the EFS file system to EC2 instances in the same AZ as the mount target\n\nCreating Amazon EFS mount targets in each Availability Zone and mounting the EFS file system to EC2 instances within the same AZ as their respective mount targets ensures the lowest possible latency and avoids inter-AZ data transfer costs. When an EC2 instance accesses an EFS mount target in the same AZ, the traffic stays within the local VPC infrastructure, resulting in faster performance and reduced latency. This setup also improves fault toleranceâ€”if one AZ becomes unavailable, instances in other AZs can still access EFS through their local mount targets. According to AWS best practices, for optimal performance and high availability, an EFS file system should have a mount target in each AZ where EC2 instances require access.\n\n\nvia - https://docs.aws.amazon.com/efs/latest/ug/accessing-fs.html\n\nIncorrect options:\n\nCreate mount targets for Amazon EFS on an EC2 instance in each AZ and use them to serve as access points for other instances - This option is incorrect because mount targets are not created on EC2 instancesâ€”they are created at the VPC level within each Availability Zone and are managed by the EFS service itself. Mount targets are Elastic Network Interfaces (ENIs) that enable EC2 instances within the same AZ to connect to the EFS file system using the VPCâ€™s internal networking. You do not and cannot host EFS mount targets on EC2 instances. Confusing mount points (where you mount the file system on an EC2 instance) with mount targets (infrastructure provided by EFS) leads to a misunderstanding of how Amazon EFS architecture works.\n\nCreate a single EFS mount target in one AZ and allow all EC2 instances in other AZs to access it using the default mount target - Creating a single mount target in one AZ and having all EC2 instances across AZs access it leads to cross-AZ traffic, which increases latency and incurs data transfer costs. This setup ignores the purpose of EFS mount targets in multiple AZs and underutilizes the capability to serve traffic locally within each AZ.\n\nUse Mountpoint for Amazon S3 to mount an S3 bucket on each EC2 instance and use it as a shared storage layer across Availability Zones - While Mountpoint for Amazon S3 is an AWS-supported, open-source file client that allows EC2 instances to mount S3 buckets as a local file system, it is optimized for high-throughput sequential access to large objects, not for low-latency, POSIX-compliant shared file system behavior. S3 is an object store, not a file systemâ€”and does not support the fine-grained consistency, low-latency access, or file locking mechanisms required by HPC workloads or tightly coupled distributed applications. Therefore, it is not a suitable substitute for Amazon EFS in scenarios requiring shared, low-latency access across EC2 instances.\n\nReferences:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/accessing-fs.html\n\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nhttps://docs.aws.amazon.com/efs/latest/ug/how-it-works.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q520",
    "questionText": "The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture.\n\nAs a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)",
    "options": [
      {
        "text": "By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources",
        "isCorrect": true
      },
      {
        "text": "AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions",
        "isCorrect": false
      },
      {
        "text": "The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package",
        "isCorrect": false
      },
      {
        "text": "Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold",
        "isCorrect": true
      },
      {
        "text": "If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code",
        "isCorrect": true
      },
      {
        "text": "Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nBy default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources\n\nAWS Lambda functions always operate from an AWS-owned VPC. By default, your function has the full ability to make network requests to any public internet address â€” this includes access to any of the public AWS APIs. For example, your function can interact with AWS DynamoDB APIs to PutItem or Query for records. You should only enable your functions for VPC access when you need to interact with a private resource located in a private subnet. An Amazon RDS instance is a good example.\n\nOnce your function is VPC-enabled, all network traffic from your function is subject to the routing rules of your VPC/Subnet. If your function needs to interact with a public resource, you will need a route through a NAT gateway in a public subnet.\n\nWhen to VPC-Enable an AWS Lambda Function:\n\nvia - https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/\n\nSince AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold\n\nSince AWS Lambda functions can scale extremely quickly, this means you should have controls in place to notify you when you have a spike in concurrency. A good idea is to deploy an Amazon CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds your threshold. You should create an AWS Budget so you can monitor costs on a daily basis.\n\nIf you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code\n\nYou can configure your AWS Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. A function can use up to 5 layers at a time.\n\nYou can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 megabytes.\n\nIncorrect options:\n\nAWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions - AWS Lambda allocates compute power in proportion to the memory you allocate to your function. This means you can over-provision memory to run your functions faster and potentially reduce your costs. However, AWS recommends that you should not over provision your function time out settings. Always understand your code performance and set a function time out accordingly. Overprovisioning function timeout often results in Lambda functions running longer than expected and unexpected costs.\n\nThe bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package - This statement is incorrect and acts as a distractor. All the dependencies are also packaged into the single Lambda deployment package.\n\nServerless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images - This statement is incorrect. You can now package and deploy AWS Lambda functions as container images.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\n\nhttps://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q521",
    "questionText": "An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website.\n\nWhat could be the reason the instances are being marked as unhealthy? (Select two)",
    "options": [
      {
        "text": "The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer",
        "isCorrect": true
      },
      {
        "text": "The route for the health check is misconfigured",
        "isCorrect": true
      },
      {
        "text": "The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted",
        "isCorrect": false
      },
      {
        "text": "Your web-app has a runtime that is not supported by the Application Load Balancer",
        "isCorrect": false
      },
      {
        "text": "You need to attach elastic IP address (EIP) to the Amazon EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nThe security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer\n\nThe route for the health check is misconfigured\n\nAn Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called health checks.\n\nEach load balancer node routes requests only to the healthy targets in the enabled Availability Zones (AZs) for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. If a target group contains only unhealthy registered targets, the load balancer nodes route requests across its unhealthy targets.\n\nYou must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.\n\nApplication Load Balancer Configuration for Security Groups and Health Check Routes:\n\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\n\nIncorrect options:\n\nThe Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted - You can access the website using the IP address which means there is no issue with the Amazon EBS volumes. So this option is not correct.\n\nYour web-app has a runtime that is not supported by the Application Load Balancer - There is no connection between a web app runtime and the application load balancer. This option has been added as a distractor.\n\nYou need to attach elastic IP address (EIP) to the Amazon EC2 instances - This option is a distractor as Elastic IPs do not need to be assigned to Amazon EC2 instances while using an Application Load Balancer.\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q522",
    "questionText": "A Big Data analytics company writes data and log files in Amazon S3 buckets. The company now wants to stream the existing data files as well as any ongoing file updates from Amazon S3 to Amazon Kinesis Data Streams.\n\nAs a Solutions Architect, which of the following would you suggest as the fastest possible way of building a solution for this requirement?",
    "options": [
      {
        "text": "Configure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams",
        "isCorrect": false
      },
      {
        "text": "Leverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams",
        "isCorrect": false
      },
      {
        "text": "Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nLeverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams\n\nYou can achieve this by using AWS Database Migration Service (AWS DMS). AWS DMS enables you to seamlessly migrate data from supported sources to relational databases, data warehouses, streaming platforms, and other data stores in AWS cloud.\n\nThe given requirement needs the functionality to be implemented in the least possible time. You can use AWS DMS for such data-processing requirements. AWS DMS lets you expand the existing application to stream data from Amazon S3 into Amazon Kinesis Data Streams for real-time analytics without writing and maintaining new code. AWS DMS supports specifying Amazon S3 as the source and streaming services like Kinesis and Amazon Managed Streaming of Kafka (Amazon MSK) as the target. AWS DMS allows migration of full and change data capture (CDC) files to these services. AWS DMS performs this task out of box without any complex configuration or code development. You can also configure an AWS DMS replication instance to scale up or down depending on the workload.\n\nAWS DMS supports Amazon S3 as the source and Kinesis as the target, so data stored in an S3 bucket is streamed to Kinesis. Several consumers, such as AWS Lambda, Amazon Kinesis Data Firehose, Amazon Kinesis Data Analytics, and the Kinesis Consumer Library (KCL), can consume the data concurrently to perform real-time analytics on the dataset. Each AWS service in this architecture can scale independently as needed.\n\nArchitecture of the proposed solution:\n\nvia - https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/\n\nIncorrect options:\n\nConfigure Amazon EventBridge events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the Amazon EventBridge event that will send the necessary data to Amazon Kinesis Data Streams - You will need to enable AWS Cloudtrail trail to use object-level actions as a trigger for Amazon EventBridge events. Also, using AWS Lambda functions would require significant custom development to write the data into Amazon Kinesis Data Streams, so this option is not the right fit.\n\nLeverage Amazon S3 event notification to trigger an AWS Lambda function for the file create event. The AWS Lambda function will then send the necessary data to Amazon Kinesis Data Streams - Using AWS Lambda functions would require significant custom development to write the data into Amazon Kinesis Data Streams, so this option is not the right fit.\n\nAmazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (Amazon SNS). Amazon SNS can then be used to send the updates to Amazon Kinesis Data Streams - Amazon S3 cannot directly write data into Amazon SNS, although it can certainly use Amazon S3 event notifications to send an event to Amazon SNS. Also, Amazon SNS cannot directly send messages to Amazon Kinesis Data Streams. So this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q523",
    "questionText": "A Pharmaceuticals company is looking for a simple solution to connect its VPCs and on-premises networks through a central hub.\n\nAs a Solutions Architect, which of the following would you suggest as the solution that requires the LEAST operational overhead?",
    "options": [
      {
        "text": "Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks",
        "isCorrect": true
      },
      {
        "text": "Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks",
        "isCorrect": false
      },
      {
        "text": "Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks",
        "isCorrect": false
      },
      {
        "text": "Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks\n\nThe AWS Transit Gateway allows customers to connect their Amazon VPCs and their on-premises networks to a single gateway. As your number of workloads running on AWS increases, you need to be able to scale your networks across multiple accounts and Amazon VPCs to keep up with the growth. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. AWS Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks, which act like spokes. This hub and spoke model simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network.\n\nAWS Transit Gateway:\n\nvia - https://aws.amazon.com/transit-gateway/\n\nIncorrect options:\n\nUse Transit VPC Solution to connect the Amazon VPCs to the on-premises networks - The Transit VPC can be used to enable connectivity between various VPCâ€™s in different regions and customer data centers. You can use this to connect multiple VPCs that are geographically disparate and/or running in separate AWS accounts, to a common VPC that serves as a global network transit center. This network topology simplifies network management and minimizes the number of connections that you need to set up.\n\nTransit VPC:\n\nvia - https://aws.amazon.com/transit-gateway/\n\nTransit VPC is not the right solution for this use-case as Transit Gateway provides several advantages over Transit VPC:\n1. Transit Gateway abstracts away the complexity of maintaining VPN connections with hundreds of VPCs.\n2. Transit Gateway removes the need to manage and scale Amazon EC2 based software appliances. AWS is responsible for managing all resources needed to route traffic.\n3. Transit Gateway removes the need to manage high availability by providing a highly available and redundant Multi-AZ infrastructure.\n4. Transit Gateway improves bandwidth for inter-VPC communication to burst speeds of 50 Gbps per Availability Zone (AZ).\n5. Transit Gateway streamlines user costs to a simple per hour per/GB transferred model.\n6. Transit Gateway decreases latency by removing Amazon EC2 proxies and the need for VPN encapsulation.\n\nPartially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks\n\nFully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks\n\nThe simplest way to connect two VPCs is to use VPC Peering. In this setup, a connection enables full bidirectional connectivity between the VPCs. This peering connection is used to route traffic between the VPCs. VPCs across accounts and AWS Regions can also be peered together. VPC peering only incurs costs for traffic traveling over the connection (there is no hourly infrastructure fee).\n\nVPC peering is point-to-point connectivity, and it does not support transitive routing. If you are using VPC peering, on-premises connectivity (VPN and/or Direct Connect) must be made to each VPC. Resources in a VPC cannot reach on-premises using the hybrid connectivity of a peered VPC. VPC peering is best used when resources in one VPC must communicate with resources in another VPC, the environment of both VPCs is controlled and secured, and the number of VPCs to be connected is less than 10 (to allow for the individual management of each connection). VPC peering offers the lowest overall cost when compared to other options for inter-VPC connectivity.\n\nNetwork setup using VPC Peering:\n\nvia - https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html\n\nYou cannot use VPC Peering to establish on-premises connectivity with AWS Cloud, so both these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-transit-vpc.html\n\nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q524",
    "questionText": "An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in the us-west-1 Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in the us-east-1 Region. The on-premises data center does not allow the use of AWS Snowball.\n\nAs a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)",
    "options": [
      {
        "text": "Copy data from the source bucket to the destination bucket using the aws S3 sync command",
        "isCorrect": true
      },
      {
        "text": "Use AWS Snowball Edge device to copy the data from one Region to another Region",
        "isCorrect": false
      },
      {
        "text": "Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console",
        "isCorrect": false
      },
      {
        "text": "Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration",
        "isCorrect": true
      },
      {
        "text": "Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nCopy data from the source bucket to the destination bucket using the aws S3 sync command\n\nThe aws S3 sync command uses the CopyObject APIs to copy objects between Amazon S3 buckets. The sync command lists the source and target buckets to identify objects that are in the source bucket but that aren't in the target bucket. The command also identifies objects in the source bucket that have different LastModified dates than the objects that are in the target bucket. The sync command on a versioned bucket copies only the current version of the objectâ€”previous versions aren't copied. By default, this preserves object metadata, but the access control lists (ACLs) are set to FULL_CONTROL for your AWS account, which removes any additional ACLs. If the operation fails, you can run the sync command again without duplicating previously copied objects.\n\nYou can use the command like so:\n\naws s3 sync s3://DOC-EXAMPLE-BUCKET-SOURCE s3://DOC-EXAMPLE-BUCKET-TARGET\n\nSet up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration\n\nAmazon S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication. This is done through the use of a Batch Operations job.\n\nYou should note that batch replication differs from live replication which continuously and automatically replicates new objects across Amazon S3 buckets. You cannot directly use the AWS S3 console to configure cross-Region replication for existing objects. By default, replication only supports copying new Amazon S3 objects after it is enabled using the AWS S3 console. Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. Object may be replicated to a single destination bucket or multiple destination buckets. Destination buckets can be in different AWS Regions or within the same Region as the source bucket. Once done, you can delete the replication configuration, as it ensures that batch replication is only used for this one-time data copy operation.\n\nIf you want to enable live replication for existing objects for your bucket, you must contact AWS Support and raise a support ticket. This is required to ensure that replication is configured correctly.\n\nIncorrect options:\n\nUse AWS Snowball Edge device to copy the data from one Region to another Region - As the given requirement is about copying the data from one AWS Region to another AWS Region, so AWS Snowball Edge cannot be used here. AWS Snowball Edge Storage Optimized is the optimal data transfer choice if you need to securely and quickly transfer terabytes to petabytes of data to AWS. You can use AWS Snowball Edge Storage Optimized if you have a large backlog of data to transfer or if you frequently collect data that needs to be transferred to AWS and your storage is in an area where high-bandwidth internet connections are not available or cost-prohibitive. AWS Snowball Edge can operate in remote locations or harsh operating environments, such as factory floors, oil and gas rigs, mining sites, hospitals, and on moving vehicles.\n\nCopy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console - AWS S3 console cannot be used to copy 1 petabytes of data from one bucket to another as it's not feasible. You should note that this option is different from using the replication options on the AWS console, since here you are using the copy and paste options provided on the AWS console, which is suggested for small or medium data volume. You should use S3 sync for the requirement of one-time copy of data.\n\nSet up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console - Amazon S3 Transfer Acceleration (Amazon S3TA) is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an Amazon S3 bucket. You cannot use Transfer Acceleration to copy objects across Amazon S3 buckets in different Regions using Amazon S3 console.\n\nReferences:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/move-objects-s3-bucket/\n\nhttps://aws.amazon.com/snowball/faqs/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q525",
    "questionText": "A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements.\n\nWhich of the following options represents a valid cost-optimization solution?",
    "options": [
      {
        "text": "Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies",
        "isCorrect": false
      },
      {
        "text": "Use AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations",
        "isCorrect": true
      },
      {
        "text": "Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances",
        "isCorrect": false
      },
      {
        "text": "Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Cost Optimization Hub to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations\n\nCost Optimization Hub is an AWS Billing and Cost Management feature that helps you consolidate and prioritize cost optimization recommendations across your AWS accounts and AWS Regions, so that you can get the most out of your AWS spend. You can use Cost Optimization Hub to identify, filter, and aggregate AWS cost optimization recommendations across your AWS accounts and AWS Regions. It makes recommendations on resource rightsizing, idle resource deletion, Savings Plans, and Reserved Instances. With a single dashboard, you avoid having to go to multiple AWS products to identify cost optimization opportunities.\n\nAWS Compute Optimizer provides Amazon EC2 recommendations to help you improve performance, save money, or both. You can use these recommendations to decide whether to change to a new instance type. To make recommendations, Compute Optimizer analyzes your existing instance specifications and utilization metrics. The compiled data is then used to recommend which Amazon EC2 instance types are best able to handle the existing workload.\n\nIncorrect options:\n\nUse Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies - By using Amazon S3 Analytics Storage Class analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. Storage class analysis does not give recommendations for transitions to the ONEZONE_IA or S3 Glacier storage classes.\n\nUse AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances - AWS Trusted Advisor checks for Amazon EC2 Reserved Instances that are scheduled to expire within the next 30 days or have expired in the preceding 30 days. Reserved Instances do not renew automatically; you can continue using an Amazon EC2 instance covered by the reservation without interruption, but you will be charged On-Demand rates. AWS Trusted advisor does not have a feature to auto-renew Reserved Instances.\n\nUse AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs - AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Over-provisioning compute can lead to unnecessary infrastructure cost and under-provisioning compute can lead to poor application performance. Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, based on your utilization data. It does not recommend instance purchase options.\n\nReferences:\n\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/coh-optimization-strategies.html\n\nhttps://aws.amazon.com/compute-optimizer/\n\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/analytics-storage-class.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q526",
    "questionText": "A retail company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web application on Amazon EC2-based web servers and uses Amazon RDS PostgreSQL database as the data store. The PostgreSQL database is set up in a private subnet that allows inbound traffic from selected Amazon EC2 instances. The database also uses AWS Key Management Service (AWS KMS) for encrypting data at rest.\n\nWhich of the following steps would you recommend to facilitate end-to-end security for the data-in-transit while accessing the database?",
    "options": [
      {
        "text": "Use IAM authentication to access the database instead of the database user's access credentials",
        "isCorrect": false
      },
      {
        "text": "Configure Amazon RDS to use SSL for data in transit",
        "isCorrect": true
      },
      {
        "text": "Create a new security group that blocks SSH from the selected Amazon EC2 instances into the database",
        "isCorrect": false
      },
      {
        "text": "Create a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure Amazon RDS to use SSL for data in transit\n\nYou can use Secure Socket Layer / Transport Layer Security (SSL/TLS) connections to encrypt data in transit. Amazon RDS creates an SSL certificate and installs the certificate on the DB instance when the instance is provisioned. For MySQL, you launch the MySQL client using the --ssl_ca parameter to reference the public key to encrypt connections. Using SSL, you can encrypt a PostgreSQL connection between your applications and your PostgreSQL DB instances. You can also force all connections to your PostgreSQL DB instance to use SSL.\n\n\nvia - https://aws.amazon.com/rds/features/security/\n\nIncorrect options:\n\nUse IAM authentication to access the database instead of the database user's access credentials - You can authenticate to your database instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a database instance. Instead, you use an authentication token.\n\nIAM authentication is just another way to authenticate the user's credentials while accessing the database. It would not significantly enhance the security in a way that enabling SSL does by facilitating the in-transit encryption for the database.\n\nCreate a new security group that blocks SSH from the selected Amazon EC2 instances into the database\n\nCreate a new network access control list (network ACL) that blocks SSH from the entire Amazon EC2 subnet into the database\n\nBoth these options are added as distractors. You cannot SSH into an Amazon RDS database instance.\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/security/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.SSLSupport\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL\n\nhttps://aws.amazon.com/blogs/database/using-iam-authentication-to-connect-with-pgadmin-amazon-aurora-postgresql-or-amazon-rds-for-postgresql/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q527",
    "questionText": "A media company uses Amazon ElastiCache Redis to enhance the performance of its Amazon RDS database layer. The company wants a robust disaster recovery strategy for its caching layer that guarantees minimal downtime as well as minimal data loss while ensuring good application performance.\n\nWhich of the following solutions will you recommend to address the given use-case?",
    "options": [
      {
        "text": "Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure",
        "isCorrect": true
      },
      {
        "text": "Schedule daily automatic backups at a time when you expect low resource utilization for your cluster",
        "isCorrect": false
      },
      {
        "text": "Schedule manual backups using Redis append-only file (AOF)",
        "isCorrect": false
      },
      {
        "text": "Add read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nOpt for Multi-AZ configuration with automatic failover functionality to help mitigate failure\n\nMulti-AZ is the best option when data retention, minimal downtime, and application performance are a priority.\n\nData-loss potential - Low. Multi-AZ provides fault tolerance for every scenario, including hardware-related issues.\n\nPerformance impact - Low. Of the available options, Multi-AZ provides the fastest time to recovery, because there is no manual procedure to follow after the process is implemented.\n\nCost - Low to high. Multi-AZ is the lowest-cost option. Use Multi-AZ when you can't risk losing data because of hardware failure or you can't afford the downtime required by other options in your response to an outage.\n\nIncorrect options:\n\nSchedule daily automatic backups at a time when you expect low resource utilization for your cluster - Data loss potential is high, almost up to a day's worth of data. Hence, this is not the right option.\n\nSchedule manual backups using Redis append-only file (AOF) - Manual backups using AOF are retained indefinitely and are useful for testing and archiving. You can schedule manual backups to occur up to 20 times per node within any 24-hour period. Although AOF provides a measure of fault tolerance, it can't protect your data from a hardware-related cache node failure, so there is a risk of data loss.\n\nAdd read-replicas across multiple availability zones (AZs) to reduce the risk of potential data loss because of failure - To scale read capacity, Amazon ElastiCache allows you to add up to five read replicas across multiple availability zones. Read replicas are used to ease out read traffic from the primary database and cannot be used as a complete fault-tolerant solution in itself.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/FaultTolerance.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q528",
    "questionText": "An IT company has a large number of clients opting to build their application programming interface (API) using Docker containers. To facilitate the hosting of these containers, the company is looking at various orchestration services available with AWS.\n\nAs a Solutions Architect, which of the following solutions will you suggest? (Select two)",
    "options": [
      {
        "text": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services",
        "isCorrect": true
      },
      {
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EMR for serverless orchestration of the containerized services",
        "isCorrect": false
      },
      {
        "text": "Use Amazon SageMaker for serverless orchestration of the containerized services",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nUse Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for serverless orchestration of the containerized services\n\nUse Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for serverless orchestration of the containerized services\n\nBuilding APIs with Docker containers has been gaining momentum over the years. For hosting and exposing these container-based APIs, they need a solution which supports HTTP requests routing, autoscaling, and high availability. In some cases, user authorization is also needed.\n\nFor this purpose, many organizations are orchestrating their containerized services with Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS), while hosting their containers on Amazon EC2 or AWS Fargate. Then, they can add scalability and high availability with Service Auto Scaling (in Amazon ECS) or Horizontal Pod Auto Scaler (in Amazon EKS), and they expose the services through load balancers.\n\nWhen you use Amazon ECS as an orchestrator (with EC2 or Fargate launch type), you also have the option to expose your services with Amazon API Gateway and AWS Cloud Map instead of a load balancer. AWS Cloud Map is used for service discovery: no matter how Amazon ECS tasks scale, AWS Cloud Map service names would point to the right set of Amazon ECS tasks. Then, API Gateway HTTP APIs can be used to define API routes and point them to the corresponding AWS Cloud Map services.\n\nIncorrect options:\n\nUse Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for serverless orchestration of the containerized services - Amazon EC2 can be used to host the container services. Amazon EC2 needs hosting and management of the instance, hence does not come under serverless solution. Fargate can be used for serverless container solutions.\n\nUse Amazon EMR for serverless orchestration of the containerized services - Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. It utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon EC2 and Amazon S3. EMR is not a docker orchestration service, as required for the use case.\n\nUse Amazon SageMaker for serverless orchestration of the containerized services - Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML. A powerful tool, SageMaker is not a docker orchestration service, as required for the use case.\n\nReference:\n\nhttps://aws.amazon.com/blogs/architecture/field-notes-serverless-container-based-apis-with-amazon-ecs-and-amazon-api-gateway/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q529",
    "questionText": "A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies.\n\nWhich is the only resource-based policy that the IAM service supports?",
    "options": [
      {
        "text": "AWS Organizations Service Control Policies (SCP)",
        "isCorrect": false
      },
      {
        "text": "Trust policy",
        "isCorrect": true
      },
      {
        "text": "Access control list (ACL)",
        "isCorrect": false
      },
      {
        "text": "Permissions boundary",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nYou manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.\nResource-based policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies.\n\nTrust policy\n\nTrust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. For this reason, you must attach both a trust policy and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.\n\nIncorrect options:\n\nAWS Organizations Service Control Policies (SCP) - If you enable all features of AWS organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides the allow.\n\nAccess control list (ACL) - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs.\n\nPermissions boundary - AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html",
    "awsService": "IAM",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q530",
    "questionText": "A CRM company has a software as a service (SaaS) application that feeds updates to other in-house and third-party applications. The SaaS application and the in-house applications are being migrated to use AWS services for this inter-application communication.\n\nAs a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?",
    "options": [
      {
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to decouple the architecture",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EventBridge to decouple the system architecture",
        "isCorrect": true
      },
      {
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture",
        "isCorrect": false
      },
      {
        "text": "Use Elastic Load Balancing (ELB) for effective decoupling of system architecture",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon EventBridge to decouple the system architecture\n\nBoth Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, but for this use case, EventBridge is the right fit.\n\nAmazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners. Amazon EventBridge also automatically ingests events from over 90 AWS services without requiring developers to create any resources in their account. Further, Amazon EventBridge uses a defined JSON-based structure for events and allows you to create rules that are applied across the entire event body to select events to forward to a target. Amazon EventBridge currently supports over 15 AWS services as targets, including AWS Lambda, Amazon SQS, Amazon SNS, and Amazon Kinesis Streams and Firehose, among others. At launch, Amazon EventBridge is has limited throughput (see Service Limits) which can be increased upon request, and typical latency of around half a second.\n\nHow Amazon EventBridge works:\n\nvia - https://aws.amazon.com/eventbridge/\n\nIncorrect options:\n\nUse Amazon Simple Notification Service (Amazon SNS) to communicate between systems and decouple the architecture - As discussed above, Amazon SNS can be used for event-based services. But, our use case needs integration with third-party SaaS services, hence Amazon EventBridge is the right choice, as Amazon SNS does not support third-party services integration.\n\nUse Amazon Simple Queue Service (Amazon SQS) to decouple the architecture - Amazon SQS is a message queuing service from amazon and works well for decoupling applications. It does not directly integrate with third-party SaaS services.\n\nUse Elastic Load Balancing (ELB) for effective decoupling of system architecture - Elastic Load Balancing (ELB) offers a synchronous decoupling of applications, which is not the right fit for the current use case.\n\nReferences:\n\nhttps://aws.amazon.com/eventbridge/\n\nhttps://aws.amazon.com/sns/",
    "awsService": "ELB",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q531",
    "questionText": "An enterprise has decided to move its secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current tape backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move this data to cloud.\n\nWhat is an optimal solution that meets these requirements while keeping the costs to a minimum?",
    "options": [
      {
        "text": "Use Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years",
        "isCorrect": true
      },
      {
        "text": "Use AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes",
        "isCorrect": false
      },
      {
        "text": "Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs",
        "isCorrect": false
      },
      {
        "text": "Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Tape Gateway, which can be used to move on-premises tape data onto AWS Cloud. Then, Amazon S3 archiving storage classes can be used to store data cost-effectively for years\n\nTape Gateway enables you to replace using physical tapes on-premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on-premises for low-latency data access. Tape Gateway encrypts data between the gateway and AWS for secure data transfer and compresses data while transitioning virtual tapes between Amazon S3 and Amazon S3 Glacier, or Amazon S3 Glacier Deep Archive, to minimize storage costs.\n\nTape Gateway compresses and stores archived virtual tapes in the lowest-cost Amazon S3 storage classes, Amazon S3 Glacier and Amazon S3 Glacier Deep Archive. This makes it feasible for you to retain long-term data in the AWS Cloud at a very low cost. With Tape Gateway, you only pay for what you consume, with no minimum commitments and no upfront fees.\n\nTape Gateway stores your virtual tapes in S3 buckets managed by the AWS Storage Gateway service, so you donâ€™t have to manage your own Amazon S3 storage. Tape Gateway integrates with all leading backup applications allowing you to start using cloud storage for on-premises backup and archive without any changes to your backup and archive workflows.\n\nTape Gateway Overview:\n\nvia - https://aws.amazon.com/storagegateway/vtl/\n\nIncorrect options:\n\nUse AWS DataSync, which makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes - AWS DataSync supports only NFS and SMB file types and hence is not the right choice for the given use case.\n\nUse AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs - AWS Direct Connect is used when customers need to retain on-premises structure because of compliance reasons and have moved the rest of the architecture to AWS Cloud. These businesses generally have an on-going requirement for low latency access to AWS Cloud and hence are willing to spend on installing the physical lines needed for this connection. The given use-case needs a cost-optimized solution and they do not have an ongoing requirement for high availability bandwidth.\n\nUse AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources - VPN connection is used when businesses have an on-going requirement for connectivity from the on-premises data center to AWS Cloud. Amazon EFS is a managed file system by AWS and cannot be used for archiving on-premises tape data onto AWS Cloud.\n\nReferences:\n\nhttps://aws.amazon.com/storagegateway/vtl/\n\nhttps://aws.amazon.com/storagegateway/faqs/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q532",
    "questionText": "A healthcare provider is experiencing rapid data growth in its on-premises servers due to increased patient imaging and record retention requirements. The organization wants to extend its storage capacity to AWS in a way that preserves quick access to critical records, including from its local file systems. The company must optimize bandwidth usage during migration and avoid any retrieval fees or delays when accessing the data in the cloud. The provider wants a hybrid cloud solution that requires minimal application reconfiguration, allows frequent local access to key datasets, and ensures that cloud storage costs remain predictable without paying extra for data retrieval.\n\nWhich AWS solution best meets these requirements?",
    "options": [
      {
        "text": "Implement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement",
        "isCorrect": false
      },
      {
        "text": "Set up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates",
        "isCorrect": false
      },
      {
        "text": "Deploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3",
        "isCorrect": true
      },
      {
        "text": "Deploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDeploy AWS Storage Gateway using cached volumes. Store frequently accessed data locally, while writing all primary data asynchronously to Amazon S3\n\nAWS Storage Gateway with cached volumes is ideal for hybrid storage environments where the application needs low-latency local access to frequently used files, but most of the data is migrated to Amazon S3. Cached volumes minimize bandwidth usage by storing only frequently accessed data subsets locally. Data stored in S3 via cached volumes is accessible without additional retrieval charges, and the gateway keeps local disk usage low while extending cloud capacity. This architecture supports immediate access to active data and cost-effective storage for archived or less-frequent data.\n\n\nvia - https://docs.aws.amazon.com/storagegateway/latest/vgw/StorageGatewayConcepts.html\n\nIncorrect options:\n\nImplement Amazon FSx for Windows File Server and configure on-premises servers to mount the file system using a VPN connection. Store all primary data in FSx and use it as the central NAS replacement - Amazon FSx is a managed Windows file system that supports SMB access, but itâ€™s designed as a replacement for on-prem NAS and requires consistent, low-latency network connectivity. Using it as the primary file system over VPN introduces latency and availability risks for frequent local access. It also does not minimize bandwidth, since all reads and writes must go over the network.\n\nSet up Amazon S3 Standard-Infrequent Access (S3 Standard-IA) as the primary storage tier. Configure the on-premises file server to replicate changes to the S3 bucket using AWS DataSync for asynchronous updates - S3 Standard-IA provides lower-cost storage but is intended for infrequently accessed data, and incurs retrieval fees for each access. Additionally, DataSync supports periodic data replication but is not a real-time solution for serving immediate local requests. This option fails both the retrieval cost and latency requirements.\n\nDeploy AWS Storage Gateway using stored volumes. Retain the full dataset on-premises and asynchronously back up point-in-time snapshots to Amazon S3. Configure applications to read from the local volume and recover data from the cloud if needed - AWS Storage Gateway with stored volumes keeps the entire primary dataset on-premises and uses asynchronous snapshots to back up data to Amazon S3. While this provides a reliable backup mechanism, it does not reduce local storage requirements, since all data must be retained onsite. Furthermore, data stored in S3 via snapshots is not directly accessible for application useâ€”it must be restored back to the on-premises environment first. This approach fails the goals of cloud migration, immediate retrieval from S3, and bandwidth optimization, making it unsuitable for the scenario.\n\n\nvia - https://docs.aws.amazon.com/storagegateway/latest/vgw/StorageGatewayConcepts.html\n\nReference:\n\nhttps://docs.aws.amazon.com/storagegateway/latest/vgw/StorageGatewayConcepts.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q533",
    "questionText": "A company has recently created a new department to handle their services workload. An IT team has been asked to create a custom VPC to isolate the resources created in this new department. They have set up the public subnet and internet gateway (IGW). However, they are not able to ping the Amazon EC2 instances with elastic IP address (EIP) launched in the newly created VPC.\n\nAs a Solutions Architect, the team has requested your help. How will you troubleshoot this scenario? (Select two)",
    "options": [
      {
        "text": "Disable Source / Destination check on the Amazon EC2 instance",
        "isCorrect": false
      },
      {
        "text": "Check if the security groups allow ping from the source",
        "isCorrect": true
      },
      {
        "text": "Contact AWS support to map your VPC with subnet",
        "isCorrect": false
      },
      {
        "text": "Create a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables",
        "isCorrect": false
      },
      {
        "text": "Check if the route table is configured with internet gateway",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nCheck if the route table is configured with internet gateway\n\nAn internet gateway (IGW) is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. An internet gateway supports IPv4 and IPv6 traffic.\n\nTo enable access to or from the internet for instances in a subnet in a VPC, you must do the following:\n    1. Attach an internet gateway to your VPC.\n    2. Add a route to your subnet's route table that directs internet-bound traffic to the internet gateway.\n    3. Ensure that instances in your subnet have a globally unique IP address\n    4. Ensure that your network access control lists and security group rules allow the relevant traffic to flow to and from your instance.\n\nA route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. After creating an IGW, make sure the route tables are updated. Additionally, ensure the security group allows the ICMP protocol for ping requests.\n\nCheck if the security groups allow ping from the source\n\nA security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, AWS uses the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. To decide whether to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated.\n\nThe following are the characteristics of security group rules:\n    1. By default, security groups allow all outbound traffic.\n    2. Security group rules are always permissive; you can't create rules that deny access.\n    3. Security groups are stateful\n\nIncorrect options:\n\nDisable Source / Destination check on the Amazon EC2 instance - The Source/Destination Check attribute controls whether source/destination checking is enabled on the instance. Disabling this attribute enables an instance to handle network traffic that isn't specifically destined for the instance. For example, instances running services such as network address translation, routing, or a firewall should set this value to disabled. The default value is enabled. Source/Destination Check is not relevant to the question and it has been added as a distractor.\n\nCreate a secondary internet gateway to attach with public subnet and move the current internet gateway to private and write route tables - There is no such thing as a secondary IGW. This option is added as a distractor.\n\nContact AWS support to map your VPC with subnet - You cannot contact AWS support to map your VPC with the subnet.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#change_source_dest_check",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q534",
    "questionText": "A company runs a popular dating website on the AWS Cloud. As a Solutions Architect, you've designed the architecture of the website to follow a serverless pattern on the AWS Cloud using Amazon API Gateway and AWS Lambda. The backend uses an Amazon RDS PostgreSQL database. Currently, the application uses a username and password combination to connect the AWS Lambda function to the Amazon RDS database.\n\nYou would like to improve the security at the authentication level by leveraging short-lived credentials. What will you choose? (Select two)",
    "options": [
      {
        "text": "Embed a credential rotation logic in the AWS Lambda, retrieving them from SSM",
        "isCorrect": false
      },
      {
        "text": "Use IAM authentication from AWS Lambda to Amazon RDS PostgreSQL",
        "isCorrect": true
      },
      {
        "text": "Restrict the Amazon RDS database security group to the AWS Lambda's security group",
        "isCorrect": false
      },
      {
        "text": "Deploy AWS Lambda in a VPC",
        "isCorrect": false
      },
      {
        "text": "Attach an AWS Identity and Access Management (IAM) role to AWS Lambda",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nUse IAM authentication from AWS Lambda to Amazon RDS PostgreSQL\n\nAttach an AWS Identity and Access Management (IAM) role to AWS Lambda\n\nYou can authenticate to your database instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a database instance. Instead, you use an authentication token.\n\nAn authentication token is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.\n\nIAM database authentication provides the following benefits:\n    1. Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL).\n    2. You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance.\n    3. For applications running on Amazon EC2, you can use profile credentials specific to your Amazon EC2 instance to access your database instead of a password, for greater security.\n\nIncorrect options:\n\nAWS Systems Manager Parameter Store (aka SSM Parameter Store) provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon EC2 instance IDs, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data.\n\nEmbed a credential rotation logic in the AWS Lambda, retrieving them from SSM - Retrieving credentials from SSM is overkill for the expected solution and hence this is not a correct option.\n\nRestrict the Amazon RDS database security group to the AWS Lambda's security group\n\nDeploy AWS Lambda in a VPC\n\nThis question is very tricky because all answers do indeed increase security. But the question is related to authentication mechanisms, and as such, deploying an AWS Lambda in a VPC or tightening security groups does not change the authentication layer. IAM authentication to Amazon RDS is supported, which must be achieved by attaching an IAM role the AWS Lambda function\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q535",
    "questionText": "A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes.\n\nAs a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?",
    "options": [
      {
        "text": "Use Secure Sockets Layer certificate (SSL certificate) with SNI",
        "isCorrect": true
      },
      {
        "text": "Use a wildcard Secure Sockets Layer certificate (SSL certificate)",
        "isCorrect": false
      },
      {
        "text": "Use an HTTP to HTTPS redirect",
        "isCorrect": false
      },
      {
        "text": "Change the Elastic Load Balancing (ELB) SSL Security Policy",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Secure Sockets Layer certificate (SSL certificate) with SNI\n\nYou can host multiple TLS secured applications, each with its own TLS certificate, behind a single load balancer. To use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client.\n\nALBâ€™s smart certificate selection goes beyond SNI. In addition to containing a list of valid domain names, certificates also describe the type of key exchange and cryptography that the server supports, as well as the signature algorithm (SHA2, SHA1, MD5) used to sign the certificate.\n\nWith SNI support AWS makes it easy to use more than one certificate with the same ALB. The most common reason you might want to use multiple certificates is to handle different domains with the same load balancer. Itâ€™s always been possible to use wildcard and subject-alternate-name (SAN) certificates with ALB, but these come with limitations. Wildcard certificates only work for related subdomains that match a simple pattern and while SAN certificates can support many different domains, the same certificate authority has to authenticate each one. That means you have to reauthenticate and reprovision your certificate every time you add a new domain.\n\nIncorrect options:\n\nUse a wildcard Secure Sockets Layer certificate (SSL certificate) - As the use case requires different domain names, so you cannot use a wildcard SSL certificate.\n\nUse an HTTP to HTTPS redirect - This will not provide multiple secure endpoints for different URLs such as checkout.mycorp.com or www.mycorp.com, therefore it is incorrect for the given use-case.\n\nChange the Elastic Load Balancing (ELB) SSL Security Policy - Elastic Load Balancing (ELB) SSL Security Policy will not provide multiple secure endpoints for different URLs such as checkout.mycorp.com or www.mycorp.com, therefore it is incorrect for the given use-case.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q536",
    "questionText": "A financial services firm has traditionally operated with an on-premise data center and would like to create a disaster recovery strategy leveraging the AWS Cloud.\n\nAs a Solutions Architect, you would like to ensure that a scaled-down version of a fully functional environment is always running in the AWS cloud, and in case of a disaster, the recovery time is kept to a minimum. Which disaster recovery strategy is that?",
    "options": [
      {
        "text": "Warm Standby",
        "isCorrect": true
      },
      {
        "text": "Pilot Light",
        "isCorrect": false
      },
      {
        "text": "Backup and Restore",
        "isCorrect": false
      },
      {
        "text": "Multi Site",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nWarm Standby\n\nThe term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. A warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems on AWS and have them always on.\n\nIncorrect options:\n\nBackup and Restore - In most traditional environments, data is backed up to tape and sent off-site regularly. If you use this method, it can take a long time to restore your system in the event of a disruption or disaster. Amazon S3 is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location. Many commercial and open-source backup solutions integrate with Amazon S3.\n\nPilot Light - The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame thatâ€™s always on can quickly ignite the entire furnace to heat up a house. This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core.\n\nMulti Site - A multi-site solution runs in AWS as well as on your existing on-site infrastructure, in an active-active configuration. The data replication method that you employ will be determined by the recovery point that you choose.\n\nReferences:\n\nhttps://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf\n\nhttps://d1.awsstatic.com/asset-repository/products/CloudEndure/CloudEndure_Affordable_Enterprise-Grade_Disaster_Recovery_Using_AWS.pdf",
    "awsService": "General",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q537",
    "questionText": "A fintech startup hosts its real-time transaction metadata in Amazon DynamoDB tables. During a recent system maintenance event, a junior engineer accidentally deleted a production table, resulting in major service downtime and irreversible data loss. Leadership has mandated an immediate solution that prevents future data loss from human error, while requiring minimal ongoing maintenance or manual effort from the engineering team.\n\nWhich approach best addresses these requirements with the least operational overhead?",
    "options": [
      {
        "text": "Enable point-in-time recovery (PITR) on each DynamoDB table",
        "isCorrect": false
      },
      {
        "text": "Enable deletion protection on DynamoDB tables",
        "isCorrect": true
      },
      {
        "text": "Configure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3",
        "isCorrect": false
      },
      {
        "text": "Manually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nEnable deletion protection on DynamoDB tables\n\nEnabling deletion protection on DynamoDB tables is the most effective, proactive, and low-overhead approach to prevent accidental table deletion. This feature is designed specifically to avoid destructive actions like the one that caused the outage in the scenario. It is easy to configure, does not require ongoing management, and is ideal for mission-critical data that must remain intact. The deletion protection feature enables you to protect tables from accidental deletion during regular DynamoDB maintenance operations. You can use deletion protection to set whether a table can or cannot be deleted during regular maintenance operations. This flexibility helps ensure that mission-critical tables are protected from deletion.\n\nIncorrect options:\n\nEnable point-in-time recovery (PITR) on each DynamoDB table - PITR enables you to restore a table to any point in time within the last 35 days, but it does not prevent deletion. If a table is accidentally deleted, you can restore its data using PITR, but this still involves manual recovery operations and does not eliminate downtime or prevent disruption. While useful for data corruption recovery, PITR is reactive rather than preventive.\n\nConfigure AWS CloudTrail to monitor DynamoDB API calls. Set up an Amazon EventBridge rule to detect DeleteTable events and trigger a Lambda function that recreates the deleted table using backup data stored in Amazon S3 - This setup is reactive and depends on a custom Lambda-based recovery workflow. While it can detect a table deletion using CloudTrail and EventBridge, by the time the deletion is logged, the table and its data are already lost. Even with backup mechanisms in place, restoring the exact configuration and data immediately is complex and error-prone. This approach introduces high operational overhead without guaranteed results.\n\nManually export each table as a full backup to Amazon S3 on a weekly basis. Use the DynamoDB export to S3 feature and rely on manual recovery if tables are deleted - Exporting backups to S3 manually on a schedule introduces operational overhead, and weekly frequency may lead to substantial data loss between backups. Additionally, recovery is a manual process, which is error-prone and time-consuming, especially in an incident response situation.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/database/how-to-use-deletion-protection-to-enhance-your-amazon-dynamodb-table-protection-strategy/\n\nhttps://aws.amazon.com/dynamodb/pitr/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q538",
    "questionText": "You have an Amazon S3 bucket that contains files in two different folders - s3://my-bucket/images and s3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. But after 45 days, analytics prove that image files are on average rarely requested, but the thumbnails still are. After 180 days, you would like to archive the image files and the thumbnails. Overall you would like the solution to remain highly available to prevent disasters happening against a whole Availability Zone (AZ).\n\nHow can you implement an efficient cost strategy for your Amazon S3 bucket? (Select two)",
    "options": [
      {
        "text": "Create a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days",
        "isCorrect": false
      },
      {
        "text": "Create a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days",
        "isCorrect": true
      },
      {
        "text": "Create a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days",
        "isCorrect": true
      },
      {
        "text": "Create a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days",
        "isCorrect": false
      },
      {
        "text": "Create a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nTo manage your S3 objects, so they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:\n\nTransition actions â€” Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.\n\nExpiration actions â€” Define when objects expire. Amazon S3 deletes expired objects on your behalf.\n\nCreate a Lifecycle Policy to transition objects to Amazon S3 Standard IA using a prefix after 45 days\n\nAmazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per gigabyte storage price and per gigabyte retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.\n\nAs the use-case mentions that after 45 days, image files are rarely requested, but the thumbnails still are. So you need to use a prefix while configuring the Lifecycle Policy so that only objects in the s3://my-bucket/images are transitioned to Standard IA and not all the objects in the bucket.\n\nCreate a Lifecycle Policy to transition all objects to Amazon S3 Glacier after 180 days\n\nAmazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.\n\nIncorrect options:\n\nCreate a Lifecycle Policy to transition all objects to Amazon S3 Standard IA after 45 days - As discussed above, you need to use a prefix while configuring the Lifecycle Policy so that only objects in the s3://my-bucket/images are transitioned to Amazon S3 Standard IA and not all the objects in the bucket.\n\nCreate a Lifecycle Policy to transition objects to Amazon S3 Glacier using a prefix after 180 days - After 180 days, you can move all the objects to Amazon S3 Glacier storage as per the use case. Glacier doesn't need prefixes for the given use-case.\n\nCreate a Lifecycle Policy to transition objects to Amazon S3 One Zone IA using a prefix after 45 days - Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.\n\nAmazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. The minimum storage duration charge is 30 days.\n\nFinally, Amazon S3 One Zone IA will not achieve the necessary availability in case an Availability Zone (AZ) goes down.\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q539",
    "questionText": "You have developed a new REST API leveraging the Amazon API Gateway, AWS Lambda and Amazon Aurora database services. Most of the workload on the website is read-heavy. The data rarely changes and it is acceptable to serve users outdated data for about 24 hours. Recently, the website has been experiencing high load and the costs incurred on the Aurora database have been very high.\n\nHow can you easily reduce the costs while improving performance, with minimal changes?",
    "options": [
      {
        "text": "Add Amazon Aurora Read Replicas",
        "isCorrect": false
      },
      {
        "text": "Enable Amazon API Gateway Caching",
        "isCorrect": true
      },
      {
        "text": "Enable AWS Lambda In Memory Caching",
        "isCorrect": false
      },
      {
        "text": "Switch to using an Application Load Balancer",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nEnable Amazon API Gateway Caching\n\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.\n\nYou can enable Amazon API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.\nWhen you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. Amazon API Gateway then responds to the request by looking up the endpoint response from the cache instead of requesting your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled. Using API Gateway Caching feature is the answer for the use case, as we can accept stale data for about 24 hours.\n\nIncorrect options:\n\nAdd Amazon Aurora Read Replicas - Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).\n\nAmazon Aurora Read Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and to Aurora Replicas in the DB cluster. Adding Aurora Read Replicas would greatly increase the cost, therefore this option is ruled out.\n\nSwitch to using an Application Load Balancer - An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply, and then selects a target from the target group for the rule action. You can configure listener rules to route requests to different target groups based on the content of the application traffic. Switching to a Load Balancer would not improve the current status as we need a caching mechanism.\n\nEnable AWS Lambda In Memory Caching - AWS Lambda has no native in-memory caching capability. AWS Lambda is a serverless compute capacity. This option is incorrect and has been added as a distractor.\n\nReference:\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q540",
    "questionText": "A small rental company had 5 employees, all working under the same AWS cloud account. These employees deployed their applications built for various functions- including billing, operations, finance, etc. Each of these employees has been operating in their own VPC. Now, there is a need to connect these VPCs so that the applications can communicate with each other.\n\nWhich of the following is the MOST cost-effective solution for this use-case?",
    "options": [
      {
        "text": "Use an AWS Direct Connect connection",
        "isCorrect": false
      },
      {
        "text": "Use a VPC peering connection",
        "isCorrect": true
      },
      {
        "text": "Use an Internet Gateway",
        "isCorrect": false
      },
      {
        "text": "Use a Network Address Translation gateway (NAT gateway)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse a VPC peering connection\n\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection). VPC Peering helps connect two VPCs and is not transitive. To connect VPCs together, the best available option is to use VPC peering.\n\nMore on VPC Peering:\n\nvia - https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n\nIncorrect options:\n\nUse an Internet Gateway - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. Internet Gateway is not meant for connecting between VPCs.\n\nUse an AWS Direct Connect connection - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. For the given use-case, direct connect gateway is overkill and is not as cost-optimal as using VPC peering.\n\nUse a Network Address Translation gateway (NAT gateway) - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. You are charged for creating and using a NAT gateway in your account. NAT gateway hourly usage and data processing rates apply. NAT Gateway is not used for connection between VPCs.\n\nReference:\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q541",
    "questionText": "You are working as a Solutions Architect for a photo processing company that has a proprietary algorithm to compress an image without any loss in quality. Because of the efficiency of the algorithm, your clients are willing to wait for a response that carries their compressed images back. You also want to process these jobs asynchronously and scale quickly, to cater to the high demand. Additionally, you also want the job to be retried in case of failures.\n\nWhich combination of choices do you recommend to minimize cost and comply with the requirements? (Select two)",
    "options": [
      {
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "isCorrect": false
      },
      {
        "text": "Amazon EC2 Spot Instances",
        "isCorrect": true
      },
      {
        "text": "Amazon Simple Queue Service (Amazon SQS)",
        "isCorrect": true
      },
      {
        "text": "Amazon EC2 Reserved Instances (RIs)",
        "isCorrect": false
      },
      {
        "text": "Amazon EC2 On-Demand Instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nAmazon EC2 Spot Instances\n\nA Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone (AZ) is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price.\n\nTo process these jobs, due to the unpredictable nature of their volume, and the desire to save on costs, spot Instances are recommended as compared to on-demand instances. As spot instances are cheaper than reserved instances and do not require long term commitment, spot instances are a better fit for the given use-case.\n\nAmazon EC2 Instance purchasing options:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\n\nAmazon Simple Queue Service (Amazon SQS)\n\nAmazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO (First-In-First-out) queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\nAmazon SQS will allow you to buffer the image compression requests and process them asynchronously. It also has a direct built-in mechanism for retries and scales seamlessly.\n\nIncorrect options:\n\nAmazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS is not the right fit for this use-case, since its not a queuing mechanism.\n\nAmazon EC2 Reserved Instances (RIs) - Reserved instances (RIs) reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years. For the given use case, this kind of annual commitment might not be a desirable option.\n\nAmazon EC2 On-Demand Instances - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycleâ€”you decide when to launch, stop, hibernate, start, reboot, or terminate it. There is no long-term commitment required when you purchase On-Demand Instances. You pay only for the seconds that your On-Demand Instances are running. AWS recommends that you use On-Demand Instances for applications with short-term, irregular workloads that cannot be interrupted.\n\nReferences:\n\nhttps://aws.amazon.com/sqs/\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q542",
    "questionText": "You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created Amazon RDS database which resulted in a production outage.\n\nHow can you ensure that Amazon RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?",
    "options": [
      {
        "text": "Store your recommendations in a custom AWS Trusted Advisor rule",
        "isCorrect": false
      },
      {
        "text": "Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases",
        "isCorrect": false
      },
      {
        "text": "Use AWS CloudFormation to manage Amazon RDS databases",
        "isCorrect": true
      },
      {
        "text": "Attach an IAM policy to interns preventing them from creating an Amazon RDS database",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS CloudFormation to manage Amazon RDS databases\n\nAWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. AWS CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This gives you a single source of truth for your AWS and third-party resources.\n\nAWS CloudFormation allows you to keep your infrastructure as code and re-use the best practices around your company for configuration parameters. Therefore, this is the correct option for the given use-case.\n\nIncorrect options:\n\nStore your recommendations in a custom AWS Trusted Advisor rule - AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. Whether establishing new workflows, developing applications, or as part of ongoing improvement, take advantage of the recommendations provided by AWS Trusted Advisor regularly to help keep your solutions provisioned optimally. AWS Trusted Advisor just provides recommendations rather than creating reusable infrastructure templates.\n\nCreate an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases - Using an AWS Lambda function to scan for a misconfigured Amazon RDS database is a reactive mechanism. It does not help in creating reusable infrastructure templates.\n\nAttach an IAM policy to interns preventing them from creating an Amazon RDS database - Using an IAM policy to prevent interns from creating an Amazon RDS database does not solve the problem of allowing any user to create resources by leveraging reusable infrastructure templates. So, this option is ruled out.\n\nReferences:\n\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/\n\nhttps://aws.amazon.com/cloudformation/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q543",
    "questionText": "A company wants to adopt a hybrid cloud infrastructure where it uses some AWS services such as Amazon S3 alongside its on-premises data center. The company wants a dedicated private connection between the on-premise data center and AWS. In case of failures though, the company needs to guarantee uptime and is willing to use the public internet for an encrypted connection.\n\nWhat do you recommend? (Select two)",
    "options": [
      {
        "text": "Use AWS Direct Connect connection as a primary connection",
        "isCorrect": true
      },
      {
        "text": "Use AWS Site-to-Site VPN as a primary connection",
        "isCorrect": false
      },
      {
        "text": "Use Egress Only Internet Gateway as a backup connection",
        "isCorrect": false
      },
      {
        "text": "Use AWS Site-to-Site VPN as a backup connection",
        "isCorrect": true
      },
      {
        "text": "Use AWS Direct Connect connection as a backup connection",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nUse AWS Direct Connect connection as a primary connection\n\nAWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC.\n\nUse AWS Site-to-Site VPN as a backup connection\n\nAWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.\n\nAWS Direct Connect as a primary connection guarantees great performance and security (as the connection is private). Using Direct Connect as a backup solution would work but probably carries a risk it would fail as well. As we don't mind going over the public internet (which is reliable, but less secure as connections are going over the public route), we should use a Site to Site VPN which offers an encrypted connection to handle failover scenarios.\n\nIncorrect options:\n\nUse Egress Only Internet Gateway as a backup connection - An Egress-Only Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances. Egress-Only Internet Gateway cannot be used to connect on-premises data centers to AWS Cloud.\n\nUse AWS Site-to-Site VPN as a primary connection - AWS Site-to-Site VPN as a primary connection is not advisable since the use of internet-based connection is only for failover scenarios, as stated in the problem.\n\nUse AWS Direct Connect connection as a backup connection - AWS Direct Connect connection is a highly secure, physical connection. It is also a costly solution and hence does not make much sense to set up the connection and keep it only as a backup.\n\nReferences:\n\nhttps://aws.amazon.com/directconnect/\n\nhttps://aws.amazon.com/vpn/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q544",
    "questionText": "As an e-sport tournament hosting company, you have servers that need to scale and be highly available. Therefore you have deployed an Elastic Load Balancing (ELB) with an Auto Scaling group (ASG) across 3 Availability Zones (AZs). When e-sport tournaments are running, the servers need to scale quickly. And when tournaments are done, the servers can be idle. As a general rule, you would like to be highly available, have the capacity to scale and optimize your costs.\n\nWhat do you recommend? (Select two)",
    "options": [
      {
        "text": "Set the minimum capacity to 1",
        "isCorrect": false
      },
      {
        "text": "Set the minimum capacity to 3",
        "isCorrect": false
      },
      {
        "text": "Set the minimum capacity to 2",
        "isCorrect": true
      },
      {
        "text": "Use Dedicated hosts for the minimum capacity",
        "isCorrect": false
      },
      {
        "text": "Use Reserved Instances (RIs) for the minimum capacity",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nSet the minimum capacity to 2\n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.\n\nYou configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.\n\nAn Auto Scaling group is elastic as long as it has different values for minimum and maximum capacity. All requests to change the Auto Scaling group's desired capacity (either by manual scaling or automatic scaling) must fall within these limits.\n\nHere, even though our ASG is deployed across 3 Availability Zones (AZs), the minimum capacity to be highly available is 2. When we specify 2 as the minimum capacity, the ASG would create these 2 instances in separate Availability Zones (AZs). If demand goes up, the ASG would spin up a new instance in the third Availability Zone (AZ). Later as the demand subsides, the ASG would scale-in and the instance count would be back to 2.\n\nUse Reserved Instances (RIs) for the minimum capacity\n\nReserved Instances (RIs) provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. Since minimum capacity will always be maintained, it is cost-effective to choose reserved instances than any other option.\n\nIn case of an Availability Zone (AZ) outage, the instance in that Availability Zone (AZ) would go down however the other instance would still be available. The ASG would provision the replacement instance in the third Availability Zone (AZ) to keep the minimum count to 2.\n\nIncorrect options:\n\nSet the minimum capacity to 1 - This is not failure proof, since only one instance will be maintained consistently and this will be from only one Availability Zone (AZ).\n\nSet the minimum capacity to 3 - This is not a cost-effective option, as two instances in two different Availability Zones (AZs) are enough to make the architecture disaster-proof.\n\nUse Dedicated hosts for the minimum capacity - As there is no use-case to utilize existing per-socket, per-core, or per-VM software licenses or to run the instance on a dedicated physical host, so the option to use dedicated hosts for the minimum capacity is ruled out.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html",
    "awsService": "ELB",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q545",
    "questionText": "A ride-hailing startup has launched a mobile app that matches passengers with nearby drivers based on real-time GPS coordinates. The application backend uses an Amazon RDS for PostgreSQL instance with read replicas to store the latitude and longitude of drivers and passengers. As the service scales, the backend experiences performance bottlenecks during peak hours, especially when thousands of updates and reads occur per second to keep location data current. The company expects its user base to double in the next few months and needs a high-performance, scalable solution that can handle frequent write and read operations with minimal latency.\n\nWhat do you recommend?",
    "options": [
      {
        "text": "Create a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration",
        "isCorrect": false
      },
      {
        "text": "Migrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards",
        "isCorrect": false
      },
      {
        "text": "Place an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy",
        "isCorrect": true
      },
      {
        "text": "Enable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nPlace an Amazon ElastiCache for Redis cluster in front of the PostgreSQL database. Modify the application to cache recent location reads and updates in Redis, using a TTL-based eviction strategy\n\nThis solution is the most cost-effective and scalable approach for improving the performance of a real-time location tracking workload where frequent updates and reads are required per second.\n\nAmazon ElastiCache for Redis is a fully managed, in-memory data store that offers sub-millisecond latency for read and write operations. It is ideal for workloads that involve frequent, short-lived data interactions, such as tracking a user's changing GPS coordinates in real time. Instead of hitting the Amazon RDS for PostgreSQL database for every update or query, the application can cache these values temporarily in Redis.\n\nRedis natively supports geospatial data types and commands like GEOADD, GEORADIUS, and GEODIST, making it a perfect fit for storing and querying latitude and longitude data. This means the app can quickly retrieve all drivers within a radius of a user without putting pressure on the relational database.\n\nTo maintain data freshness and avoid memory bloat, a TTL (Time To Live) policy can be used on each cache entryâ€”ensuring that each player's location automatically expires after, say, 30 seconds unless refreshed. This TTL-based eviction strategy helps keep the dataset current while minimizing the Redis memory footprint.\n\nWorking with geospatial data in Amazon ElastiCache for Redis:\n\nvia - https://aws.amazon.com/blogs/database/working-with-geospatial-data-in-amazon-elasticache-for-redis/\n\nIncorrect options:\n\nCreate a read-replica Auto Scaling policy for the PostgreSQL database to dynamically add replicas during peak load. Distribute traffic evenly using an RDS proxy with failover configuration - While read replicas and RDS Proxy can improve read throughput and connection pooling, they do not reduce write load, which is the primary performance bottleneck in a real-time location tracking use case. Auto Scaling read replicas also introduce replication lag, making them unsuitable for real-time scenarios where immediate consistency is needed.\n\nMigrate the location data to Amazon OpenSearch Service and use its geospatial indexing features to retrieve and store coordinates in near real-time. Visualize tracking data using OpenSearch Dashboards - OpenSearch does offer geospatial querying, but it is not optimized for high-frequency, low-latency updates. It is a search and analytics engine, not a real-time transactional data store. Frequent writes can degrade OpenSearch cluster performance, and it's better suited for log analytics or location-based search, not rapid read/write cycles per second.\n\nEnable Multi-AZ deployment for the primary RDS instance to improve write resilience and fault tolerance. Use Multi-AZ standby failover to distribute reads during peak hours - Multi-AZ deployments improve availability and failover, not scalability or performance. It also does nothing to improve write throughput or reduce latency under normal operations.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/database/working-with-geospatial-data-in-amazon-elasticache-for-redis/\n\nhttps://aws.amazon.com/blogs/database/amazon-elasticache-utilizing-redis-geospatial-capabilities/\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q546",
    "questionText": "A company's business logic is built on several microservices that are running in the on-premises data center. They currently communicate using a message broker that supports the MQTT protocol. The company is looking at migrating these applications and the message broker to AWS Cloud without changing the application logic.\n\nWhich technology allows you to get a managed message broker that supports the MQTT protocol?",
    "options": [
      {
        "text": "Amazon MQ",
        "isCorrect": true
      },
      {
        "text": "Amazon Simple Queue Service (Amazon SQS)",
        "isCorrect": false
      },
      {
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "isCorrect": false
      },
      {
        "text": "Amazon Kinesis Data Streams",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon MQ\n\nAmazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systemsâ€“often using different programming languages, and on different platformsâ€“to communicate and exchange information. If an organization is using messaging with existing applications and wants to move the messaging service to the cloud quickly and easily, AWS recommends Amazon MQ for such a use case. Connecting your current applications to Amazon MQ is easy because it uses industry-standard APIs and protocols for messaging, including JMS, NMS, AMQP, STOMP, MQTT, and WebSocket.\n\nTherefore, the only possible answer is Amazon MQ.\n\nIncorrect option:\n\nAmazon Simple Queue Service (Amazon SQS) - Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.\n\nAmazon Kinesis Data Streams - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nAmazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging.\n\nAmazon SNS, Amazon SQS, and Amazon Kinesis are AWS's proprietary technologies and do not come with MQTT compatibility.\n\nReferences:\n\nhttps://aws.amazon.com/amazon-mq/",
    "awsService": "SNS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q547",
    "questionText": "A developer in your company has set up a classic 2 tier architecture consisting of an Application Load Balancer and an Auto Scaling group (ASG) managing a fleet of Amazon EC2 instances. The Application Load Balancer is deployed in a subnet of size 10.0.1.0/24 and the Auto Scaling group is deployed in a subnet of size 10.0.4.0/22.\n\nAs a solutions architect, you would like to adhere to the security pillar of the well-architected framework. How do you configure the security group of the Amazon EC2 instances to only allow traffic coming from the Application Load Balancer?",
    "options": [
      {
        "text": "Add a rule to authorize the security group of the Application Load Balancer",
        "isCorrect": true
      },
      {
        "text": "Add a rule to authorize the CIDR 10.0.4.0/22",
        "isCorrect": false
      },
      {
        "text": "Add a rule to authorize the security group of the Auto Scaling group",
        "isCorrect": false
      },
      {
        "text": "Add a rule to authorize the CIDR 10.0.1.0/24",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAn Auto Scaling group (ASG) contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.\n\nAdd a rule to authorize the security group of the Application Load Balancer\n\nA security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When deciding to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated.\n\nThe following are the characteristics of security group rules:\n    1. By default, security groups allow all outbound traffic.\n    2. Security group rules are always permissive; you can't create rules that deny access.\n    3. Security groups are stateful\n\nApplication Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets â€“ Amazon EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nIncorrect option:\n\nAdd a rule to authorize the CIDR 10.0.4.0/22\n\nAdd a rule to authorize the security group of the Auto Scaling group\n\nAdd a rule to authorize the CIDR 10.0.1.0/24\n\nAdding the entire CIDR of the Application Load Balancer would work, but wouldn't guarantee that only the Auto Scaling group can access the Amazon EC2 instances that are part of the Auto Scaling group. Here, the right solution is to add a rule on the Auto Scaling group security group to allow incoming traffic only from the security group configured for the Application Load Balancer.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q548",
    "questionText": "The engineering team at a leading e-commerce company is anticipating a surge in the traffic because of a flash sale planned for the weekend. You have estimated the web traffic to be 10x. The content of your website is highly dynamic and changes very often.\n\nAs a Solutions Architect, which of the following options would you recommend to make sure your infrastructure scales for that day?",
    "options": [
      {
        "text": "Use an Amazon Route 53 Multi Value record",
        "isCorrect": false
      },
      {
        "text": "Use an Amazon CloudFront distribution in front of your website",
        "isCorrect": false
      },
      {
        "text": "Use an Auto Scaling Group",
        "isCorrect": true
      },
      {
        "text": "Deploy the website on Amazon S3",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse an Auto Scaling Group\n\nAn Auto Scaling group (ASG) contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.\n\nThe size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its size to meet demand, either manually or by using automatic scaling.\n\nAn Auto Scaling group starts by launching enough instances to meet its desired capacity. It maintains this number of instances by performing periodic health checks on the instances in the group. The Auto Scaling group continues to maintain a fixed number of instances even if an instance becomes unhealthy. If an instance becomes unhealthy, the group terminates the unhealthy instance and launches another instance to replace it.\n\nAuto Scaling group is the correct answer here.\n\nIncorrect option:\n\nUse an Amazon CloudFront distribution in front of your website - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. You can use Amazon CloudFront to improve the performance of your website. Amazon CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away.\n\nAmazon CloudFront is not a good solution here as the content is highly dynamic, and Amazon CloudFront will cache things.\n\nDeploy the website on Amazon S3 - You can use Amazon S3 to host a static website. On a static website, individual web pages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document. Depending on your website requirements, you can also configure other options, including redirects, web traffic logging, and custom error documents.\n\nDynamic applications cannot be deployed to Amazon S3. This option has been added as a distractor.\n\nUse an Amazon Route 53 Multi Value record - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Use Multi Value answer routing policy when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random. Amazon Route 53 does not help in scaling your application. This option has been added as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q549",
    "questionText": "A niche social media application allows users to connect with sports athletes. As a solutions architect, you've designed the architecture of the application to be fully serverless using Amazon API Gateway and AWS Lambda. The backend uses an Amazon DynamoDB table. Some of the star athletes using the application are highly popular, and therefore Amazon DynamoDB has increased the read capacity units (RCUs). Still, the application is experiencing a hot partition problem.\n\nWhat can you do to improve the performance of Amazon DynamoDB and eliminate the hot partition problem without a lot of application refactoring?",
    "options": [
      {
        "text": "Use Amazon ElastiCache",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB Streams",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB DAX",
        "isCorrect": true
      },
      {
        "text": "Use Amazon DynamoDB Global Tables",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon DynamoDB DAX\n\nAmazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement â€“ from milliseconds to microseconds â€“ even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.\n\nDAX will be transparent and won't require an application refactoring, and will cache the \"hotkeys\". Therefore, this is the correct option.\n\nIncorrect options:\n\nUse Amazon DynamoDB Global Tables - Amazon DynamoDB Global Tables builds upon DynamoDBâ€™s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. But Global Tables cannot address the hotkey issue.\n\nUse Amazon DynamoDB Streams - Amazon DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. DynamoDB Streams cannot address the hotkey issue.\n\nUse Amazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. ElastiCache could also be a solution, but it will require a lot of refactoring work on the AWS Lambda side.\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/dax/\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q550",
    "questionText": "A genomics research firm is processing sporadic bursts of data-intensive workloads using Amazon EC2 instances. The shared storage must support unpredictable spikes in file operations, but the average daily throughput demand remains relatively low. The team has selected Amazon Elastic File System (EFS) for its scalability and wants to ensure optimal cost and performance during bursts without provisioning throughput manually.\n\nWhich approach should the team take to best meet these requirements?",
    "options": [
      {
        "text": "Switch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode",
        "isCorrect": false
      },
      {
        "text": "Change the throughput mode to provisioned and configure the desired throughput value to support burst workloads",
        "isCorrect": false
      },
      {
        "text": "Enable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode",
        "isCorrect": false
      },
      {
        "text": "Enable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nEnable EFS burst throughput mode on the file system using the General Purpose performance mode and EFS Standard storage class\n\nEnabling burst throughput mode on an Amazon EFS file system using the General Purpose performance mode and the EFS Standard storage class is the most effective approach for workloads with sporadic spikes in throughput demand but low average usage. In burst throughput mode, which is enabled by default for EFS file systems smaller than 1 TiB, EFS accumulates and consumes throughput credits that allow the system to burst to higher levels of throughput as neededâ€”without requiring manual intervention. This mode is ideal for unpredictable workloads like genomics analysis, where short periods of intense activity are followed by idle time. Importantly, using the EFS Standard storage class ensures high availability across multiple Availability Zones, aligning with best practices for fault-tolerant and latency-sensitive environments.\n\n\nvia - https://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nIncorrect options:\n\nSwitch the EFS storage class to EFS One Zone to reduce cost, which will automatically enable burst throughput mode - Switching to the EFS One Zone storage class does not automatically enable burst throughput mode. While One Zone reduces costs by storing data in a single Availability Zone, it is unrelated to throughput mode configuration. Burst throughput can still be used with One Zone, but enabling One Zone alone doesn't guarantee that the file system will operate in burst mode. The throughput mode must be explicitly set to burst or provisioned, and One Zone should only be used when high availability across AZs is not a requirement.\n\nChange the throughput mode to provisioned and configure the desired throughput value to support burst workloads - Switching to provisioned throughput mode contradicts the requirement of not manually managing throughput. Provisioned mode allows specifying a fixed throughput level regardless of file system size, which can be useful for consistent high-throughput workloads but adds unnecessary cost and management overhead for workloads with low average throughput and occasional bursts. Burst throughput mode is better suited here since it automatically scales with usage patterns and does not require manual configuration.\n\nEnable EFS Infrequent Access (IA) storage class to reduce storage cost while continuing to benefit from burst throughput mode - Enabling the EFS Infrequent Access (IA) storage class is useful for reducing cost on rarely accessed files but has no impact on throughput mode. Moreover, once files are transitioned to IA, they may incur retrieval latency and additional costs when accessed during a burst, which may negatively impact performance. Also, to revert files from IA back to Standard, you must disable lifecycle management, but this change is not automatic. Relying on IA for burst performance is not appropriate and may lead to suboptimal behavior.\n\nReferences:\n\nhttps://docs.aws.amazon.com/efs/latest/ug/whatisefs.html\n\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q551",
    "questionText": "A media company operates a video rendering pipeline on Amazon EKS, where containerized jobs are scheduled using Kubernetes deployments. The application experiences bursty traffic patterns, particularly during peak streaming hours. The platform uses the Kubernetes Horizontal Pod Autoscaler (HPA) to scale pods based on CPU utilization. A solutions architect observes that the total number of EC2 worker nodes remains constant during traffic spikes, even when all nodes are at maximum resource utilization. The company needs a solution that enables automatic scaling of the underlying compute infrastructure when pod demand exceeds cluster capacity.\n\nWhich solution should the architect implement to resolve this issue with the least operational overhead?",
    "options": [
      {
        "text": "Enable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group",
        "isCorrect": false
      },
      {
        "text": "Implement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected",
        "isCorrect": false
      },
      {
        "text": "Use AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure",
        "isCorrect": false
      },
      {
        "text": "Deploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nDeploy the Kubernetes Cluster Autoscaler to the EKS cluster. Configure it to integrate with the existing EC2 Auto Scaling group to automatically launch or terminate nodes based on pending pod demands\n\nThe Kubernetes Cluster Autoscaler is the recommended and most seamless solution to automatically manage the size of an EKS cluster's underlying compute resources. It works in tandem with the Kubernetes scheduler and continuously monitors the cluster for unschedulable podsâ€”these are pods that cannot be scheduled due to a lack of available resources (CPU, memory, etc.) on the current set of EC2 nodes.\n\nWhen the Cluster Autoscaler detects unschedulable pods, it attempts to find an Auto Scaling group (ASG) attached to the EKS cluster that can satisfy the resource requirements of the pending pods. If it finds one, it automatically increases the desired capacity of that ASG, triggering the launch of new EC2 nodes. These new nodes register with the cluster and become available to host the previously unschedulable pods. In the same way, the Cluster Autoscaler also supports scale-in operations: it identifies underutilized nodes and safely drains and terminates them if their workloads can be moved elsewhereâ€”leading to cost savings and higher resource efficiency.\n\nThis approach ensures the EKS cluster is highly elastic, cost-effective, and operationally simple, making it the best fit for real-time, variable workloads with minimal administrative overhead.\n\n\nvia - https://aws.amazon.com/blogs/aws/introducing-karpenter-an-open-source-high-performance-kubernetes-cluster-autoscaler/\n\nIncorrect options:\n\nEnable Amazon EC2 Auto Scaling with custom CloudWatch alarms based on cluster-wide CPU and memory usage to dynamically adjust node count in the EKS worker node group - This solution monitors EC2-level metrics like CPU or memory usage, which do not reflect pod scheduling constraints directly. It cannot detect when pods are pending due to resource fragmentation or node capacity mismatch. Additionally, scaling decisions are decoupled from Kubernetes, which could result in overprovisioning or underprovisioning.\n\nImplement an AWS Lambda function that runs every 10 minutes and checks EKS pod scheduling status. Trigger node scaling manually using the eksctl CLI or AWS SDK if unschedulable pods are detected - While feasible, this solution introduces high operational overhead by requiring a custom polling mechanism, Lambda orchestration, and script maintenance. It lacks the native intelligence of Cluster Autoscaler and is harder to maintain and troubleshoot.\n\nUse AWS Fargate to replace the EKS worker nodes with serverless compute profiles, allowing Fargate to scale pods automatically without managing EC2 infrastructure - Although AWS Fargate offers serverless pod execution, it has limitations: not all Kubernetes workloads are Fargate-compatible, especially those requiring custom networking, daemon sets, or GPU-intensive jobs like video rendering. Migrating from EC2 to Fargate requires application redesign, which contradicts the goal of minimal administrative overhead.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/introducing-karpenter-an-open-source-high-performance-kubernetes-cluster-autoscaler/\n\nhttps://aws.amazon.com/blogs/containers/amazon-eks-cluster-multi-zone-auto-scaling-groups/\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/fargate.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q552",
    "questionText": "A development team has configured Elastic Load Balancing for host-based routing. The idea is to support multiple subdomains and different top-level domains.\n\nThe rule *.example.com matches which of the following?",
    "options": [
      {
        "text": "example.com",
        "isCorrect": false
      },
      {
        "text": "example.test.com",
        "isCorrect": false
      },
      {
        "text": "EXAMPLE.COM",
        "isCorrect": false
      },
      {
        "text": "test.example.com",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\ntest.example.com\n\nYou can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple subdomains and different top-level domains using a single load balancer.\n\nA hostname is not case-sensitive, can be up to 128 characters in length, and can contain any of the following characters:\n1. Aâ€“Z, aâ€“z, 0â€“9\n2. - .\n3. * (matches 0 or more characters)\n4. ? (matches exactly 1 character)\n\nYou must include at least one \".\" character. You can include only alphabetical characters after the final \".\" character.\n\nThe rule *.example.com matches test.example.com but doesn't match example.com.\n\nIncorrect options:\n\nexample.com\n\nexample.test.com\n\nEXAMPLE.COM\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
    "awsService": "General",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q553",
    "questionText": "A company uses Application Load Balancers in multiple AWS Regions. The Application Load Balancers receive inconsistent traffic that varies throughout the year. The engineering team at the company needs to allow the IP addresses of the Application Load Balancers in the on-premises firewall to enable connectivity.\n\nWhich of the following represents the MOST scalable solution with minimal configuration changes?",
    "options": [
      {
        "text": "Migrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers",
        "isCorrect": false
      },
      {
        "text": "Set up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Develop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers",
        "isCorrect": false
      },
      {
        "text": "Set up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nSet up AWS Global Accelerator. Register the Application Load Balancers in different Regions to the AWS Global Accelerator. Configure the on-premises firewall's rule to allow static IP addresses associated with the AWS Global Accelerator\n\nAWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. AWS Global Accelerator is easy to set up, configure, and manage. It provides static IP addresses that provide a fixed entry point to your applications and eliminate the complexity of managing specific IP addresses for different AWS Regions and Availability Zones.\n\nAssociate the static IP addresses provided by AWS Global Accelerator to regional AWS resources or endpoints, such as Network Load Balancers, Application Load Balancers, Amazon EC2 Instances, and Elastic IP addresses. The IP addresses are anycast from AWS edge locations so they provide onboarding to the AWS global network close to your users.\n\nSimplified and resilient traffic routing for multi-Region applications using AWS Global Accelerator:\n\nvia - https://aws.amazon.com/global-accelerator/\n\nIncorrect options:\n\nMigrate all Application Load Balancers in different Regions to the Network Load Balancers. Configure the on-premises firewall's rule to allow the Elastic IP addresses of all the Network Load Balancers - Although you could potentially migrate the Application Load Balancers to Network Load Balancers, this option requires changes to the on-premises firewall's configuration rules, hence this is not the right fit for the given use-case. It is more optimal to manage the two static IPs provided by the AWS Global Accelerator for configuring the firewall.\n\nSet up a Network Load Balancer in one Region. Register the private IP addresses of the Application Load Balancers in different Regions with the Network Load Balancer. Configure the on-premises firewall's rule to allow the Elastic IP address attached to the Network Load Balancer - Using a single Network Load Balancer is not possible across AWS regions since an Network Load Balancer is Region bound. Multiple Network Load Balancers have to be registered for the on-premises firewall.\n\nDevelop an AWS Lambda script to get the IP addresses of the Application Load Balancers in different Regions. Configure the on-premises firewall's rule to allow the IP addresses of the Application Load Balancers - This option requires on-going changes to the on-premises firewall's configuration rules because the IP addresses of the Application Load Balancers would keep changing. Hence this is not the right fit for the given use-case. It is more optimal to configure the firewall with a one-time change for the two static IPs provided by the AWS Global Accelerator.\n\nReference:\n\nhttps://aws.amazon.com/global-accelerator/faqs/",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q554",
    "questionText": "Amazon Route 53 is configured to route traffic to two Network Load Balancer nodes belonging to two Availability Zones (AZs): AZ-A and AZ-B. Cross-zone load balancing is disabled. AZ-A has four targets and AZ-B has six targets.\n\nWhich of the below statements is true about traffic distribution to the target instances from Amazon Route 53?",
    "options": [
      {
        "text": "Each of the six targets in AZ-B receives 10% of the traffic",
        "isCorrect": false
      },
      {
        "text": "Each of the four targets in AZ-A receives 8% of the traffic",
        "isCorrect": false
      },
      {
        "text": "Each of the four targets in AZ-A receives 12.5% of the traffic",
        "isCorrect": true
      },
      {
        "text": "Each of the four targets in AZ-A receives 10% of the traffic",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nEach of the four targets in AZ-A receives 12.5% of the traffic\n\nThe nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones (AZs). When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone (AZ).\n\nAmazon Route 53 will distribute traffic such that each load balancer node receives 50% of the traffic from the clients.\n\nIf cross-zone load balancing is disabled:\n1. Each of the four targets in AZ-A receives 12.5% of the traffic.\n2. Each of the six targets in AZ-B receives 8.3% of the traffic.\n\nThis is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone (AZ).\n\nIncorrect options:\n\nEach of the six targets in AZ-B receives 10% of the traffic - As mentioned above in the correct explanation, each of the six targets in AZ-B receives 8.3% of the traffic.\n\nEach of the four targets in AZ-A receives 8% of the traffic - As mentioned above in the correct explanation, each of the four targets in AZ-A receives 12.5% of the traffic.\n\nEach of the four targets in AZ-A receives 10% of the traffic - As mentioned above in the correct explanation, each of the four targets in AZ-A receives 12.5% of the traffic.\n\nReference:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html",
    "awsService": "Route 53",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q555",
    "questionText": "An IT company runs a high-performance computing (HPC) workload on AWS. The workload requires high network throughput and low-latency network performance along with tightly coupled node-to-node communications. The Amazon EC2 instances are properly sized for compute and storage capacity and are launched using default options.\n\nWhich of the following solutions can be used to improve the performance of the workload?",
    "options": [
      {
        "text": "Select the appropriate capacity reservation while launching Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Select a cluster placement group while launching Amazon EC2 instances",
        "isCorrect": true
      },
      {
        "text": "Select dedicated instance tenancy while launching Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Select an Elastic Inference accelerator while launching Amazon EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSelect a cluster placement group while launching Amazon EC2 instances\n\nWhen you launch a new Amazon EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload.\n\nCluster placement group packs instances close together inside an Availability Zone (AZ). This strategy enables workloads to achieve the low-latency network performance necessary for tightly coupled node-to-node communication that is typical of HPC applications.\n\nMore on Cluster placement groups:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nIncorrect options:\n\nSelect the appropriate capacity reservation while launching Amazon EC2 instances - Capacity Reservations enable you to reserve compute capacity for your Amazon EC2 instances in a specific Availability Zone (AZ) for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances (RIs). By creating Capacity Reservations, you ensure that you always have access to Amazon EC2 capacity when you need it, for as long as you need it. Capacity Reservations cannot impact the performance of the underlying Amazon EC2 instances.\n\nSelect dedicated instance tenancy while launching Amazon EC2 instances - Dedicated Instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. Your Dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. Dedicated Instances are useful from a compliance perspective, but are not meant for performance improvement.\n\nSelect an Elastic Inference accelerator while launching Amazon EC2 instances - Amazon Elastic Inference accelerators are GPU-powered hardware devices that are designed to work with any Amazon EC2 instance, Amazon Sagemaker instance, or Amazon ECS task to accelerate deep learning inference workloads at a low cost. They are for workloads that need deep learning. Also, AWS PrivateLink VPC Endpoints are needed for Elastic Inference accelerators, which makes it unsuitable for the current scenario.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q556",
    "questionText": "A social media company wants the capability to dynamically alter the size of a geographic area from which traffic is routed to a specific server resource.\n\nWhich feature of Amazon Route 53 can help achieve this functionality?",
    "options": [
      {
        "text": "Geolocation routing",
        "isCorrect": false
      },
      {
        "text": "Latency-based routing",
        "isCorrect": false
      },
      {
        "text": "Geoproximity routing",
        "isCorrect": true
      },
      {
        "text": "Weighted routing",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nGeoproximity routing\n\nGeoproximity routing lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource.\n\nTo optionally change the size of the geographic region from which Amazon Route 53 routes traffic to a resource, specify the applicable value for the bias:\n1. To expand the size of the geographic region from which Amazon Route 53 routes traffic to a resource, specify a positive integer from 1 to 99 for the bias. Amazon Route 53 shrinks the size of adjacent regions.\n\n\nTo shrink the size of the geographic region from which Amazon Route 53 routes traffic to a resource, specify a negative bias of -1 to -99. Amazon Route 53 expands the size of adjacent regions.\n\n\nMore on how bias works in Geoproximity routing:\n\nvia - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nIncorrect options:\n\nGeolocation routing - Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an Elastic Load Balancing (ELB) load balancer in the Frankfurt region.\n\nWhen you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights. Another possible use is for balancing load across endpoints in a predictable, easy-to-manage way so that each user location is consistently routed to the same endpoint.\n\nLatency-based routing - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency.\n\nTo use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Amazon Route 53 responds with the value from the selected record, such as the IP address for a web server.\n\nWeighted routing - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.\n\nTo configure weighted routing, you create records that have the same name and type for each of your resources. You assign each record a relative weight that corresponds with how much traffic you want to send to each resource. Amazon Route 53 sends traffic to a resource based on the weight that you assign to the record as a proportion of the total weight for all records in the group.\n\nReference:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html",
    "awsService": "Route 53",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q557",
    "questionText": "A ride-sharing company wants to use an Amazon DynamoDB table for data storage. The table will not be used during the night hours whereas the read and write traffic will often be unpredictable during day hours. When traffic spikes occur they will happen very quickly.\n\nWhich of the following will you recommend as the best-fit solution?",
    "options": [
      {
        "text": "Set up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled",
        "isCorrect": false
      },
      {
        "text": "Set up Amazon DynamoDB table in the on-demand capacity mode",
        "isCorrect": true
      },
      {
        "text": "Set up Amazon DynamoDB table with a global secondary index",
        "isCorrect": false
      },
      {
        "text": "Set up Amazon DynamoDB global table in the provisioned capacity mode",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up Amazon DynamoDB table in the on-demand capacity mode\n\nAmazon DynamoDB has two read/write capacity modes for processing reads and writes on your tables:\n\nOn-demand\n\nProvisioned (default, free-tier eligible)\n\nAmazon DynamoDB on-demand is a flexible billing option capable of serving thousands of requests per second without capacity planning. DynamoDB on-demand offers pay-per-request pricing for read and write requests so that you pay only for what you use.\n\nThe on-demand mode is a good option if any of the following are true:\n\nYou create new tables with unknown workloads.\n\nYou have unpredictable application traffic.\n\nYou prefer the ease of paying for only what you use.\n\nIf you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto-scaling to adjust your tableâ€™s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate to obtain cost predictability.\n\nProvisioned mode is a good option if any of the following are true:\n\nYou have predictable application traffic.\n\nYou run applications whose traffic is consistent or ramps gradually.\n\nYou can forecast capacity requirements to control costs.\n\n\nvia - https://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\n\nWith on-demand, Amazon DynamoDB instantly allocates capacity as it is needed. There is no concept of provisioned capacity, and there is no delay waiting for Amazon CloudWatch thresholds or the subsequent table updates. On-demand is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when underprovisioned capacity would impact the user experience. On-demand is a perfect solution if your team is moving to a NoOps or serverless environment.\n\nThe given use case clearly states that when the traffic spikes occur they happen very quickly, thereby implying an unpredictable traffic pattern, therefore the on-demand capacity mode is the correct option for the given use case.\n\nIncorrect options:\n\nSet up Amazon DynamoDB table in the provisioned capacity mode with auto-scaling enabled - As mentioned in the explanation above, you should use the provisioned capacity mode (even with auto-scaling) only when you have predictable application traffic.\n\nWhen you create Amazon DynamoDB table, auto-scaling is the default capacity setting, but you can also enable auto-scaling on any table that does not have it active. Behind the scenes, as illustrated in the following diagram, DynamoDB auto scaling uses a scaling policy in Application Auto Scaling. To configure auto-scaling in DynamoDB, you set the minimum and maximum levels of read and write capacity in addition to the target utilization percentage. Auto-scaling uses Amazon CloudWatch to monitor a tableâ€™s read and write capacity metrics. To do so, it creates CloudWatch alarms that track consumed capacity.\n\nSet up Amazon DynamoDB table with a global secondary index - A global secondary index (GSI) is an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table. GSI cannot be used to handle an unpredictable load on a DynamoDB table.\n\nSet up Amazon DynamoDB global table in the provisioned capacity mode - Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.\n\nGlobal tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region.\n\nAmazon DynamoDB global tables:\n\nvia - https://aws.amazon.com/dynamodb/global-tables/\n\nAmazon DynamoDB global table cannot be used to handle an unpredictable load on a Amazon DynamoDB table.\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\n\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\n\nhttps://aws.amazon.com/dynamodb/global-tables/",
    "awsService": "DynamoDB",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q558",
    "questionText": "The engineering team at a social media company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating Amazon CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Solutions Architect â€“ Associate to advise on Amazon CloudFront capabilities on routing, security, and high availability.\n\nWhich of the following would you identify as correct regarding Amazon CloudFront? (Select three)",
    "options": [
      {
        "text": "Use AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content",
        "isCorrect": false
      },
      {
        "text": "Use geo restriction to configure Amazon CloudFront for high-availability and failover",
        "isCorrect": false
      },
      {
        "text": "Amazon CloudFront can route to multiple origins based on the price class",
        "isCorrect": false
      },
      {
        "text": "Amazon CloudFront can route to multiple origins based on the content type",
        "isCorrect": true
      },
      {
        "text": "Use an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover",
        "isCorrect": true
      },
      {
        "text": "Use field level encryption in Amazon CloudFront to protect sensitive data for specific content",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nAmazon CloudFront can route to multiple origins based on the content type\n\nYou can configure a single Amazon CloudFront web distribution to serve different types of requests from multiple origins. For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a Amazon CloudFront web distribution.\n\nUse an origin group with primary and secondary origins to configure Amazon CloudFront for high-availability and failover\n\nYou can set up Amazon CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.\n\nTo set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group.\n\n\nvia - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\n\nUse field level encryption in Amazon CloudFront to protect sensitive data for specific content\n\nField-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the dataâ€”and have the credentials to decrypt itâ€”are able to do so.\n\nTo use field-level encryption, when you configure your Amazon CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request. (You canâ€™t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt.)\n\n\nvia - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\n\nIncorrect options:\n\nUse AWS Key Management Service (AWS KMS) encryption in Amazon CloudFront to protect sensitive data for specific content - This option has been added as a distractor. You can use field level encryption in Amazon CloudFront to protect sensitive data for specific content.\n\nUse geo restriction to configure Amazon CloudFront for high-availability and failover - You can use geo restriction, also known as geo blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront distribution. Geo restriction is not used to configure Amazon CloudFront for high availability and failover.\n\nAmazon CloudFront can route to multiple origins based on the price class - Amazon CloudFront edge locations are grouped into geographic regions, and AWS has grouped regions into price classes. The default price class includes all regions. Another price class includes most regions (the United States; Canada; Europe; Hong Kong, Philippines, South Korea, Taiwan, and Singapore; Japan; India; South Africa; and Middle East regions) but excludes the most expensive regions. A third price class includes only the least expensive regions (the United States, Canada, and Europe regions). CloudFront can only route to multiple origins based on content type and not on the basis of the price class.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html",
    "awsService": "CloudFront",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q559",
    "questionText": "A financial services company is implementing two separate data retention policies to comply with regulatory standards:\n\nPolicy A: Critical transaction records must be immediately available for audit and must not be deleted or overwritten for 7 years.\n\nPolicy B: Archived compliance data must be stored in a low-cost, long-term storage solution and locked from deletion or modification for at least 10 years.\n\nAs a solutions architect, which combination of AWS features should you recommend to enforce these policies effectively?",
    "options": [
      {
        "text": "Use Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B",
        "isCorrect": true
      },
      {
        "text": "Use Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon S3 Object Lock in Compliance mode for Policy A, and S3 Glacier Vault Lock for Policy B\n\nUsing Amazon S3 Object Lock in Compliance mode for Policy A and S3 Glacier Vault Lock for Policy B is the ideal solution for meeting the companyâ€™s regulatory needs. S3 Object Lock in Compliance mode provides write-once-read-many (WORM) protection, ensuring that critical transaction data cannot be overwritten or deleted for the duration of the specified retention period. This is essential for audit-ready data that must remain immutable and immediately accessible. On the other hand, S3 Glacier Vault Lock allows organizations to enforce compliance controls for long-term archived data by setting a lock policy that prevents deletions or modifications for a defined period. Once locked, these policies cannot be altered, which is ideal for meeting stringent 10-year archival requirements in a cost-effective manner. This combination ensures both data availability for audits and secure long-term retention at a lower cost.\n\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\n\n\nvia - https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/\n\n\n\nIncorrect options:\n\nUse Amazon S3 Standard storage class with S3 Lifecycle policies for Policy A, and S3 Glacier Flexible Retrieval for Policy B - This option is incorrect because Lifecycle policies alone do not prevent deletion or modificationâ€”they are primarily for automated transitions and expirations.\n\nUse Amazon S3 Glacier Vault Lock for both policies to reduce storage costs while enforcing retention - Glacier Vault Lock is not suitable for use cases requiring immediate access to data like Policy A.\n\nUse Amazon S3 Object Lock in Governance mode for both policies to ensure data cannot be deleted prematurely - Governance mode allows certain users to override locks if they have permissions, which is not appropriate for strict compliance use cases.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\n\nhttps://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q560",
    "questionText": "A company wants to grant access to an Amazon S3 bucket to users in its own AWS account as well as to users in another AWS account. Which of the following options can be used to meet this requirement?",
    "options": [
      {
        "text": "Use either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account",
        "isCorrect": false
      },
      {
        "text": "Use a bucket policy to grant permission to users in its account as well as to users in another account",
        "isCorrect": true
      },
      {
        "text": "Use a user policy to grant permission to users in its account as well as to users in another account",
        "isCorrect": false
      },
      {
        "text": "Use permissions boundary to grant permission to users in its account as well as to users in another account",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse a bucket policy to grant permission to users in its account as well as to users in another account\n\nA bucket policy is a type of resource-based policy that can be used to grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. For cross-account permissions to other AWS accounts or users in another account, you must use a bucket policy.\n\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\n\nIncorrect options:\n\nUse either a bucket policy or a user policy to grant permission to users in its account as well as to users in another account\n\nUse a user policy to grant permission to users in its account as well as to users in another account\n\nIf an AWS account that owns a bucket wants to grant permission to users in its own AWS account, it can use either a bucket policy or a user policy. The user policies are for managing permissions for users in their own AWS account and NOT for users in other AWS accounts. Therefore both these options are incorrect.\n\nUse permissions boundary to grant permission to users in its account as well as to users in another account - Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions, so this option is not correct for the given use case.\n\n\nvia - https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q561",
    "questionText": "A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance.\n\nWhat could explain this anomaly?",
    "options": [
      {
        "text": "The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check",
        "isCorrect": true
      },
      {
        "text": "The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check",
        "isCorrect": false
      },
      {
        "text": "Both the Auto Scaling group and Application Load Balancer are using ALB based health check",
        "isCorrect": false
      },
      {
        "text": "Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThe Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check\n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management.\n\nAuto Scaling Group Overview:\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nApplication Load Balancer automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and AWS Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.\n\nIf the Auto Scaling group (ASG) is using EC2 as the health check type and the Application Load Balancer (ALB) is using its in-built health check, there may be a situation where the ALB health check fails because the health check pings fail to receive a response from the instance. At the same time, ASG health check can come back as successful because it is based on EC2 based health check.\nTherefore, in this scenario, the ALB will remove the instance from its inventory, however, the Auto Scaling Group will fail to provide the replacement instance. This can lead to the scaling issues mentioned in the problem statement.\n\nIncorrect options:\n\nThe Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check - Application Load Balancer cannot use EC2 based health checks, so this option is incorrect.\n\nBoth the Auto Scaling group and Application Load Balancer are using ALB based health check - It is recommended to use ALB based health checks for both Auto Scaling group and Application Load Balancer. If both the Auto Scaling group and Application Load Balancer use ALB based health checks, then you will be able to avoid the scenario mentioned in the question.\n\nBoth the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check - Application Load Balancer cannot use EC2 based health checks, so this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/health-checks-overview.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q562",
    "questionText": "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is approximately 600 kilobytes.\n\nWhich of the following options offers the LOWEST data transfer egress cost for the company?",
    "options": [
      {
        "text": "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region",
        "isCorrect": false
      },
      {
        "text": "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region",
        "isCorrect": false
      },
      {
        "text": "Deploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region",
        "isCorrect": true
      },
      {
        "text": "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDeploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over a Direct Connect connection at a location in the same region\n\nAWS Direct Connect is a networking service that provides an alternative to using the internet to connect to AWS. Using AWS Direct Connect, data that would have previously been transported over the internet is delivered through a private network connection between your on-premises data center and AWS.\n\nFor the given use case, the main pricing parameter while using the AWS Direct Connect connection is the Data Transfer Out (DTO) from AWS to the on-premises data center. DTO refers to the cumulative network traffic that is sent through AWS Direct Connect to destinations outside of AWS. This is charged per gigabyte (GB), and unlike capacity measurements, DTO refers to the amount of data transferred, not the speed.\n\n\nvia - https://aws.amazon.com/directconnect/pricing/\n\nEach query response is 60 megabytes in size and each webpage for the visualization tool is 600 kilobytes in size. If you deploy the visualization tool in the same AWS region as the data warehouse, then you only need to pay for the 600 kilobytes of DTO charges for the webpage. Therefore this option is correct.\n\nHowever, if you deploy the visualization tool on-premises, then you need to pay for the 60 MB of DTO charges for the query response from the data warehouse to the visualization tool.\n\nIncorrect options:\n\nDeploy the visualization tool in the same AWS region as the data warehouse. Access the visualization tool over the internet at a location in the same region\n\nDeploy the visualization tool on-premises. Query the data warehouse over the internet at a location in the same AWS region\n\nData transfer pricing over AWS Direct Connect is lower than data transfer pricing over the internet, so both of these options are incorrect.\n\nDeploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct Connect connection at a location in the same AWS region - As mentioned in the explanation above, if you deploy the visualization tool on-premises, then you need to pay for the 60 megabytes of DTO charges for the query response from the data warehouse to the visualization tool. So this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/directconnect/pricing/\n\nhttps://aws.amazon.com/getting-started/hands-on/connect-data-center-to-aws/services-costs/\n\nhttps://aws.amazon.com/directconnect/faqs/",
    "awsService": "General",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q563",
    "questionText": "The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB.\n\nWhich of the following steps would you combine to address the given scenario? (Select two)",
    "options": [
      {
        "text": "Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes",
        "isCorrect": false
      },
      {
        "text": "Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica",
        "isCorrect": true
      },
      {
        "text": "Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target",
        "isCorrect": false
      },
      {
        "text": "Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nHandle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica\n\nWhen you create a second, third, and so on DB instance in an Aurora-provisioned DB cluster, Aurora automatically sets up replication from the writer DB instance to all the other DB instances. These other DB instances are read-only and are known as Aurora Replicas.\n\nAurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer.\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\nCreate a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target\n\nIf the primary instance in a DB cluster using single-master replication fails, Aurora automatically fails over to a new primary instance in one of two ways:\n\nBy promoting an existing Aurora Replica to the new primary instance\nBy creating a new primary instance\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\n\nIncorrect options:\n\nCreate a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target - There are no standby instances in Aurora. Aurora performs an automatic failover to a read replica when a problem is detected. So this option is incorrect.\n\nRead replicas, Multi-AZ deployments, and multi-region deployments:\n\nvia - https://aws.amazon.com/rds/features/read-replicas/\n\nIncrease the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes - Increasing the concurrency of the AWS Lambda function would not resolve the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption for the Aurora instance. This option has been added as a distractor.\n\nUse Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster - Using Amazon EC2 instances behind an Application Load Balancer would not resolve the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption for the Aurora instance. This option has been added as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\n\nhttps://aws.amazon.com/rds/features/read-replicas/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q564",
    "questionText": "A global enterprise is modernizing its hybrid IT infrastructure to improve both availability and network performance. The company operates a TCP-based application hosted on Amazon EC2 instances that are deployed across multiple AWS Regions, while a secondary UDP-based component of the application is hosted in its on-premises data centers. These application components must be accessed by customers around the world with minimal latency and consistent uptime.\n\nWhich combination of options should a solutions architect implement for the given use case? (Select two)",
    "options": [
      {
        "text": "Configure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers",
        "isCorrect": true
      },
      {
        "text": "Set up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover",
        "isCorrect": false
      },
      {
        "text": "Deploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers",
        "isCorrect": false
      },
      {
        "text": "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups",
        "isCorrect": true
      },
      {
        "text": "Create a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nConfigure an AWS Global Accelerator standard accelerator, and register the TCP-based EC2 workloads behind the load balancers\n\nAWS Global Accelerator supports improving the availability and performance of global applications with TCP-based traffic. It routes users to the closest healthy endpoint in multiple Regions via the AWS global network and supports automatic failover. Registering load balancers as endpoints ensures that the TCP workload hosted on EC2 instances benefits from low-latency access and high availability.\n\nCreate a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure NLBs in each Region to route to the on-premises endpoints via IP-based target groups\n\nNetwork Load Balancers support both TCP and UDP traffic. By creating NLBs in each Region, the architecture allows load balancing for EC2 workloads and can forward UDP traffic to IP-based targets in on-premises networks. This setup ensures that both stateful TCP and stateless UDP workloads are accessible and scalable from AWS.\n\n\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\nIncorrect Options:\n\nSet up AWS Direct Connect connections to route all TCP and UDP traffic through a single Region, using static routes and BGP failover - AWS Direct Connect enhances dedicated network performance between AWS and on-premises infrastructure but does not inherently provide intelligent global routing, failover, or multi-region resilience. It is not optimal for distributing traffic across Regions or managing global availability.\n\nDeploy AWS PrivateLink to connect each on-premises UDP workload to the AWS Regions through interface endpoints exposed by the Network Load Balancers - AWS PrivateLink enables secure connectivity to AWS services via VPC endpoints but supports only TCP-based traffic. It cannot be used to route or expose UDP-based services, making it unsuitable for the on-premises UDP workload.\n\nCreate a Network Load Balancer (NLB) in each Region to handle the EC2-based TCP traffic. For the UDP-based on-premises workload, configure Application Load Balancers in each Region to route to the on-premises endpoints via IP-based target groups - While the first part of the option is valid â€” using NLBs to handle EC2-based TCP traffic is appropriate because NLBs support TCP connections and can scale efficiently across Regions. The second part of the option is flawed. The issue lies in attempting to use Application Load Balancers (ALBs) for UDP-based traffic. ALBs only support HTTP/HTTPS (Layer 7) and WebSocket protocols. They do not support Layer 4 protocols such as TCP or UDP.\n\n\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n\nReferences:\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q565",
    "questionText": "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center to durable long term storage.\n\nAs a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
    "options": [
      {
        "text": "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier",
        "isCorrect": false
      },
      {
        "text": "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
        "isCorrect": false
      },
      {
        "text": "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier",
        "isCorrect": false
      },
      {
        "text": "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nTransfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3 Glacier\n\nAWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases.\nThe data stored on AWS Snowball Edge device can be copied into Amazon S3 bucket and later transitioned into Amazon S3 Glacier via a lifecycle policy. You can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier.\n\nIncorrect options:\n\nTransfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 Glacier - As mentioned earlier, you can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier. Hence, this option is incorrect.\n\nSetup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. Direct Connect involves significant monetary investment and takes more than a month to set up, therefore it's not the correct fit for this use-case where just a one-time data transfer has to be done.\n\nSetup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into Amazon S3 Glacier - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). VPN Connections are a good solution if you have an immediate need, and have low to modest bandwidth requirements. Because of the high data volume for the given use-case, Site-to-Site VPN is not the correct choice.\n\nReference:\n\nhttps://aws.amazon.com/snowball/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q566",
    "questionText": "A fintech company currently operates a real-time search and analytics platform on-premises. This platform ingests streaming data from multiple data-producing systems and provides immediate search capabilities and interactive visualizations for end users. As part of its cloud migration strategy, the company wants to rearchitect the solution using AWS-native services.\n\nWhich of the following represents the most efficient solution?",
    "options": [
      {
        "text": "Deploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights",
        "isCorrect": false
      },
      {
        "text": "Ingest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data",
        "isCorrect": true
      },
      {
        "text": "Use AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshiftâ€™s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nIngest and process the streaming data using Amazon Kinesis Data Streams, then index the data with Amazon OpenSearch Service for real-time search capabilities. Use Amazon QuickSight to build interactive dashboards and visualizations based on the indexed data\n\nThis solution leverages Amazon Kinesis Data Streams for high-throughput, low-latency stream ingestion, and Amazon OpenSearch Service for full-text indexing and real-time search. OpenSearch is purpose-built for search and analytics workloads. For visualization, Amazon QuickSight can be connected to OpenSearch or other data sources to build rich dashboards. This architecture provides a scalable, fully managed, real-time search and analytics platform using AWS-native services.\n\n\nvia - https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\n\nIncorrect options:\n\nDeploy Amazon EC2 instances to handle the ingestion and processing of streaming data, storing the results in Amazon S3. Utilize Amazon Athena to search the stored data, and use Amazon Managed Grafana to generate dashboards and visual insights - While this solution uses serverless analytics via Amazon Athena and integrates with visualization tools like Amazon Managed Grafana, the use of Amazon EC2 to ingest and process real-time streaming data adds unnecessary operational overhead. Additionally, Athena is optimized for querying data on Amazon S3 rather than low-latency, real-time search requirements. So, this architecture lacks the responsiveness and indexing performance needed for real-time search applications.\n\nUse AWS Glue streaming ETL to process data streams and load the data into Amazon Redshift. Use Amazon Redshiftâ€™s full-text search capabilities for querying. Use Amazon QuickSight for data visualizations - Although AWS Glue streaming ETL can handle data ingestion and Redshift supports powerful analytics, Redshift is primarily a data warehouse and not optimized for low-latency, full-text search. Redshiftâ€™s search capabilities are limited compared to dedicated search engines like OpenSearch. Furthermore, the combination of Glue and Redshift adds complexity and may introduce latency for real-time applications.\n\nUse Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to ingest the data into Amazon DynamoDB to facilitate full text search. Use Amazon CloudWatch to create dashboards and query the data - DynamoDB is a NoSQL database optimized for key-value access, not full-text search. Additionally, Amazon CloudWatch is designed for infrastructure and application monitoring, not advanced search or visualization of dynamic user data. This setup lacks the native indexing and real-time query capabilities required for a robust search experience.\n\nReferences:\n\nhttps://docs.aws.amazon.com/streams/latest/dev/introduction.html\n\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\n\nhttps://docs.aws.amazon.com/quicksight/latest/user/welcome.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q567",
    "questionText": "An enterprise SaaS provider is currently operating a legacy web application hosted on a single Amazon EC2 instance within a public subnet. The same instance also hosts a MySQL database. DNS records for the application are configured through Amazon Route 53. As part of a modernization initiative, the company wants to rearchitect this application for high availability and scalability. In addition, the company wants to improve read performance on the database layer to handle increasing user traffic.\n\nWhich combination of solutions will meet these requirements? (Select two)",
    "options": [
      {
        "text": "Deploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages",
        "isCorrect": false
      },
      {
        "text": "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly",
        "isCorrect": true
      },
      {
        "text": "Migrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones",
        "isCorrect": true
      },
      {
        "text": "Use an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nUse an Auto Scaling group to deploy EC2 instances across multiple Availability Zones within a single Region. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly\n\nThis option enhances scalability and high availability of the web application. Deploying EC2 instances across multiple Availability Zones ensures that the application remains available even if one Availability Zone experiences an outage. The Application Load Balancer (ALB) evenly distributes incoming web traffic across all healthy instances in the target group, improving performance and fault tolerance. The Auto Scaling group automatically adjusts the number of EC2 instances in response to changing traffic, reducing operational burden and costs while ensuring optimal performance.\n\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nMigrate the existing MySQL database to an Amazon Aurora MySQL cluster. Deploy the primary DB instance and one or more read replicas in different Availability Zones\n\nThis option addresses the need to reduce read latency and improve database availability. Amazon Aurora is a fully managed, MySQL-compatible relational database engine designed for high performance and availability. By deploying a writer (primary) instance and at least one reader instance in different Availability Zones, the application can offload read-heavy workloads to the reader instance, improving throughput and reducing response times. Auroraâ€™s automatic failover capabilities also help ensure continued operation in the event of a failure of the primary instance. Aurora further supports features like replication, automatic backups, and storage autoscaling, reducing operational overhead.\n\nIncorrect options:\n\nDeploy an additional EC2 instance in a different AWS Region, and configure Amazon Route 53 with a failover routing policy to direct traffic to the secondary instance during primary Region outages - This setup lacks auto scaling capabilities. If traffic spikes in either Region, the architecture would require manual intervention or additional setup to scale.\nNo Application Load Balancer (ALB) or Auto Scaling group is used in this option â€” which are best practices for horizontally scaling stateless workloads.\n\nUse an Auto Scaling group to deploy EC2 instances across multiple Availability Zones in two AWS Regions. Register the instances in a target group behind an Application Load Balancer to distribute web traffic evenly - An Application Load Balancer operates only within a single AWS Region and can route traffic to targets in multiple Availability Zones within that Region, but not across Regions. So, this option is incorrect.\n\nUse Amazon CloudFront with Lambda@Edge to serve dynamic content from EC2 instances located in different Regions - CloudFront is ideal for caching static content, but itâ€™s not designed for dynamic database-driven applications hosted on EC2. Using Lambda@Edge for a database-backed application introduces architectural mismatches and increases complexity.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q568",
    "questionText": "An enterprise runs a critical Oracle database workload in its on-premises environment. The company now plans to replicate both existing records and continuous transactional changes to a managed Oracle environment in AWS. The target database will run on Amazon RDS for Oracle. Data transfer volume is expected to fluctuate throughout the day, and the team wants the solution to provision compute resources automatically based on actual workload requirements.\n\nWhich solution will meet these requirements?",
    "options": [
      {
        "text": "Use AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected",
        "isCorrect": false
      },
      {
        "text": "Deploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies.",
        "isCorrect": false
      },
      {
        "text": "Configure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle",
        "isCorrect": true
      },
      {
        "text": "Use AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time.",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure an AWS DMS Serverless replication task to synchronize historical and ongoing changes between the on-premises Oracle database and Amazon RDS for Oracle\n\nAWS DMS Serverless is designed specifically for scenarios like this â€” where data replication workloads vary and automation is required. AWS DMS Serverless automatically provisions and scales compute resources based on workload demand. It supports ongoing replication, including change data capture (CDC), and is fully managed, reducing operational complexity. This solution avoids the need to guess capacity requirements or manually adjust instance sizes.\n\n\nvia - https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Serverless.html\n\nIncorrect options:\n\nUse AWS Glue to extract data from the on-premises Oracle database and write the output to Amazon RDS for Oracle. Configure Glue to run on demand when changes are detected - AWS Glue is an ETL service, not designed for real-time or change data capture (CDC) replication from transactional databases. It works well for batch processing and data transformation pipelines, but not for continuously syncing ongoing changes from an Oracle source. Using it in this context would lead to inconsistent data and increased latency.\n\nDeploy the AWS DMS replication instance on Amazon EC2. Configure the instance with custom scripts that monitor CPU usage and resize the instance using EC2 Auto Scaling policies. - Deploying DMS on EC2 and building custom Auto Scaling logic introduces unnecessary operational burden. EC2 Auto Scaling is not natively integrated with DMS replication tasks and cannot scale based on replication task load. This approach defeats the goal of minimizing management and may introduce performance instability due to lag in scaling triggers.\n\nUse AWS Lambda to capture change data from the on-premises Oracle database. Trigger Lambda functions to write the updates to Amazon RDS for Oracle in real time. - AWS Lambda is not designed to process high-volume, stateful, transactional replication workloads from an Oracle database. Oracle does not natively trigger Lambda functions, and capturing CDC via Lambda would require complex, non-scalable workarounds. This option also lacks support for maintaining transactional consistency.\n\nReference:\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Serverless.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q569",
    "questionText": "A tech company runs a web application that includes multiple internal services deployed across Amazon EC2 instances within a VPC. These services require communication with a third-party SaaS provider's API for analytics and billing, which is also hosted on the AWS infrastructure. The company is concerned about minimizing public internet exposure while maintaining secure and reliable connectivity. The solution must ensure private access without allowing unsolicited incoming traffic from the SaaS provider.\n\nWhich solution will best meet these requirements?",
    "options": [
      {
        "text": "Establish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider",
        "isCorrect": false
      },
      {
        "text": "Use AWS PrivateLink to create a private endpoint within the applicationâ€™s VPC that connects securely to the SaaS providerâ€™s VPC",
        "isCorrect": true
      },
      {
        "text": "Set up VPC peering between the application VPC and the SaaS providerâ€™s VPC to allow direct communication",
        "isCorrect": false
      },
      {
        "text": "Use AWS CloudFront to route requests from the applicationâ€™s internal services to the SaaS provider through edge locations",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS PrivateLink to create a private endpoint within the applicationâ€™s VPC that connects securely to the SaaS providerâ€™s VPC\n\nAWS PrivateLink is a fully managed service that enables private connectivity between Virtual Private Clouds (VPCs), AWS services, and supported third-party SaaS applications over the AWS network, without exposing traffic to the public internet. In this case, the internal service components of the application can securely access the third-party SaaS APIs by creating an interface VPC endpoint for the SaaS provider's service.\n\nThis endpoint appears as an elastic network interface (ENI) in the applicationâ€™s VPC, allowing internal services to communicate with the SaaS API using private IP addresses. Importantly, the traffic never leaves the Amazon network, which significantly reduces the risk of data exposure or interception. Additionally, the SaaS provider cannot initiate connections to the applicationâ€™s services because PrivateLink is inherently unidirectional from the service consumer (the company's VPC) to the service provider (the SaaS providerâ€™s VPC). This architecture helps meet strict security and compliance requirements, such as zero-trust networking and least privilege access.\n\n\nvia - https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/aws-privatelink.html\n\nIncorrect options:\n\nEstablish a VPN connection using AWS Site-to-Site VPN to create a secure tunnel between the internal services and the third-party SaaS provider - While Site-to-Site VPN can establish a secure connection, it typically requires IPsec-compatible devices and may route traffic over the internet, depending on the SaaS provider's setup. It also introduces more operational complexity and latency compared to AWS-native private connectivity options.\n\nSet up VPC peering between the application VPC and the SaaS providerâ€™s VPC to allow direct communication - VPC peering enables direct network connectivity between VPCs; however, it lacks the granular access controls that AWS PrivateLink provides. Unlike PrivateLink, VPC peering allows broader network-level access, which can potentially permit unsolicited inbound traffic from the SaaS provider. Additionally, due to limitations in scalability and security management, SaaS providers typically do not support peering with individual customer VPCs.\n\nUse AWS CloudFront to route requests from the applicationâ€™s internal services to the SaaS provider through edge locations - CloudFront is designed for content distribution and caching at edge locations. It does not establish private or secure backend connectivity between internal services and SaaS APIs, and traffic still traverses the public internet.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/what-is-privatelink.html\n\nhttps://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/aws-privatelink.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q570",
    "questionText": "A retail enterprise is expanding its hybrid IT infrastructure and plans to securely connect its on-premises corporate network to its AWS environment. The company wants to ensure that all data exchanged between on-premises systems and AWS is encrypted at both the network and session layers. Additionally, the solution must incorporate granular security controls that restrict unnecessary or unauthorized access between the cloud and on-premises environments. A solutions architect must recommend a scalable and secure approach that supports these goals.\n\nWhich solution best meets these requirements?",
    "options": [
      {
        "text": "Establish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed",
        "isCorrect": false
      },
      {
        "text": "Use AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies",
        "isCorrect": false
      },
      {
        "text": "Set up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems",
        "isCorrect": true
      },
      {
        "text": "Set up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nSet up AWS Site-to-Site VPN to connect the on-premises network to the AWS VPC. Use route tables to manage traffic flow and configure security groups and network ACLs to allow only authorized communication between systems\n\nAWS Site-to-Site VPN uses IPsec tunnels, which provide encryption at the network layer, ensuring that traffic is securely transmitted between the on-premises network and the VPC. By applying security groups and network ACLs, fine-grained traffic control is possible. Additionally, session-layer encryption (e.g., HTTPS, TLS) can be layered on top, satisfying both encryption requirements with minimal overhead. This is the most cost-effective and secure solution for the given requirements.\n\nIncorrect Options:\n\nEstablish a dedicated AWS Direct Connect connection between the corporate network and AWS. Configure VPC route tables to control traffic flow and use security groups and network ACLs to restrict access as needed - While AWS Direct Connect provides a dedicated private connection with high bandwidth and low latency, it does not encrypt traffic at the network layer by default. Additional technologies (e.g., MACsec or an overlay VPN) are needed to secure the connection. Furthermore, session-layer encryption is not inherently addressed by Direct Connect. Without additional configuration, this approach does not meet the encryption requirements.\n\nUse AWS Client VPN to allow corporate users to connect to the VPC individually. Manage access controls with security groups and IAM policies - While AWS Client VPN does provide encrypted connectivity over the internet and can enable individual client devices to access AWS resources securely, it is not designed for network-to-network connectivity. This solution fails to meet the requirement of establishing a site-to-site connection between the corporate network and the VPC for seamless hybrid access. It also introduces operational complexity and does not scale well for scenarios where full internal network routing and access controls are required between entire corporate networks and AWS.\n\nSet up a bastion host in a public subnet of the VPC to provide SSH-based access to AWS resources from the corporate network. Use security groups to control access - A bastion host provides access at the individual server level, typically over SSH or RDP, and is useful for administrative tasks. However, it does not establish a full network-layer or session-layer encrypted connection between the corporate network and AWS resources. It also does not scale for use cases requiring broad application or service access between on-premises and AWS. This solution lacks centralized routing and traffic control, making it insufficient for secure, scalable hybrid connectivity.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\n\nhttps://docs.aws.amazon.com/vpn/latest/clientvpn-admin/what-is.html\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q571",
    "questionText": "A data analytics team at a global media firm is building a new analytics platform to process large volumes of both historical and real-time data. This data is stored in Amazon S3. The team wants to implement a serverless solution that allows them to query the data directly using SQL. Additionally, the solution must ensure that all data is encrypted at rest and automatically replicated to another AWS Region to support business continuity.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "text": "Create an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data",
        "isCorrect": true
      },
      {
        "text": "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data",
        "isCorrect": false
      },
      {
        "text": "Create an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL",
        "isCorrect": false
      },
      {
        "text": "Enable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an Amazon S3 bucket configured with server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Athena to run SQL queries on the data\n\nThis option is the most efficient and serverless approach that meets all of the stated requirements. Amazon S3 supports server-side encryption using AWS KMS multi-Region keys, which allows secure data storage and supports automatic replication to a secondary Region via CRR. Amazon Athena is a serverless query service that can run SQL queries directly against S3 data without requiring a database engine or managing infrastructure. This solution supports encryption at rest, regional redundancy, and SQL-based querying with minimal overhead.\n\n\nvia - https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\n\nIncorrect options:\n\nEnable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using Amazon S3 managed keys (SSE-S3). Use Amazon Athena to run SQL queries on the replicated data\n\nEnable Cross-Region Replication (CRR) on the existing Amazon S3 bucket. Apply server-side encryption using AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to run SQL queries on the replicated data\n\nBoth of these options skip the step of creating a new bucket in the target AWS Region. You cannot enable CRR on a bucket without specifying the target bucket for replication in another Region. So, these options are incorrect.\n\nCreate an Amazon S3 bucket configured with server-side encryption using Amazon S3 managed keys (SSE-S3). Enable cross-Region replication (CRR) on the source bucket. Use Amazon Redshift Spectrum to query the S3 data using SQL - While Redshift Spectrum can query data stored in S3 using SQL, this solution introduces unnecessary operational complexity because Amazon Redshift requires cluster management. It is not entirely serverless. Although Redshift Spectrum itself is serverless, it still depends on a provisioned Redshift cluster backend, which contradicts the requirement to minimize operational overhead.\n\nReferences:\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\n\nhttps://docs.aws.amazon.com/athena/latest/ug/what-is.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q572",
    "questionText": "The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into the raw zone of the Amazon S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake.\n\nWhat AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?",
    "options": [
      {
        "text": "Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks",
        "isCorrect": false
      },
      {
        "text": "Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks",
        "isCorrect": false
      },
      {
        "text": "Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Athena to run SQL based analytics against Amazon S3 data",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon Athena to run SQL based analytics against Amazon S3 data\n\nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Amazon Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.\n\nAmazon Athena Benefits:\n\nvia - https://aws.amazon.com/athena/\n\nIncorrect options:\n\nLoad the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based sanity checks - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis.\nAs the development team would have to maintain and monitor the Amazon Redshift cluster size and would require significant development time to set up the processes to consume the data periodically, so this option is ruled out.\n\nLoad the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an Amazon EMR cluster would imply managing the underlying infrastructure so itâ€™s ruled out because the correct solution for the given use-case should require the least amount of development effort and ongoing maintenance.\n\nLoad the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity checks - Loading the incremental data into Amazon RDS implies data migration jobs will have to be written via a AWS Lambda function or an Amazon EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct.\n\nReference:\n\nhttps://aws.amazon.com/athena/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q573",
    "questionText": "A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power its video streaming application. To improve the performance of the application, the engineering team has also created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The security team at the company has noticed a spike in the number and types of SQL injection and cross-site scripting attack vectors on the application.\n\nAs a solutions architect, which of the following solutions would you recommend as the MOST effective in countering these malicious attacks?",
    "options": [
      {
        "text": "Use Amazon Route 53 with Amazon CloudFront distribution",
        "isCorrect": false
      },
      {
        "text": "Use AWS Firewall Manager with CloudFront distribution",
        "isCorrect": false
      },
      {
        "text": "Use AWS Security Hub with Amazon CloudFront distribution",
        "isCorrect": false
      },
      {
        "text": "Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution\n\nAWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.\n\nHow AWS WAF Works:\n\nvia - https://aws.amazon.com/waf/\n\nA web access control list (web ACL) gives you fine-grained control over the web requests that your Amazon CloudFront distribution, Amazon API Gateway API, or Application Load Balancer responds to.\n\nWhen you create a web ACL, you can specify one or more Amazon CloudFront distributions that you want AWS WAF to inspect. AWS WAF starts to allow, block, or count web requests for those distributions based on the conditions that you identify in the web ACL. Therefore, combining AWS WAF with Amazon CloudFront can prevent SQL injection and cross-site scripting attacks. So this is the correct option.\n\nIncorrect options:\n\nUse Amazon Route 53 with Amazon CloudFront distribution - Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. You cannot use Route 53 to prevent SQL injection and cross-site scripting attacks. So this option is incorrect.\n\nUse AWS Security Hub with Amazon CloudFront distribution - AWS Security Hub gives you a comprehensive view of your high-priority security alerts and security posture across your AWS accounts. With Security Hub, you have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, Amazon Macie, AWS Identity and Access Management (IAM) Access Analyzer, and AWS Firewall Manager, as well as from AWS Partner solutions. You cannot use Security Hub to prevent SQL injection and cross-site scripting attacks. So this option is incorrect.\n\nUse AWS Firewall Manager with CloudFront distribution - AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organization. You cannot use AWS Firewall Manager to prevent SQL injection and cross-site scripting attacks. So this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/waf/features/\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q574",
    "questionText": "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity between all VPCs.\n\nAs a solutions architect, which of the following would you recommend as the MOST resource-efficient and scalable solution?",
    "options": [
      {
        "text": "Establish VPC peering connections between all VPCs",
        "isCorrect": false
      },
      {
        "text": "Use AWS transit gateway to interconnect the VPCs",
        "isCorrect": true
      },
      {
        "text": "Use an internet gateway to interconnect the VPCs",
        "isCorrect": false
      },
      {
        "text": "Use a VPC endpoint to interconnect the VPCs",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS transit gateway to interconnect the VPCs\n\nAn AWS transit gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks.\n\nAWS Transit Gateway Overview:\n\nvia - https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html\n\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Transitive Peering does not work for VPC peering connections. So, if you have a VPC peering connection between VPC A and VPC B (pcx-aaaabbbb), and between VPC A and VPC C (pcx-aaaacccc). Then, there is no VPC peering connection between VPC B and VPC C. Instead of using VPC peering, you can use an AWS Transit Gateway that acts as a network transit hub, to interconnect your VPCs or connect your VPCs with on-premises networks. Therefore this is the correct option.\n\nVPC Peering Connections Overview:\n\nvia - https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html\n\nIncorrect options:\n\nUse an internet gateway to interconnect the VPCs - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. You cannot use an internet gateway to interconnect your VPCs and on-premises networks, hence this option is incorrect.\n\nUse a VPC endpoint to interconnect the VPCs - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. You cannot use a VPC endpoint to interconnect your VPCs and on-premises networks, hence this option is incorrect.\n\nEstablish VPC peering connections between all VPCs - Establishing VPC peering between all VPCs is an inelegant and clumsy way to establish connectivity between all VPCs. Instead, you should use a Transit Gateway that acts as a network transit hub to interconnect your VPCs and on-premises networks.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n\nhttps://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q575",
    "questionText": "A media company operates a web application that enables users to upload photos. These uploads are stored in an Amazon S3 bucket located in the eu-west-2 Region. To enhance performance and provide secure access under a custom domain name, the company wants to integrate Amazon CloudFront for uploads to the S3 bucket. The architecture must support secure HTTPS connections using a custom domain, and the upload process must ensure optimal speed and security.\n\nWhich combination of actions will fulfill these requirements? (Select two)",
    "options": [
      {
        "text": "Create a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations",
        "isCorrect": false
      },
      {
        "text": "Set up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL",
        "isCorrect": false
      },
      {
        "text": "Request a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution",
        "isCorrect": true
      },
      {
        "text": "Request a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution",
        "isCorrect": false
      },
      {
        "text": "Set up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC)",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nRequest a public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region and associate it with the CloudFront distribution\n\nCloudFront only supports ACM certificates that are created in the us-east-1 (N. Virginia) Region. Even if the content resides in a different Region (like eu-west-2), a public certificate for HTTPS on a custom domain name must originate from us-east-1. This certificate is used to secure content access through CloudFront over HTTPS.\n\nSet up Amazon S3 to accept uploads from CloudFront by enabling origin access control (OAC)\n\nOrigin Access Control (OAC) is the recommended method for granting CloudFront permission to upload to (write into) an S3 bucket securely. OAC supersedes the older Origin Access Identity (OAI) approach and supports both read and write operations. This allows you to restrict direct access to the S3 bucket and ensure that only CloudFront can act as a secure intermediary for uploads.\n\n\nvia - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\n\nIncorrect options:\n\nCreate a CloudFront distribution with an S3 static website endpoint as the origin and enable upload operations - This is incorrect because S3 static website endpoints only support GET and HEAD requests and are intended for public read access. They do not support PUT/POST operations required for user uploads. Therefore, they are not suitable for the secure upload workflow described.\n\nSet up a custom origin request policy in CloudFront that includes all viewer headers and query strings. Enable S3 Object Ownership to allow CloudFront to assume control of uploaded files via a signed URL - While customizing origin request policies and enabling S3 Object Ownership might help with some access controls, they do not provide a mechanism to upload content directly via CloudFront. Additionally, signed URLs are primarily used for download access, not for file uploads via CloudFront.\n\nRequest a public certificate from AWS Certificate Manager (ACM) in the eu-west-2 Region and associate it with the CloudFront distribution - This option is incorrect because CloudFront cannot use ACM public certificates that are created in any Region other than us-east-1. If a certificate is created in eu-west-2, it wonâ€™t be attachable to the CloudFront distribution.\n\nReferences:\n\nhttps://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q576",
    "questionText": "A company hosts a Microsoft SQL Server database on Amazon EC2 instances with attached Amazon EBS volumes. The operations team takes daily snapshots of these EBS volumes as backups. However, a recent incident occurred in which an automated script designed to clean up expired snapshots accidentally deleted all available snapshots, leading to potential data loss. The company wants to improve the backup strategy to avoid permanent data loss while still ensuring that old snapshots are eventually removed to optimize cost. A solutions architect needs to implement a mechanism that prevents immediate and irreversible deletion of snapshots.\n\nWhich solution will best meet these requirements with the least development effort?",
    "options": [
      {
        "text": "Enable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection",
        "isCorrect": false
      },
      {
        "text": "Set up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots",
        "isCorrect": true
      },
      {
        "text": "Implement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery",
        "isCorrect": false
      },
      {
        "text": "Set up the IAM policy of the user to deny EBS snapshot deletion",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots\n\nThe Amazon EBS Snapshot Recycle Bin allows organizations to retain deleted snapshots for a fixed duration before they are permanently deleted. By configuring a 7-day retention rule, snapshots that are deleted by mistake (e.g., through a faulty script) can still be recovered within the defined period. This provides protection from accidental deletion and satisfies the requirement to eventually remove old snapshotsâ€”thus balancing safety and cost. It requires minimal setup and no significant development effort.\n\nIncorrect Options:\n\nImplement a Lambda-based backup automation workflow that archives snapshot metadata in DynamoDB and stores backups in Amazon S3 Glacier Deep Archive for long-term recovery - This approach adds significant development effort and does not natively prevent snapshot deletion. So, it is incorrect.\n\nEnable AWS Backup Vault Lock on the backup vault and store EBS snapshots in that vault to enforce deletion protection - This option appears secure by referencing Vault Lock, which can enforce write-once, read-many (WORM) policies for backups. However, EBS snapshots are not directly stored in AWS Backup vaults unless they are created through AWS Backup, and the original setup is using manual or script-driven EBS snapshot creation. Additionally, Vault Lock applies only to backups managed by AWS Backup, not to independently created snapshots through Amazon EC2 or custom scripts. Therefore, while it introduces a secure mechanism, it does not address the current architecture or snapshot process, making it an unsuitable solution without significant re-architecture.\n\nSet up the IAM policy of the user to deny EBS snapshot deletion - While denying deletion can prevent accidental loss, it also completely blocks all snapshot deletions, including those that are genuinely expired or no longer needed. This violates the requirement to avoid indefinite snapshot retention, leading to storage bloat and increased cost.\n\nReferences:\n\nhttps://docs.aws.amazon.com/ebs/latest/userguide/recycle-bin.html\n\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q577",
    "questionText": "A fintech company recently conducted a security audit and discovered that some IAM roles and Amazon S3 buckets might be unintentionally shared with external accounts or publicly accessible. The security team wants to identify these overly permissive resources and ensure that only intended principals (within their AWS Organization or specific AWS accounts) have access. They need a solution that can analyze IAM policies and resource policies to detect unintended access paths to AWS resources such as S3 buckets, IAM roles, KMS keys, and SNS topics.\n\nWhich solution should the team use to meet this requirement?",
    "options": [
      {
        "text": "Use IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access",
        "isCorrect": false
      },
      {
        "text": "Use AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Inspector to detect over-permissive IAM policies and access paths across the environment",
        "isCorrect": false
      },
      {
        "text": "Use AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Identity and Access Management (IAM) Access Analyzer to evaluate resource-based and identity-based policies and identify resources shared outside the account or organization\n\nAWS IAM Access Analyzer helps identify resources shared with external entities by analyzing resource-based policies (e.g., S3 buckets, KMS keys, SNS topics, IAM roles) and detecting unintended access. It uses automated reasoning to evaluate the effects of policies and determines if a resource is accessible from outside the account or organization. Access Analyzer also supports policy validation, helping administrators craft secure and compliant access policies. Itâ€™s purpose-built for detecting and auditing external access paths to resources.\n\nIncorrect options:\n\nUse IAM Access Advisor to get detailed access analysis of S3 bucket policies and determine which principals outside the organization have access - IAM Access Advisor shows the last accessed information for AWS services used by IAM users and roles, but it does not evaluate resource-based policies or determine if resources are shared externally. It is designed to help with least privilege enforcement by identifying unused permissions, not external access analysis. It cannot detect whether S3 buckets or IAM roles are publicly accessible or shared with specific accounts.\n\nUse AWS Config to track configuration changes and infer resource-sharing behavior by analyzing compliance rules - AWS Config is used to record configuration changes and evaluate resource compliance against custom or managed rules. While it can track policy changes or detect public S3 access if configured properly, it does not natively reason over policy semantics to determine whether a policy grants external access. Config works best when used in conjunction with Access Analyzer for a comprehensive compliance and access strategy but does not replace Access Analyzerâ€™s policy evaluation capabilities.\n\nUse Amazon Inspector to detect over-permissive IAM policies and access paths across the environment - Amazon Inspector is primarily a vulnerability assessment tool focused on EC2 instances, container images, and Lambda functions. It checks for software vulnerabilities and security best practices, but it does not evaluate IAM policies or analyze whether resources like S3 buckets or roles are accessible to external accounts. It is not designed for access path analysis or policy-level visibility.\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html\n\nhttps://aws.amazon.com/blogs/security/set-permission-guardrails-using-iam-access-advisor-analyze-service-last-accessed-information-aws-organization/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q578",
    "questionText": "An enterprise is developing an internal compliance framework for its cloud infrastructure hosted on AWS. The enterprise uses AWS Organizations to group accounts under various organizational units (OUs) based on departmental function. As part of its governance controls, the security team mandates that all Amazon EC2 instances must be tagged to indicate the level of data classification â€” either 'confidential' or 'public'. Additionally, the organization must ensure that IAM users cannot launch EC2 instances without assigning a classification tag, nor should they be able to remove the tag from running instances. A solutions architect must design a solution to meet these compliance controls while minimizing operational overhead.\n\nWhich combination of steps will meet these requirements? (Select two)",
    "options": [
      {
        "text": "Create a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources",
        "isCorrect": false
      },
      {
        "text": "Use AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning",
        "isCorrect": false
      },
      {
        "text": "Define a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts",
        "isCorrect": true
      },
      {
        "text": "Enable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected",
        "isCorrect": false
      },
      {
        "text": "Create a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nDefine a tag policy in AWS Organizations that enforces the dataClassification key and restricts values to 'confidential' and 'public'. Attach this tag policy to the applicable organizational unit (OU) to enforce uniform tagging behavior across accounts\n\nAWS Tag Policies, managed through AWS Organizations, allow administrators to define rules for tag keys and values. These policies do not directly prevent resource creation, but they help enforce tag consistency and visibility by flagging noncompliant resources.\n\nIn this case, the solutions architect creates a tag policy that requires the dataClassification key and restricts its values to 'confidential' and 'public'. By attaching this policy to the relevant Organizational Unit (OU), the tag rules automatically apply to all member accounts. Tag policies work across AWS services that support tagging compliance checks. While they donâ€™t block actions like RunInstances, they play a governance and auditing role, helping organizations monitor and enforce consistent tagging.\n\nCreate a service control policy (SCP) that denies the ec2:RunInstances API action unless the required tag key is present in the request. Create a second SCP that denies the ec2:DeleteTags action for EC2 resources. Attach both SCPs to the relevant OU in AWS Organizations\n\nService Control Policies (SCPs) allow you to centrally govern permissions for accounts in AWS Organizations. An SCP defines the maximum set of permissions available to IAM users and roles in an account. When combined with condition keys, SCPs can enforce fine-grained controls on specific operations.\n\nTo meet the requirements - one SCP is created to deny the ec2:RunInstances action unless the request includes a dataClassification tag key and a second SCP explicitly denies the ec2:DeleteTags action on EC2 resources to ensure tags cannot be removed. These SCPs are then attached to the OU, thereby applying the policies to all member accounts. This ensures that - EC2 instances cannot be launched without the required tag and tags like dataClassification cannot be deleted, preserving compliance integrity.\n\n\nvia - https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html\n\nIncorrect options:\n\nCreate a tag enforcement Lambda function that runs on a schedule to identify EC2 instances without the required tag. The function sends a notification to administrators and optionally shuts down noncompliant resources - This reactive approach relies on periodic audits rather than enforcing tagging at the time of resource creation. It increases operational overhead and cannot guarantee immediate compliance, especially for short-lived resources.\n\nUse AWS Identity and Access Management (IAM) permission boundaries to restrict EC2-related actions unless the dataClassification tag is present. Apply these boundaries to all IAM roles used for EC2 provisioning - While IAM permission boundaries can restrict actions within a boundary, they are not designed to enforce tag-based controls across all users or to prevent tag deletion. SCPs and tag policies are more appropriate at the organizational level.\n\nEnable AWS Config rules to detect noncompliant EC2 instances. Trigger an AWS Systems Manager Automation runbook to reapply missing tags automatically when noncompliance is detected - AWS Config and automation runbooks offer remediation, but do not prevent noncompliant resources from being created in the first place. This violates the requirement for proactive enforcement and minimal public exposure of misclassified data.\n\nReferences:\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q579",
    "questionText": "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console.\n\nWhich of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?",
    "options": [
      {
        "text": "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance",
        "isCorrect": false
      },
      {
        "text": "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSetup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance\n\nUsing Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your Amazon EC2 instances. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.\n\nYou can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically reboots the instance. The reboot alarm action is recommended for Instance Health Check failures (as opposed to the recover alarm action, which is suited for System Health Check failures).\n\nIncorrect options:\n\nSetup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance\n\nUse Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance\n\nUse Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes\n\nUsing Amazon EventBridge event or Amazon CloudWatch alarm to trigger an AWS lambda function, directly or indirectly, is wasteful of resources. You should just use the EC2 Reboot CloudWatch Alarm Action to reboot the instance. So all the options that trigger the AWS lambda function are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q580",
    "questionText": "A media streaming company expects a major increase in user activity during the launch of a highly anticipated live event. The streaming platform is deployed on AWS and uses Amazon EC2 instances for the application layer and Amazon RDS for persistent storage. The operations team needs to proactively monitor system performance to ensure a smooth user experience during the event. Their monitoring setup must provide data visibility with intervals of no more than 2 minutes, and the team prefers a solution that is quick to implement and low-maintenance.\n\nWhich solution should the team implement?",
    "options": [
      {
        "text": "Use Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics",
        "isCorrect": false
      },
      {
        "text": "Enable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance",
        "isCorrect": true
      },
      {
        "text": "Stream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics",
        "isCorrect": false
      },
      {
        "text": "Install the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nEnable detailed monitoring on all EC2 instances and use Amazon CloudWatch metrics to track performance\n\nBy default, EC2 instances publish CloudWatch metrics at 5-minute intervals. Enabling detailed monitoring changes this to 1-minute intervals, allowing the team to observe system performance with the necessary granularity. CloudWatch metrics provide native integration with EC2 and are the most efficient way to analyze CPU, network, and disk utilization without complex setups.\n\n\nvia - https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-metrics-basic-detailed.html\n\nIncorrect options:\n\nUse Amazon EventBridge to collect EC2 state changes and publish them to Amazon SNS. Subscribe a monitoring dashboard to the SNS topic to visualize metrics - EventBridge and SNS are designed for event-driven architectures and alertingâ€”not for performance monitoring or detailed metric analysis. This approach would not provide the continuous, granular data the company needs to evaluate real-time performance trends. Moreover, SNS is not built to directly support monitoring dashboards.\n\nStream EC2 system logs to an Amazon OpenSearch Service domain for real-time indexing and visualization. Use OpenSearch Dashboards to monitor CPU and memory metrics - While Amazon OpenSearch Service is excellent for log indexing and search, it does not natively collect EC2 performance metrics like CPU utilization or disk I/O. To make this work, the company would need to implement custom log collection and parsing, which adds unnecessary complexity compared to using CloudWatch.\n\nInstall the CloudWatch agent on all EC2 instances. Configure the agent to collect high-resolution custom metrics and stream them to CloudWatch Logs for analysis via Amazon Athena - Although this setup could support fine-grained metric collection, it introduces significant operational overhead. It requires manual agent installation, configuration, and maintenance across all instances. Analyzing logs via Athena also introduces query latency, which is not ideal for real-time performance tracking.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-metrics-basic-detailed.html\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\n\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q581",
    "questionText": "Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue using a serverless storage solution such as Amazon S3 but wants to improve the performance.\n\nAs a solutions architect, which of the following solutions do you propose to address this issue? (Select two)",
    "options": [
      {
        "text": "Create new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets",
        "isCorrect": false
      },
      {
        "text": "Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files",
        "isCorrect": true
      },
      {
        "text": "Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files",
        "isCorrect": true
      },
      {
        "text": "Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nUse Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files\n\nAmazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, within a developer-friendly environment.\nWhen an object from Amazon S3 that is set up with Amazon CloudFront CDN is requested, the request would come through the Edge Location transfer paths only for the first request. Thereafter, it would be served from the nearest edge location to the users until it expires. So in this way, you can speed up uploads as well as downloads for the video files.\n\nFollowing is a good reference blog for a deep-dive:\n\nhttps://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/\n\nEnable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up uploads as well as downloads for the video files\n\nAmazon S3 Transfer Acceleration (Amazon S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of Amazon CloudFrontâ€™s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. So this option is also correct.\n\nAmazon S3TA:\n\nvia - https://aws.amazon.com/s3/transfer-acceleration/\n\nIncorrect options:\n\nCreate new Amazon S3 buckets in every region where the agency has a remote office, so that each office can maintain its storage for the media assets - Creating new Amazon S3 buckets in every region is not an option, since the agency maintains centralized storage. Hence this option is incorrect.\n\nMove Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering connection\n\nSpin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances\n\nBoth these options using Amazon EC2 instances are not correct for the given use-case, as the agency wants a serverless storage solution.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\n\nhttps://aws.amazon.com/s3/transfer-acceleration/\n\nhttps://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q582",
    "questionText": "A company needs a massive PostgreSQL database and the engineering team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an Amazon EC2 instance with the optimal storage type on the attached Amazon EBS volume.\n\nAs a solutions architect, which of the following configurations would you suggest to the engineering team?",
    "options": [
      {
        "text": "Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type",
        "isCorrect": false
      },
      {
        "text": "Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type",
        "isCorrect": true
      },
      {
        "text": "Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type",
        "isCorrect": false
      },
      {
        "text": "Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type\n\nAmazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications.\n\nThe volumes types fall into two categories:\n\nSSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS\n\nHDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS\n\nProvision IOPS type supports critical business applications that require sustained IOPS performance, or more than 16,000 IOPS or 250 MiB/s of throughput per volume. Examples are large database workloads, such as:\nMongoDB\nCassandra\nMicrosoft SQL Server\nMySQL\nPostgreSQL\nOracle\n\nTherefore, Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type is the right fit for the given use-case.\n\nPlease see this detailed overview of the volume types for Amazon EBS volumes.\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\n\nIncorrect options:\n\nAmazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type\n\nAmazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type\n\nAmazon EC2 with Amazon EBS volume of cold HDD (sc1) type\n\nPer the explanation in the detailed overview provided above, these three options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q583",
    "questionText": "A digital media startup allows users to submit images through its web portal. These images are uploaded directly into an Amazon S3 bucket. On average, around 200 images are uploaded daily. The company wants to automatically generate a smaller preview version (thumbnail) of each new image and store the resulting thumbnails in a separate Amazon S3 bucket. The team prefers a design that is low-cost, requires minimal infrastructure management, and automatically reacts to new uploads.\n\nWhich solution will meet these requirements MOST cost-effectively?",
    "options": [
      {
        "text": "Set up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket",
        "isCorrect": false
      },
      {
        "text": "Deploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket",
        "isCorrect": false
      },
      {
        "text": "Enable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail",
        "isCorrect": false
      },
      {
        "text": "Configure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nConfigure the S3 bucket to send an event notification to an AWS Lambda function each time a new image is uploaded. Use the Lambda function to process the image, create a thumbnail, and store the thumbnail in the second S3 bucket\n\nThis approach follows a serverless, event-driven architecture that is highly cost-effective, scalable, and operationally efficient. The solution involves configuring Amazon S3 to send an event notification to an AWS Lambda function each time a new photo is uploaded to the primary S3 bucket. When triggered, the Lambda function processes the uploaded image, generates a thumbnail and stores it in a separate S3 bucket. This event-driven architecture ensures near real-time processing with minimal operational overhead. Because AWS Lambda is a fully managed, serverless compute service, it runs only when invoked and scales automatically, making it highly cost-effective for handling approximately 200 uploads per day. Additionally, S3 event notifications are natively supported and integrate seamlessly with Lambda, eliminating the need for persistent infrastructure or scheduled jobs. This solution provides scalability, low latency, and cost efficiency while ensuring clean separation of original images and their thumbnails.\n\nIncorrect options:\n\nSet up a step-based processing workflow using AWS Glue jobs triggered on a regular interval. Use the jobs to scan the primary S3 bucket for new files and generate thumbnails for any that lack them. Write the thumbnails to a second S3 bucket - AWS Glue is designed for ETL (Extract, Transform, Load) operations on structured and semi-structured data, not real-time image processing. Running Glue jobs on a schedule for this use case introduces unnecessary complexity and cost.\n\nDeploy a containerized application on AWS Fargate that polls the S3 bucket every minute to detect new uploads. Configure the container to generate thumbnails and save them in the second bucket - While Fargate is serverless, continuously polling the S3 bucket is inefficient and incurs costs even when no new images are present. It also adds operational complexity compared to event-driven processing.\n\nEnable Amazon S3 Access Analyzer and configure it to call an AWS Lambda function whenever a new image is added. Use the Lambda function to generate and store the thumbnail - S3 Access Analyzer is used for auditing and security analysis, not event-driven processing or triggering Lambda functions. It cannot be used to detect object uploads or initiate thumbnail generation.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-analyzer.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q584",
    "questionText": "An organization operates a legacy reporting tool hosted on an Amazon EC2 instance located within a public subnet of a VPC. This tool aggregates scanned PDF reports from field devices and temporarily stores them on an attached Amazon EBS volume. At the end of each day, the tool transfers the accumulated files to an Amazon S3 bucket for archival. A solutions architect identifies that the files are being uploaded over the internet using S3's public endpoint. To improve security and avoid exposing data traffic to the public internet, the architect needs to reconfigure the setup so that uploads to Amazon S3 occur privately without using the public S3 endpoint.\n\nWhich solution will fulfill these requirements?",
    "options": [
      {
        "text": "Create a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instanceâ€™s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint",
        "isCorrect": true
      },
      {
        "text": "Create an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data",
        "isCorrect": false
      },
      {
        "text": "Set up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway",
        "isCorrect": false
      },
      {
        "text": "Provision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a gateway VPC endpoint for Amazon S3 in the VPC. Ensure that the EC2 instanceâ€™s subnet route table is updated to route S3 traffic through the endpoint. Confirm that appropriate IAM policies are in place to permit access via the VPC endpoint\n\nA gateway VPC endpoint for Amazon S3 enables private connectivity between your Amazon VPC and S3 without routing traffic over the public internet. By creating a gateway VPC endpoint, the EC2 instance uploads data to S3 over AWS's internal network, ensuring the traffic remains entirely within the AWS infrastructure. This approach satisfies the requirement to avoid using the public endpoint, enhances security, and minimizes exposure to external threats. Additionally, route table entries in the public subnet can be updated to direct all S3-bound traffic through the VPC endpoint, and IAM or bucket policies can further restrict access to only traffic coming from the VPC endpoint. This solution requires no application changes and involves minimal operational overhead, making it both secure and efficient.\n\nThere is no additional charge for using gateway endpoints. Amazon S3 supports both gateway endpoints and interface endpoints. With a gateway endpoint, you can access Amazon S3 from your VPC, without requiring an internet gateway or NAT device for your VPC, and with no additional cost. However, gateway endpoints do not allow access from on-premises networks, from peered VPCs in other AWS Regions, or through a transit gateway.\n\nIncorrect options:\n\nCreate an S3 access point within the same Region and attach a policy that grants the EC2 instance access. Update the application to use the access point alias to upload data - While Amazon S3 access points simplify managing access to shared buckets across many applications or teams, they do not change the network path used to reach Amazon S3. Without a VPC endpoint, traffic using an access point still flows over the public internet. This option does not meet the requirement to avoid using S3â€™s public endpoint.\n\nSet up a NAT gateway in the public subnet and modify the route table of the EC2 instance's subnet to direct Amazon S3 traffic through the NAT gateway - Although NAT gateways can route traffic to the internet, they do not provide private access to S3. Traffic sent through a NAT gateway to S3 still uses public endpoints and incurs data transfer charges. This does not satisfy the requirement to avoid public endpoint usage.\n\nProvision a dedicated AWS Direct Connect link to route traffic from the VPC to Amazon S3 privately - Direct Connect is ideal for high-throughput, low-latency, hybrid workloads where traffic flows from on-premises to AWS. However, it is unnecessary for this use case, which involves internal traffic between EC2 and S3 within AWS.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q585",
    "questionText": "A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt for Multi-AZ deployment and they would like to understand what happens when the primary instance of the Multi-AZ configuration goes down.\n\nAs a Solutions Architect, which of the following will you identify as the outcome of the scenario?",
    "options": [
      {
        "text": "An email will be sent to the System Administrator asking for manual intervention",
        "isCorrect": false
      },
      {
        "text": "The CNAME record will be updated to point to the standby database",
        "isCorrect": true
      },
      {
        "text": "The URL to access the database will change to the standby database",
        "isCorrect": false
      },
      {
        "text": "The application will be down until the primary database has recovered itself",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThe CNAME record will be updated to point to the standby database\n\nAmazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology. SQL Server DB instances use SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs).\n\nIn a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption.\n\nFailover is automatically handled by Amazon RDS so that you can resume database operations as quickly as possible without administrative intervention. When failing over, Amazon RDS simply flips the canonical name record (CNAME) for your DB instance to point at the standby, which is in turn promoted to become the new primary. Multi-AZ means the URL is the same, the failover is automated, and the CNAME will automatically be updated to point to the standby database.\n\nIncorrect options:\n\nThe URL to access the database will change to the standby database - As discussed above, URL remains the same.\n\nAn email will be sent to the System Administrator asking for manual intervention - This option is incorrect and it has been added as a distractor.\n\nThe application will be down until the primary database has recovered itself - This option is incorrect and it has been added as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\n\nhttps://aws.amazon.com/rds/faqs/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q586",
    "questionText": "A logistics company runs a two-step job handling process on AWS. The first step quickly receives job submissions from clients, while the second step requires longer processing time to complete each job. Currently, both steps run on separate Amazon EC2 Auto Scaling groups. However, during high-demand hours, the job processing stage falls behind, and there is concern that jobs may be lost due to instance termination during scaling events. A solutions architect needs to design a more scalable and reliable architecture that preserves job data and accommodates fluctuating demand in both stages.\n\nWhich solution will meet these requirements?",
    "options": [
      {
        "text": "Set up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue",
        "isCorrect": false
      },
      {
        "text": "Configure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior",
        "isCorrect": false
      },
      {
        "text": "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue",
        "isCorrect": false
      },
      {
        "text": "Set up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nSet up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on number of messages in each queue\n\nThis solution effectively addresses both the need for scalability and the requirement to avoid data loss. By introducing Amazon SQS queues between the two processing stages (order collection and order fulfillment), the architecture becomes decoupled and resilient. One queue buffers incoming orders, while the second queue holds orders awaiting fulfillment, ensuring no data is lost even when processing slows during peak traffic. Each group of EC2 instances polls its respective queue for work, and Auto Scaling policies can be configured based on the number of messages in each queue. This allows the system to dynamically scale based on real-time demand, ensuring responsiveness without over-provisioning resources. This approach minimizes latency for order intake while allowing asynchronous processing of order fulfillment, leading to a highly available and cost-efficient solution.\n\nIncorrect options:\n\nSet up a single Amazon SQS queue for both the job intake and job processing stages. Assign the SQS queue to collect incoming jobs as well as processing jobs. Configure all EC2 instances to poll this queue. Scale the Auto Scaling groups based on number of messages in the queue - This option is incorrect because using a single Amazon SQS queue for both job intake and job processing introduces ambiguity and tight coupling between two logically distinct stagesâ€”order collection and order fulfillment. In this architecture, the order intake is fast, while order fulfillment is slower and more resource-intensive. By combining both types of messages into a single SQS queue, thereâ€™s no way to prioritize, distinguish, or independently scale resources for each task. For example, the fulfillment EC2 instances might mistakenly process intake-type messages or vice versa unless complex custom filtering logic is implemented, which increases operational complexity and risk.\n\nConfigure each Auto Scaling group to maintain its maximum expected size during peak hours by setting a fixed minimum capacity. Monitor CPUUtilization through Amazon CloudWatch to ensure consistent scaling behavior - This approach pre-provisions the infrastructure for peak traffic, using CPUUtilization as a general metric. While this reduces the risk of lag due to insufficient instances, it lacks flexibility and may result in over-provisioning, leading to unnecessary costs. It also does not guarantee data persistence if instances terminate unexpectedly during processing, since thereâ€™s no queue-based buffering.\n\nSet up two Amazon SQS queues to decouple the job intake and job processing stages respectively. Assign one SQS queue to collect incoming jobs, and another to queue them for processing. Configure the EC2 instances to poll the relevant queue. Scale the Auto Scaling groups based on notifications from each queue - While this solution correctly suggests decoupling the order collection and order fulfillment processes using Amazon SQS queues, it proposes using notifications from the queues to trigger scaling. This acts as a distractor, since SQS queues do not support sending notifications. Notifications (e.g., using Amazon SNS) are not the most effective mechanism for dynamic and responsive scaling. The more reliable and recommended approach is to scale based on the number of pending messages in each queue.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q587",
    "questionText": "A mobile-based e-learning platform is migrating its backend storage layer to Amazon DynamoDB to support a rapidly increasing number of student users and learning transactions. The platform must ensure seamless availability and minimal disruption for a global user base. The DynamoDB design must provide low-latency performance, high availability, and automatic fault tolerance across geographies with the lowest possible operational overhead and cost.\n\nWhich solution will fulfill these needs in the most cost-efficient manner?",
    "options": [
      {
        "text": "Use DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability",
        "isCorrect": true
      },
      {
        "text": "Enable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions",
        "isCorrect": false
      },
      {
        "text": "Deploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions",
        "isCorrect": false
      },
      {
        "text": "Create separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse DynamoDB global tables for automatic multi-Region replication. Enable provisioned capacity mode with auto scaling to optimize cost and ensure consistent availability\n\nDynamoDB global tables offer a native, multi-Region solution that provides high availability and resiliency by replicating data automatically across AWS Regions. This ensures users around the world experience low-latency read and write operations. By using provisioned capacity mode with auto scaling, the company can optimize cost based on actual throughput requirements rather than overprovisioning resources. This approach eliminates the need for manual replication or custom logic, and it ensures continuous availability in the event of a Regional disruption.\n\nIncorrect options:\n\nEnable DynamoDB Accelerator (DAX) to reduce response time for read operations. Deploy DAX in one Region, and use scheduled Lambda functions to replicate data to other Regions - DAX improves read performance through in-memory caching, but it does not help with write scalability, cross-Region replication, or resiliency. Scheduled Lambda functions for replication are not real-time, can fail under high load, and add complexity to the architecture. This setup does not fulfill the requirement for continuous availability and resilience for a global user base.\n\nDeploy separate DynamoDB tables in each required AWS Region using on-demand capacity mode. Implement a custom cross-Region replication mechanism by streaming data changes with DynamoDB Streams and processing them through AWS Lambda functions - While DynamoDB Streams can capture real-time data modifications in a table, using them for manual cross-Region replication requires building and maintaining custom replication logicâ€”typically with AWS Lambda functions or AWS Glue jobs. This approach introduces significant complexity, potential latency, and increased operational burden. It also lacks automated conflict resolution, version control, and automatic failover capabilities. Additionally, this setup doesn't guarantee strong consistency across Regions, and managing data conflicts or write ordering becomes challenging.\n\nCreate separate DynamoDB tables in multiple Regions. Use AWS Data Pipeline to synchronize data periodically between Regions to maintain availability - Manually managing multiple DynamoDB tables across Regions using AWS Data Pipeline introduces significant operational complexity and replication delays. This method does not ensure real-time synchronization or automatic failover, making it unsuitable for high-throughput, latency-sensitive applications like online gaming. It also requires custom conflict resolution and lacks the seamless integration offered by global tables.\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html",
    "awsService": "Auto Scaling",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q588",
    "questionText": "A digital content production company has transitioned all of its media assets to Amazon S3 in an effort to reduce storage costs. However, the rendering engine used in production continues to run in an on-premises data center and requires frequent and low-latency access to large media files. The company wants to implement a storage solution that maintains application performance while keeping costs low.\n\nWhich approach should the company choose to meet these requirements in the most cost-effective way?",
    "options": [
      {
        "text": "Use Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon S3 File Gateway to provide storage for the on-premises application",
        "isCorrect": true
      },
      {
        "text": "Set up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory",
        "isCorrect": false
      },
      {
        "text": "Deploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up an Amazon S3 File Gateway to provide storage for the on-premises application\n\nAmazon S3 File Gateway is designed specifically for on-premises applications that need low-latency access to S3 data. It caches frequently accessed data locally and exposes the S3 bucket as an NFS or SMB file share, making it ideal for performance-sensitive workloads like rendering. It is also cost-effective as the majority of data remains in S3.\n\n\nvia - https://docs.aws.amazon.com/filegateway/latest/files3/what-is-file-s3.html\n\nIncorrect options:\n\nDeploy an Amazon FSx for Lustre file system and sync media data from Amazon S3 into it using DataSync. Mount the FSx file system on the on-premises render servers using a VPN tunnel - While Amazon FSx for Lustre is a high-performance file system suited for HPC and rendering workloads, it incurs higher costs than simpler alternatives and is typically optimized for cloud-based compute rather than on-premises usage. Additionally, the use of AWS DataSync and VPN tunneling introduces complexity and potential performance bottlenecks for consistent two-way file access.\n\nUse Mountpoint for Amazon S3 on the on-premises rendering servers to facilitate low-latency access to the S3 bucket - Mountpoint for Amazon S3 is an open source file client that makes it easy for your file-aware Linux applications to connect directly to Amazon S3 buckets. It can also be installed on your existing on-premises systems, with access to S3 either directly or over an AWS Direct Connect connection via AWS PrivateLink for Amazon S3. It is not POSIX compliant. It is not intended for low-latency or local caching use-cases from on-premises environments.\n\nSet up a dedicated on-premises storage array that periodically fetches data from Amazon S3 using a custom-built application. Mount this storage volume on the rendering servers as their primary working directory - This solution requires developing and maintaining a custom integration layer to periodically download files from S3 and sync with local storage. It increases operational burden and introduces latency and consistency issues. Additionally, the costs of maintaining on-premises hardware negate the cost benefits of moving to S3 in the first place.\n\nReferences:\n\nhttps://docs.aws.amazon.com/filegateway/latest/files3/what-is-file-s3.html\n\nhttps://aws.amazon.com/blogs/aws/mountpoint-for-amazon-s3-generally-available-and-ready-for-production-workloads/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q589",
    "questionText": "An e-commerce company uses a two-tier architecture with application servers in the public subnet and an Amazon RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to access the MySQL database and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection timed out\" error messages.\n\nWhich of the following options represent the root cause for this issue?",
    "options": [
      {
        "text": "The security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers",
        "isCorrect": true
      },
      {
        "text": "The database user credentials (username and password) configured for the application are incorrect",
        "isCorrect": false
      },
      {
        "text": "The security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance",
        "isCorrect": false
      },
      {
        "text": "The database user credentials (username and password) configured for the application do not have the required privilege for the given database",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThe security group configuration for the database instance does not have the correct rules to allow inbound connections from the application servers\n\nYou should use security groups to control the inbound and outbound traffic for your database instance. For your application servers, create a security group with inbound rules that use the IP addresses of the client application as the source. This security group allows your client application to connect to your application servers. Then create a second security group for your database instance and create a new rule by specifying the security group that you created earlier as the source for this database-specific security group.\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\n\nIncorrect options:\n\nThe security group configuration for the application servers does not have the correct rules to allow inbound connections from the database instance - As mentioned in the explanation above, the application servers don't need inbound connections from the database instance, rather the database instance needs the correct inbound rule with application servers' security group as the source.\n\nThe database user credentials (username and password) configured for the application are incorrect\n\nThe database user credentials (username and password) configured for the application do not have the required privilege for the given database\n\nThese two options have been added as a distractor since the error mentions a \"connection timeout\" issue rather than an \"access denied\" error.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q590",
    "questionText": "A multinational logistics company operates its shipment tracking platform from Amazon EC2 instances deployed in the AWS us-west-2 Region. The platform exposes a set of APIs over HTTPS, which are used by logistics partners and customers around the world to retrieve real-time tracking data. The company has observed that users from Europe and Asia experience latency issues and inconsistent API response times when accessing the service. As a cloud architect, you have been tasked to propose the most cost-effective solution to improve performance for these international users without migrating the application.\n\nWhich solution should you recommend?",
    "options": [
      {
        "text": "Deploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency",
        "isCorrect": false
      },
      {
        "text": "Set up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups",
        "isCorrect": true
      },
      {
        "text": "Deploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up AWS Global Accelerator with listeners configured for HTTPS. Create endpoint groups for Europe and Asia and add the existing us-west-2 EC2 endpoint to these groups\n\nThis option is correct because AWS Global Accelerator is purpose-built to improve the performance and availability of applications accessed by global users, without requiring infrastructure replication across Regions. It uses static anycast IP addresses and routes user traffic through the highly optimized AWS global network, reducing latency and improving consistency by avoiding unpredictable internet routes. In this scenario, Global Accelerator receives API requests at edge locations near users in Europe and Asia and forwards them efficiently to the EC2 instances in the us-west-2 Region. This approach enhances response times without altering the existing application architecture or incurring the high costs associated with deploying and managing multiple regional stacks. It is a cost-effective and scalable solution for optimizing global access to Regionally-hosted APIs.\n\nIncorrect options:\n\nDeploy an Amazon CloudFront distribution in front of the API endpoint and apply the CachingOptimized managed policy to enhance caching behavior and improve content delivery efficiency - This option is incorrect because Amazon CloudFront is primarily optimized for delivering static or cacheable content, and is not well-suited for APIs that serve highly dynamic or personalized responses, such as those in a shipment tracking system. These APIs typically return user-specific or real-time data that cannot be effectively cached, even with the use of the CachingOptimized managed cache policy. As a result, most requests would result in cache misses, causing CloudFront to forward the traffic to the origin in the us-west-2 Region. This adds an extra network hop and can actually increase latency for international users rather than reduce it. Furthermore, using CloudFront in this scenario adds unnecessary complexity and incurs additional costs without delivering significant performance improvements, making it a suboptimal and non-cost-effective solution for reducing API response times.\n\nUse Amazon Route 53 latency-based routing to direct user requests to a copy of the EC2 API deployed in each major geographic Region - Latency-based routing works well when application stacks are deployed in multiple Regions. However, this approach involves replicating the EC2-based API infrastructure across multiple Regions, significantly increasing compute, data transfer, and management overhead. This is not the most cost-effective option as requested in the scenario.\n\nDeploy Amazon API Gateway in multiple AWS Regions and synchronize the API definitions. Use AWS Lambda as a proxy to forward requests to the EC2-hosted API in us-west-2 - While deploying API Gateway in multiple Regions might help reduce DNS resolution time and provide regional endpoints, using Lambda to proxy requests back to a central Region introduces additional latency and unnecessary costs. This design offloads little from the original EC2 endpoints and does not leverage edge optimization services like CloudFront or Global Accelerator. Additionally, synchronizing API definitions and handling cross-region Lambda invocation can be complex.\nReference:\n\nReferences:\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html\n\nhttps://aws.amazon.com/global-accelerator/features/#Use_cases\n\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-endpoint-types.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q591",
    "questionText": "A DevOps team is tasked with enabling secure and temporary SSH access to Amazon EC2 instances for developers during deployments. The team wants to avoid distributing long-term SSH key pairs and instead prefers ephemeral access that can be audited and revoked immediately after the session ends. The team wants direct access via the AWS Management Console.\n\nWhat do you recommend?",
    "options": [
      {
        "text": "Use EC2 Instance Connect to inject a temporary public key and establish SSH access using the instanceâ€™s public IP address",
        "isCorrect": true
      },
      {
        "text": "Use EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet",
        "isCorrect": false
      },
      {
        "text": "Use EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint",
        "isCorrect": false
      },
      {
        "text": "Use an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse EC2 Instance Connect to inject a temporary public key and establish SSH access using the instanceâ€™s public IP address\n\nAmazon EC2 Instance Connect provides a secure and auditable way to establish temporary SSH sessions to EC2 instances by injecting a one-time-use public key into the instance at connection time. When used without Session Manager, EC2 Instance Connect requires the instance to have a public IP address, and connections are made over the internet via the public IP. This setup is ideal for temporary administrative access to public instances without the need to distribute or manage long-term SSH key pairs. The session is short-lived, and logs are available via CloudTrail for auditing purposes.\n\nIncorrect options:\n\nUse EC2 Instance Connect to inject a static SSH key and connect via the instance's private IP address directly from the internet - EC2 Instance Connect does not use static SSH keys and does not support connecting over private IPs from the internet unless paired with AWS Systems Manager Session Manager. This option incorrectly assumes that private IPs are routable from the public internet, which is not the case in VPC networking. Additionally, using a static key contradicts the security design of EC2 Instance Connect, which relies on ephemeral keys to minimize risk.\n\nUse EC2 Instance Connect with Systems Manager Agent disabled, and connect via private IP using an internal proxy endpoint - EC2 Instance Connect does not support connecting via private IP unless integrated with Session Manager, which requires that the SSM Agent is installed and running on the instance. This option incorrectly states that you can connect via private IP without SSM Agent or Session Manager, which is technically invalid. Instance Connect without Session Manager only supports public IP-based access.\n\nUse an EC2 Instance Connect Endpoint to reach the instances even though they already have public IP addresses, because Instance Connect requires an endpoint for all SSH sessions - EC2 Instance Connect does not require an EC2 Instance Connect Endpoint when the target instance has a public IPv4 address; in that case you can connect directly over the internet and have a temporary SSH public key injected at connection time. EC2 Instance Connect Endpoints are specifically intended to let you connect to instances in private subnets without public IPs or direct internet connectivity, providing a managed entry point within your VPC. Adding an endpoint for publicly reachable instances adds unnecessary operational overhead and cost, and it misinterprets the featureâ€™s purpose.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-methods.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-with-ec2-instance-connect-endpoint.html\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q592",
    "questionText": "A streaming service provider collects user experience feedback through embedded feedback forms in their mobile and web apps. Feedback submissions frequently spike to thousands per hour during content launches or service outages. Currently, the feedback is sent via email to the operations team for manual review. The company now wants to automate feedback collection and sentiment analysis so that insights can be generated quickly and stored for a full year for trend analysis.\n\nWhich solution provides the most scalable and automated approach to meet these requirements?",
    "options": [
      {
        "text": "Design a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item",
        "isCorrect": true
      },
      {
        "text": "Build a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months",
        "isCorrect": false
      },
      {
        "text": "Route all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDesign a RESTful API with Amazon API Gateway that forwards incoming feedback data to an Amazon SQS queue. Set up an AWS Lambda function to process the queue messages, analyze sentiment using Amazon Comprehend, and store results in a DynamoDB table with a 365-day TTL configured on each item\n\nThis architecture decouples feedback ingestion from processing by using Amazon SQS as a buffer for potentially high volumes of incoming data. API Gateway handles secure and scalable API access, while Lambda automatically scales to process feedback messages asynchronously. Amazon Comprehend is specifically designed for sentiment analysis on text input. Storing results in DynamoDB offers scalable, low-latency access to structured insights, and Time to Live (TTL) ensures automatic data expiry after one year. This is a serverless, fully managed solution that scales efficiently with bursts and long-term retention.\n\nIncorrect options:\n\nRoute all feedback submissions through Amazon Kinesis Data Streams. Use an AWS Lambda consumer to batch process incoming records, invoke Amazon Translate to detect language and convert input to English, and save the processed content in an Amazon OpenSearch Service index. Configure OpenSearch Index State Management (ISM) policies to delete documents after 12 months - This option introduces Kinesis Data Streams to handle high-throughput ingestion, which is appropriate for large-scale, real-time data capture. However, the use of Amazon Translate is misaligned with the core requirement, which is sentiment analysis, not language translation. Additionally, storing feedback data in Amazon OpenSearch Service can enable full-text search and visualization, but it adds significant operational overhead, especially for a solution that doesn't require search capabilities. Furthermore, OpenSearch is less cost-effective and scalable for basic structured storage when compared to DynamoDB for this use case.\n\nUse Amazon EventBridge to capture feedback events and forward them to an AWS Step Functions workflow. The workflow invokes Lambda functions for validation, calls Amazon Transcribe to convert the text to audio for archival, and stores the results in an Amazon RDS database. Configure a lifecycle policy to remove records after 12 months - This option introduces unnecessary complexity and the wrong tools for the job. Amazon Transcribe is used to convert speech to text, not the other way around, and has no benefit in archiving text-based user feedback. Storing the output in Amazon RDS increases management burden and doesnâ€™t scale as well as DynamoDB for high-velocity, write-heavy workloads. Although EventBridge and Step Functions are powerful for orchestrating workflows, this particular implementation misuses services and introduces overhead without delivering meaningful improvements.\n\nBuild a web service on Amazon EC2 that receives feedback data and stores each record in a DynamoDB table. Use the EC2 application to invoke Amazon Comprehend for sentiment detection and write results to a second table. Apply a TTL of 365 days to each table - Although technically functional, this solution uses Amazon EC2 for feedback ingestion and sentiment analysis, which introduces operational overhead, limits scalability, and requires provisioning, patching, and scaling of EC2 instances. This is not a serverless or event-driven architecture, and the use of a second DynamoDB table adds unnecessary complexity. While Amazon Comprehend is correctly applied, the solution lacks elasticity for sudden spikes and isn't the most cost-effective or scalable approach.\n\nReferences:\n\nhttps://docs.aws.amazon.com/comprehend/latest/dg/what-is.html\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\n\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/ism.html\n\nhttps://docs.aws.amazon.com/transcribe/latest/dg/what-is.html\n\nhttps://docs.aws.amazon.com/translate/latest/dg/what-is.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q593",
    "questionText": "You have built an application that is deployed with Elastic Load Balancing and an Auto Scaling Group. As a Solutions Architect, you have configured aggressive Amazon CloudWatch alarms, making your Auto Scaling Group (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue, because the instance has already been terminated by the Auto Scaling Group. The log files are saved on the Amazon EC2 instance.\n\nHow will you resolve the issue and make sure it doesn't happen again?",
    "options": [
      {
        "text": "Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch",
        "isCorrect": true
      },
      {
        "text": "Disable the Termination from the Auto Scaling Group any time a user reports an issue",
        "isCorrect": false
      },
      {
        "text": "Make a snapshot of the Amazon EC2 instance just before it gets terminated",
        "isCorrect": false
      },
      {
        "text": "Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nInstall an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon CloudWatch\n\nYou can use the Amazon CloudWatch Logs agent installer on an existing Amazon EC2 instance to install and configure the Amazon CloudWatch Logs agent. After installation is complete, logs automatically flow from the instance to the log stream you create while installing the agent. The agent confirms that it has started and it stays running until you disable it.\n\nHere, the natural and by far the easiest solution would be to use the Amazon CloudWatch Logs agents on the Amazon EC2 instances to automatically send log files into Amazon CloudWatch, so we can analyze them in the future easily should any problem arise.\n\nTo control whether an Auto Scaling group can terminate a particular instance when scaling in, use instance scale-in protection. You can enable the instance scale-in protection setting on an Auto Scaling group or on an individual Auto Scaling instance. When the Auto Scaling group launches an instance, it inherits the instance scale-in protection setting of the Auto Scaling group. You can change the instance scale-in protection setting for an Auto Scaling group or an Auto Scaling instance at any time.\n\nIncorrect options:\n\nDisable the Termination from the Auto Scaling Group any time a user reports an issue - Disabling the Termination from the Auto Scaling Group would prevent our Auto Scaling Group from being Elastic and impact our costs. Therefore this option is incorrect.\n\nMake a snapshot of the Amazon EC2 instance just before it gets terminated - Making a snapshot of the Amazon EC2 instance before it gets terminated could work but it's tedious, not elastic and very expensive, since our interest is just the log files. Therefore this option is not the best fit for the given use-case.\n\nYou can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data.\n\nUse AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3 - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics. Using AWS Lambda would be extremely hard to use for this task. Therefore this option is not the best fit for the given use-case.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q594",
    "questionText": "A digital media company runs its content rendering service on Amazon EC2 instances that are registered with an Application Load Balancer (ALB) using IP-based target groups. The company relies on AWS Systems Manager to manage and patch these instances regularly. According to new compliance requirements, EC2 instances must be safely removed from production traffic during patching to prevent user disruption and maintain application integrity. However, during the most recent patch cycle, the operations team noticed application failures and API timeouts, even though patching succeeded on the instances. You are asked to suggest a reliable and scalable way to ensure safe patching while preserving service availability.\n\nWhich solution will best meet the new compliance and operational requirements? (Select two)",
    "options": [
      {
        "text": "Configure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes",
        "isCorrect": false
      },
      {
        "text": "Modify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata",
        "isCorrect": false
      },
      {
        "text": "Use AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching",
        "isCorrect": true
      },
      {
        "text": "Use Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window",
        "isCorrect": false
      },
      {
        "text": "Configure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nUse AWS Systems Manager Automation with the AWSEC2-PatchLoadBalancerInstance document to manage patching\n\nThe AWSEC2-PatchLoadBalancerInstance Systems Manager Automation document is specifically designed for patching EC2 instances that are part of a load balancer. It automatically removes the instance from the ALB target group, waits for in-flight requests to complete, applies patches, performs reboots if needed, and then safely re-registers the instance. This workflow prevents application downtime or request failures during patching and ensures compliance with security policies.\n\nConfigure Systems Manager Maintenance Windows to coordinate patching and instance removal from the ALB during the defined window\n\nSystems Manager Maintenance Windows can be configured to run Automation documents or Lambda functions at precise times. You can schedule a task to remove instances from the load balancer using pre-defined documents (such as AWSEC2-PatchLoadBalancerInstance) and then re-register them once patching is complete. This offers fine-grained scheduling and orchestration for controlled patching without disrupting traffic.\n\nIncorrect options:\n\nConfigure a custom Lambda function triggered by an Amazon EventBridge rule that disables the EC2 instance's network interface during the patching window and re-enables it after patching completes - This is a completely impractical and risky approach. Disabling the instance's network interface during patching would sever communication with AWS Systems Manager, which relies on the SSM Agent and internet/network connectivity to manage the patching workflow. This would likely cause patching failures or leave instances in an inconsistent state. It also introduces custom automation and complexity that is neither secure nor supported by AWS best practices.\n\nModify the load balancer configuration to attach EC2 instances using instance ID-based target groups instead of IP-based targets, allowing Systems Manager to directly communicate with instance metadata - Switching from IP-based to instance ID-based targets does not solve the problem related to application failures during patching. The issue lies in how traffic is handled during the patching processâ€”not in the communication between Systems Manager and the instances. Furthermore, IP-based target groups are commonly used when EC2 instances are managed through services like ECS or are deployed in different subnets or VPCs. Changing the target type could introduce compatibility or architectural issues without addressing the root cause.\n\nUse Amazon CloudWatch Logs Insights to monitor patching success and then manually adjust ALB target group registrations before and after each patch window - While monitoring with CloudWatch Logs Insights can provide visibility into patching status, it does not automate the removal or addition of instances to/from the ALB. Manually adjusting target group registrations is not scalable, prone to human error, and does not align with the goal of automation and compliance. This option lacks operational efficiency and doesn't prevent traffic from reaching instances while they are being patched.\n\nReferences:\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html\n\nhttps://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-awsec2-patch-load-balancer-instance.html\n\nhttps://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-runbook-reference.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q595",
    "questionText": "A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval.\n\nAs an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
    "options": [
      {
        "text": "Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications",
        "isCorrect": false
      },
      {
        "text": "Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream",
        "isCorrect": true
      },
      {
        "text": "Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose",
        "isCorrect": false
      },
      {
        "text": "Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nFeed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream\n\nYou can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs.\nAmazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams.\n\nHow Amazon Kinesis Data Streams Work:\n\nvia - https://aws.amazon.com/kinesis/data-streams/\n\nAmazon Kinesis Data Streams Key Concepts:\n\n\nvia - https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\n\nFor the given use case, you can stream the raw financial transactions into Amazon Kinesis Data Streams, which in turn, are processed by the AWS Lambda function that is set up as one of the consumers of the data stream. The Lambda would remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can be configured as the other consumers of the data stream and ingest the raw transactions\n\nIncorrect options:\n\nBatch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS Lambda function to remove sensitive data from the raw transactions in the flat file and then store the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with the internal applications- The use case requires a near-real-time solution for cleansing, processing and storing the transactions, so using a batch process would be incorrect.\n\nFeed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Firehose - Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services.\n\n\nvia - https://aws.amazon.com/kinesis/data-firehose/\n\nYou cannot set up multiple consumers for Amazon Kinesis Data Firehose delivery streams as it can dump data in a single data repository at a time, so this option is incorrect.\n\nPersist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon DynamoDB Streams to share the transactions data with the internal applications - There is no such rule within Amazon DynamoDB that can auto-update every time a new item is written in a DynamoDB table. You would need to use a Amazon DynamoDB trigger to invoke an external service like a Lambda function on every new write, which can then cleanse and update the item. In addition, this process introduces inefficiency in the workflow as the same item is written and then updated for cleansing purposes. Therefore this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/kinesis/data-streams/\n\nhttps://aws.amazon.com/kinesis/data-firehose/\n\nhttps://docs.aws.amazon.com/streams/latest/dev/key-concepts.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q596",
    "questionText": "A healthcare startup runs a lightweight reporting application on a single Amazon EC2 On-Demand instance. The application is designed to be stateless, fault-tolerant, and optimized for fast rendering of analytics dashboards. During major health events or news cycles, the team observes latency issues and occasional 5xx errors due to traffic spikes. To meet growing demand without over-provisioning resources during off-peak hours, the company wants to implement a cost-effective, scalable solution that ensures consistent performance even under unpredictable load.\n\nWhich approach best meets the requirements while minimizing costs?",
    "options": [
      {
        "text": "Clone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly",
        "isCorrect": false
      },
      {
        "text": "Configure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70%",
        "isCorrect": false
      },
      {
        "text": "Build an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances",
        "isCorrect": true
      },
      {
        "text": "Containerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nBuild an Amazon Machine Image (AMI) from the existing EC2 instance and configure a launch template. Create an Auto Scaling group using the launch template with Spot Instance pricing enabled. Attach an Application Load Balancer to distribute traffic across dynamically launched instances\n\nThis approach leverages EC2 Auto Scaling to automatically scale the number of instances based on demand while using Spot Instances to minimize compute costs. The applicationâ€™s stateless design is ideal for this setup. The Application Load Balancer distributes traffic across healthy instances. Using a launch template with an AMI ensures consistency across instances. This solution balances scalability with cost efficiency and aligns with AWS best practices for elastic, stateless workloads.\n\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/which-fleet-method-to-use.html\n\nIncorrect options:\n\nClone the EC2 instance using an AMI and launch a second On-Demand instance. Register both instances with an Application Load Balancer to distribute incoming traffic evenly - While this setup provides basic load distribution and supports horizontal scaling, using multiple On-Demand instances significantly increases operational costs, especially if the application only experiences brief traffic bursts. This design lacks elasticity and automationâ€”instances run continuously even during idle periods. It's functional but not cost-effective compared to using Auto Scaling and Spot Instances.\n\nConfigure an Amazon EventBridge rule to monitor system-level metrics from the EC2 instance. Trigger a Lambda function to re-deploy the application in a different Availability Zone when CPU utilization exceeds 70% - Although EventBridge can be used to respond to metrics, re-deploying the application to another Availability Zone in response to high CPU usage does not address scalability. This approach is reactive and inefficient, and it assumes a single-instance architecture. It also doesn't eliminate the source of the 5xx errors, which stem from overloadâ€”not instance location. Furthermore, there's no load balancing or auto-scaling involved.\n\nContainerize the application using Amazon ECS with Fargate launch type. Deploy the container to a single Fargate task and set a CloudWatch alarm to increase memory and CPU allocation dynamically based on load - While Amazon ECS with Fargate offers a serverless way to run containers without managing infrastructure, this approach is not inherently scalable when only one task is deployed. Even with increased CPU and memory allocation triggered by a CloudWatch alarm, a single Fargate task will still remain a bottleneck under heavy load. ECS services are designed to scale horizontally by adding more tasks, not by resizing a single task. Additionally, there is no load balancing mechanism included in this setup, and it doesnâ€™t address traffic distribution or redundancy. This configuration underutilizes the scalable nature of ECS and fails to deliver the needed elasticity.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-template-spot-instances.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/which-fleet-method-to-use.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launch-instances-from-launch-template.html#launch-templates-spot-fleet\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q597",
    "questionText": "A Customer relationship management (CRM) application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple Amazon EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution.\n\nAs a solutions architect, which of the following solutions would you recommend?",
    "options": [
      {
        "text": "Use Amazon RDS for distributed in-memory cache based session management",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Elasticache for distributed in-memory cache based session management",
        "isCorrect": true
      },
      {
        "text": "Use Application Load Balancer sticky sessions",
        "isCorrect": false
      },
      {
        "text": "Use Amazon DynamoDB for distributed in-memory cache based session management",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon Elasticache for distributed in-memory cache based session management\n\nAmazon ElastiCache can be used as a distributed in-memory cache for session management. Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Session stores can be set up using both Memcached or Redis for ElastiCache.\n\nAmazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\n\nAmazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Session stores are easy to create with Amazon ElastiCache for Memcached.\n\nHow Amazon ElastiCache Works:\n\nvia - https://aws.amazon.com/elasticache/\n\nIncorrect options:\n\nUse Amazon RDS for distributed in-memory cache based session management - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It cannot be used as a distributed in-memory cache for session management, hence this option is incorrect.\n\nUse Amazon DynamoDB for distributed in-memory cache based session management - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. Amazon DynamoDB is a NoSQL database and is not the right fit for a distributed in-memory cache-based session management solution.\n\nUse Application Load Balancer sticky sessions - Although sticky sessions enable each user to interact with one server and one server only, however, in case of an unhealthy server, all the session data is gone as well. Therefore Amazon Elasticache powered distributed in-memory cache-based session management is a better solution.\n\nReferences:\n\nhttps://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/\n\nhttps://aws.amazon.com/elasticache/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q598",
    "questionText": "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.\n\nAs a solutions architect, which of the following would you recommend for improving the performance for the given use-case?",
    "options": [
      {
        "text": "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues",
        "isCorrect": false
      },
      {
        "text": "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues",
        "isCorrect": false
      },
      {
        "text": "Use Enhanced Fanout feature of Amazon Kinesis Data Streams",
        "isCorrect": true
      },
      {
        "text": "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Enhanced Fanout feature of Amazon Kinesis Data Streams\n\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nBy default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.\n\nAmazon Kinesis Data Streams Fanout:\n\nvia - https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\n\nIncorrect options:\n\nSwap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Amazon Kinesis Data Firehose can only write to Amazon S3, Amazon Redshift, Amazon Elasticsearch or Splunk. You can't have applications consuming data streams from Amazon Kinesis Data Firehose, that's the job of Amazon Kinesis Data Streams. Therefore this option is not correct.\n\nSwap out Amazon Kinesis Data Streams with Amazon SQS Standard queues\n\nSwap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues\n\nAmazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both Amazon SQS Standard and Amazon SQS FIFO are not the right fit for the given use-case.\n\nExam Alert:\n\nPlease understand the differences between the capabilities of Amazon Kinesis Data Streams vs Amazon SQS, as you may be asked scenario-based questions on this topic in the exam.\n\n\nvia - https://aws.amazon.com/kinesis/data-streams/faqs/\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\n\nhttps://aws.amazon.com/kinesis/data-streams/faqs/",
    "awsService": "SQS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q599",
    "questionText": "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access.\n\nAs a solutions architect, which of the following solutions would you recommend?",
    "options": [
      {
        "text": "Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls",
        "isCorrect": true
      },
      {
        "text": "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls\n\nAmazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.\n\nAn Amazon S3 Glacier vault is a container for storing archives. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. Amazon S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual Amazon S3 Glacier vaults with a vault lock policy. You can specify controls such as â€œwrite once read manyâ€ (WORM) in a vault lock policy and lock the policy from future edits. Therefore, this is the correct option.\n\nIncorrect options:\n\nUse Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle policy to enforce compliance controls - You can use lifecycle policy to define actions you want Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to another storage class, archive them, or delete them after a specified period. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.\n\nUse Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls- Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.\n\nUse Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control List to enforce compliance controls - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html\n\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q600",
    "questionText": "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on Amazon EBS.\n\nWhich of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select three)",
    "options": [
      {
        "text": "Data at rest inside the volume is encrypted",
        "isCorrect": true
      },
      {
        "text": "Data moving between the volume and the instance is NOT encrypted",
        "isCorrect": false
      },
      {
        "text": "Any snapshot created from the volume is encrypted",
        "isCorrect": true
      },
      {
        "text": "Any snapshot created from the volume is NOT encrypted",
        "isCorrect": false
      },
      {
        "text": "Data moving between the volume and the instance is encrypted",
        "isCorrect": true
      },
      {
        "text": "Data at rest inside the volume is NOT encrypted",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nData at rest inside the volume is encrypted\n\nAny snapshot created from the volume is encrypted\n\nData moving between the volume and the instance is encrypted\n\nAmazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with Amazon EC2 instances. When you create an encrypted Amazon EBS volume and attach it to a supported instance type, data stored at rest on the volume, data moving between the volume and the instance, snapshots created from the volume and volumes created from those snapshots are all encrypted. It uses AWS Key Management Service (AWS KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. Encryption operations occur on the servers that host Amazon EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached Amazon EBS storage.\n\nTherefore, the incorrect options are:\n\nData moving between the volume and the instance is NOT encrypted\n\nAny snapshot created from the volume is NOT encrypted\n\nData at rest inside the volume is NOT encrypted\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q601",
    "questionText": "A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit.\n\nAs a solutions architect, which of the following solutions would you suggest to the company?",
    "options": [
      {
        "text": "Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "isCorrect": false
      },
      {
        "text": "Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "isCorrect": false
      },
      {
        "text": "Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "isCorrect": false
      },
      {
        "text": "Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud\n\nAWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your on-premises network and Amazon VPC over the Internet. IPsec is a protocol suite for securing IP communications by authenticating and encrypting each IP packet in a data stream.\n\nIncorrect options:\n\nUse AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not encrypt your traffic that is in transit. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service. As AWS Direct Connect does not support encrypted network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.\n\nUse AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud - AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and AWS. AWS DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network utilization. As AWS Data Sync cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.\n\nUse AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. As AWS Secrets Manager cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html",
    "awsService": "Secrets Manager",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q602",
    "questionText": "The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect - Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective.\n\nWhich of the following features will help you in disaster recovery of the database? (Select two)",
    "options": [
      {
        "text": "Use cross-Region Read Replicas",
        "isCorrect": true
      },
      {
        "text": "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions",
        "isCorrect": true
      },
      {
        "text": "Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage",
        "isCorrect": false
      },
      {
        "text": "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region",
        "isCorrect": false
      },
      {
        "text": "Use the database cloning feature of the Amazon RDS Database cluster",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nUse cross-Region Read Replicas\n\nIn addition to using Read Replicas to reduce the load on your source database instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue.\n\nEnable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions\n\nAmazon RDS provides high availability and failover support for database instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.\n\nThe automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will back up your database and transaction logs and store both for a user-specified retention period. If itâ€™s a Multi-AZ configuration, backups occur on standby to reduce the I/O impact on the primary. Amazon RDS supports Cross-Region Automated Backups. Manual snapshots and Read Replicas are also supported across multiple Regions.\n\nIncorrect options:\n\nEnable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region - This is an incorrect statement. Automated backups can be created across AWS Regions.\n\nUse Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option.\n\nUse the database cloning feature of the Amazon RDS Database cluster - This option has been added as a distractor. Database cloning is only available for Amazon Aurora and not for Amazon RDS.\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/\n\nhttps://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\n\nhttps://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q603",
    "questionText": "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3.\n\nAs a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?",
    "options": [
      {
        "text": "Use Amazon EC2 instances with Instance Store as the storage option",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon EC2 instances with Instance Store as the storage option\n\nAn instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.\n\nAs Instance Store delivers high random I/O performance, it can act as a temporary storage space, and these volumes are included as part of the instance's usage cost, therefore this is the correct option.\n\nAmazon EC2 Instance Store:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nIncorrect options:\n\nUse Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver its provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.\nAmazon EBS gp2 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct.\n\nUse Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option - Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.\nAmazon EBS io1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct.\n\nUse Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option - Throughput Optimized HDD (st1) are low-cost HDD volumes designed for frequently accessed, throughput-intensive workloads such as Big data and Data warehouses. Amazon EBS st1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the Amazon EC2 instance), therefore this option is not correct.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q604",
    "questionText": "An e-commerce company uses Amazon Simple Queue Service (Amazon SQS) queues to decouple their application architecture. The engineering team has observed message processing failures for some customer orders.\n\nAs a solutions architect, which of the following solutions would you recommend for handling such message failures?",
    "options": [
      {
        "text": "Use a temporary queue to handle message processing failures",
        "isCorrect": false
      },
      {
        "text": "Use a dead-letter queue to handle message processing failures",
        "isCorrect": true
      },
      {
        "text": "Use short polling to handle message processing failures",
        "isCorrect": false
      },
      {
        "text": "Use long polling to handle message processing failures",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse a dead-letter queue to handle message processing failures\n\nDead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.\nSometimes, messages canâ€™t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.\n\nHow do dead-letter queues work?:\n\nvia - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\n\nIncorrect options:\n\nUse a temporary queue to handle message processing failures - The most common use case for temporary queues is the request-response messaging pattern (for example, processing a login request), where a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing failures.\n\nUse short polling to handle message processing failures\n\nUse long polling to handle message processing failures\n\nAmazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\nNeither short polling nor long polling can be used to handle message processing failures.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
    "awsService": "SQS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q605",
    "questionText": "An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer with AWS Web Application Firewall (AWS WAF).\n\nAs a solutions architect, which of the following solutions can be combined to address the given use-case? (Select two)",
    "options": [
      {
        "text": "Create a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Use Application Load Balancer geo match statement listing the countries that you want to block",
        "isCorrect": false
      },
      {
        "text": "Use Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through",
        "isCorrect": false
      },
      {
        "text": "Use AWS WAF geo match statement listing the countries that you want to block",
        "isCorrect": true
      },
      {
        "text": "Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nUse AWS WAF geo match statement listing the countries that you want to block\n\nUse AWS WAF IP set statement that specifies the IP addresses that you want to allow through\n\nAWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define.\n\nYou can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on Amazon EC2, or Amazon API Gateway for your APIs.\n\nAWS WAF - How it Works?:\n\nvia - https://aws.amazon.com/waf/\n\nTo block specific countries, you can create a AWS WAF geo match statement listing the countries that you want to block, and to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. You can combine the two rules as shown below:\n\nIncorrect options:\n\nCreate a deny rule for the blocked countries in the network access control list (network ACL) associated with each of the Amazon EC2 instances - A network access control list (network ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. A network access control list (network ACL) does not have the capability to block traffic based on geographic match conditions.\n\nUse Application Load Balancer geo match statement listing the countries that you want to block\n\nUse Application Load Balancer IP set statement that specifies the IP addresses that you want to allow through\n\nAn Application Load Balancer operates at the request level (layer 7), routing traffic to targets â€“ Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nAn Application Load Balancer cannot block or allow traffic based on geographic match conditions or IP based conditions. Both these options have been added as distractors.\n\nReferences:\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\n\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q606",
    "questionText": "A digital publishing platform stores large volumes of media assets (such as images and documents) in an Amazon S3 bucket. These assets are accessed frequently during business hours by internal editors and content delivery tools. The company has strict encryption policies and currently uses AWS KMS to handle server-side encryption. The cloud operations team notices that AWS KMS request costs are increasing significantly due to the high frequency of object uploads and accesses. The team is now looking for a way to maintain the same encryption method but reduce the cost of KMS usage, especially for frequent access patterns.\n\nWhich solution meets the company's encryption and cost optimization goals?",
    "options": [
      {
        "text": "Configure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges",
        "isCorrect": false
      },
      {
        "text": "Use client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each objectâ€™s metadata for decryption",
        "isCorrect": false
      },
      {
        "text": "Switch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control",
        "isCorrect": false
      },
      {
        "text": "Enable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nEnable S3 Bucket Keys for server-side encryption with AWS KMS (SSE-KMS) so that new objects use a bucket-level key rather than requesting individual KMS data keys for every object\n\nAmazon S3 Bucket Keys reduce the cost of Amazon S3 server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS). Using a bucket-level key for SSE-KMS can reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS. Enabling S3 Bucket Keys with SSE-KMS allows S3 to generate a unique data key for each object locally using a bucket-level KMS key, which dramatically reduces the number of direct AWS KMS API calls. This approach maintains the same security and compliance properties as SSE-KMS, while reducing request-related costsâ€”especially beneficial for workloads with frequent access or write operations.\n\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html\n\nIncorrect options:\n\nConfigure a VPC endpoint for S3 and restrict access to the bucket to traffic originating from the endpoint to avoid additional KMS charges - Setting up a VPC endpoint for S3 helps with network cost optimization and security controls, but it has no effect on AWS KMS pricing. KMS requests are charged separately from network data transfer, and restricting access through a VPC endpoint does not eliminate or reduce encryption-related API calls. This option improves access control but does not meet the goal of reducing encryption cost.\n\nUse client-side encryption by generating a local symmetric key and uploading it to Amazon S3 along with each objectâ€™s metadata for decryption - Client-side encryption offloads encryption from AWS to the client, but this method adds significant management complexity, such as key rotation, secure key storage, and decryption logic on the client side. Uploading the key with the metadata is also a security risk and not compliant with best practices. Additionally, this approach does not reduce costs related to KMS unless KMS is removed entirely from the encryption pipeline, which may violate encryption policy requirements.\n\nSwitch to server-side encryption using Amazon S3 managed keys (SSE-S3) to eliminate all AWS KMS-related encryption charges while maintaining the same level of encryption control - While SSE-S3 does eliminate AWS KMS usage and its associated costs, it does not meet the requirement to maintain the companyâ€™s strict encryption policies that rely specifically on AWS KMS-managed keys. SSE-S3 uses S3-managed keys, which are not integrated with AWS KMS and lack the granular key management, audit logging, and fine-grained access control provided by KMS. Therefore, although this option reduces costs, it fails to comply with the companyâ€™s requirement to continue using KMS for encryption, making it an invalid solution in this scenario.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q607",
    "questionText": "A SaaS analytics company is deploying a microservices-based application on Amazon ECS using the Fargate launch type. The application requires access to a shared, POSIX-compliant file system that is available across multiple Availability Zones for redundancy and availability. To meet compliance requirements, the system must support regional backups and cross-Region data recovery with a recovery point objective (RPO) of no more than 8 hours. A backup strategy will be implemented using AWS Backup to automate replication across Regions. As the lead cloud architect, you are evaluating file storage solutions that align with these requirements.\n\nWhich option best meets the applicationâ€™s availability, durability, and RPO objectives?",
    "options": [
      {
        "text": "Deploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis",
        "isCorrect": true
      },
      {
        "text": "Configure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region",
        "isCorrect": false
      },
      {
        "text": "Use Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon Elastic File System (Amazon EFS) with the Standard storage class and configure AWS Backup to create cross-Region backups on a scheduled basis\n\nAmazon EFS is a fully managed, scalable, NFS-compatible file system ideal for containerized workloads. It provides multi-AZ mount targets by default and supports integration with AWS Backup for automated cross-Region backup and restore. When used with AWS Backup, EFS can achieve an RPO of 8 hours or less through scheduled backups. The Standard storage class offers high durability and low latency, making it a strong fit for active workloads that span Availability Zones.\n\nIncorrect options:\n\nDeploy Amazon FSx for Lustre and configure a backup plan using AWS Backup for cross-Region replication of the file system metadata -  Although Amazon FSx for Lustre supports integration with AWS Backup, it is optimized for high-performance compute (HPC) workloads, not general-purpose container applications. It lacks native cross-AZ mount targets and is not a good fit for persistent shared storage across AZs. In addition, data durability and recovery use cases are not ideal with Lustre unless linked to Amazon S3, which adds architectural complexity and delay.\n\nConfigure Amazon S3 with the S3 Standard storage class and mount it in containers using Mountpoint for Amazon S3. Use AWS Backup to replicate objects to another Region - While Amazon S3 is durable and supports cross-Region replication, it is an object storage service, not a file system. It lacks POSIX compliance even when used with Mountpoint for Amazon S3. So, this option is incorrect.\n\nUse Amazon FSx for NetApp ONTAP with a Multi-AZ deployment and rely on its native high availability and AWS Backup integration to replicate the file system to another Region automatically - While Amazon FSx for NetApp ONTAP does support Multi-AZ deployments for high availability within a Region and integrates with AWS Backup for local backups, cross-Region replication is not managed automatically by AWS Backup for FSx for ONTAP. Instead, it requires manual configuration using NetApp SnapMirror, which introduces additional operational overhead and does not provide a managed RPO guarantee like EFS does when used with AWS Backup. Therefore, although the storage service is resilient and enterprise-grade, this setup does not fully meet the requirement for an RPO of 8 hours using AWS Backup for seamless cross-Region recovery.\n\nReferences:\n\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/cross-region-backup.html\n\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/backup-feature-availability.html#features-by-resource\n\nhttps://aws.amazon.com/blogs/aws/mountpoint-for-amazon-s3-generally-available-and-ready-for-production-workloads/\n\nhttps://www.amazonaws.cn/en/fsx/lustre/faqs/#Availability_and_durability\n\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q608",
    "questionText": "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications.\n\nAs a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?",
    "options": [
      {
        "text": "Use AWS DataSync to migrate existing data to Amazon  S3 as well as access the Amazon S3 data for ongoing updates",
        "isCorrect": false
      },
      {
        "text": "Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications",
        "isCorrect": false
      },
      {
        "text": "Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications\n\nAWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect.\n\nAWS DataSync fully automates and accelerates moving large active datasets to AWS, up to 10 times faster than command-line tools. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage services, as well as detailed monitoring of the transfer.\nDataSync uses a purpose-built network protocol and scale-out architecture to transfer data. A single AWS DataSync agent is capable of saturating a 10 Gbps network link.\n\nAWS DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the AWS DataSync API and Console, and Amazon CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. AWS DataSync performs data integrity verification both during the transfer and at the end of the transfer.\n\nHow AWS DataSync Works:\n\nvia - https://aws.amazon.com/datasync/\n\nAWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways â€“ Tape Gateway, File Gateway, and Volume Gateway â€“ that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching.\n\nThe combination of AWS DataSync and File Gateway is the correct solution. AWS DataSync enables you to automate and accelerate online data transfers to AWS storage services. File Gateway then provides your on-premises applications with low latency access to the migrated data.\n\nIncorrect options:\n\nUse AWS DataSync to migrate existing data to Amazon  S3 as well as access the Amazon S3 data for ongoing updates - AWS DataSync is used to easily transfer data to and from AWS with up to 10x faster speeds. It is used to transfer data and cannot be used to facilitate ongoing updates to the migrated files from the on-premises applications.\n\nUse File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications - File Gateway can be used to move on-premises data to AWS Cloud, but it not an optimal solution for high volumes. Migration services such as AWS DataSync are best suited for this purpose. Amazon S3 Transfer Acceleration cannot facilitate ongoing updates to the migrated files from the on-premises applications.\n\nUse Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then use AWS DataSync for ongoing updates from the on-premises applications -  If your application is already integrated with the Amazon S3 API, and you want higher throughput for transferring large files to Amazon S3, Amazon S3 Transfer Acceleration can be used. However AWS DataSync cannot be used to facilitate ongoing updates to the migrated files from the on-premises applications.\n\nReference:\n\nhttps://aws.amazon.com/datasync/features/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q609",
    "questionText": "A medical devices company uses Amazon S3 buckets to store critical data. Hundreds of buckets are used to keep the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on the Amazon S3 buckets have not been applied optimally, resulting in higher costs.\n\nAs a Solutions Architect, can you recommend a solution to reduce storage costs on Amazon S3 while keeping the IT team's involvement to a minimum?",
    "options": [
      {
        "text": "Configure Amazon EFS to provide a fast, cost-effective and sharable storage service",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs",
        "isCorrect": true
      },
      {
        "text": "Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs\n\nThe Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.\n\nFor a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in Amazon S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. There are no retrieval fees when using the Amazon S3 Intelligent-Tiering storage class, and no additional tiering fees when objects are moved between access tiers. It is the ideal storage class for long-lived data with access patterns that are unknown or unpredictable.\n\nAmazon S3 Storage Classes can be configured at the object level and a single bucket can contain objects stored in Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One Zone-IA. You can upload objects directly to Amazon S3 Intelligent-Tiering, or use S3 Lifecycle policies to transfer objects from Amazon S3 Standard and Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering. You can also archive objects from Amazon S3 Intelligent-Tiering to Amazon S3 Glacier.\n\nIncorrect options:\n\nConfigure Amazon EFS to provide a fast, cost-effective and sharable storage service - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS offers sharable service, unlike Amazon Elastic Block Storage (EBS) that cannot be shared by instances. Amazon EFS is costlier than storing data in Amazon S3. Also, Amazon EFS needs an Amazon EC2 instance or an AWS Direct Connect network connection. Hence, this is not the correct option.\n\nUse Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage - Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. Not a right option, since data stored is business-critical and cannot be risked by using Amazon S3 One Zone-IA.\n\nUse Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data on-premises - This is a distractor as Amazon S3 on Outposts (S3 Outposts) delivers object storage to your on-premises AWS Outposts environment. It is used in conjunction with AWS Outposts and has no relevance to the current use case.\n\nReference:\n\nhttps://aws.amazon.com/s3/storage-classes/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q610",
    "questionText": "A financial auditing firm uses Amazon S3 to store sensitive client records that are subject to write-once-read-many (WORM) regulations to prevent alteration or deletion of records for a specific retention period. The firm wants to enforce immutable storage, such that even administrators cannot overwrite or delete the records during the lock duration. They also need audit-friendly enforcement to prevent accidental or malicious deletion.\n\nWhich configuration of S3 Object Lock will ensure that the retention policy is strictly enforced, and no user (including root or administrators) can override or delete protected objects during the lock period?",
    "options": [
      {
        "text": "Use S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings",
        "isCorrect": false
      },
      {
        "text": "Enable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period",
        "isCorrect": false
      },
      {
        "text": "Use S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period",
        "isCorrect": false
      },
      {
        "text": "Use S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse S3 Object Lock in Compliance Mode, which enforces retention policies strictly and prevents all users from modifying or deleting data during the retention period\n\nS3 Object Lock in Compliance Mode is designed to meet stringent regulatory requirements. In this mode, no user, not even the root user, can override or delete a locked object until the retention period expires. This ensures strict WORM behavior, making Compliance Mode ideal for regulated industries where immutability must be enforced regardless of user permissions. Attempting to modify or delete objects locked in compliance mode results in an AccessDenied error, even for administrators.\n\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html#object-lock-retention-modes\n\nIncorrect options:\n\nUse S3 Object Lock in Governance Mode, which allows only IAM users with elevated permissions to override or remove retention settings - Governance Mode also provides WORM-like protections, but unlike Compliance Mode, it allows privileged users (with the s3:BypassGovernanceRetention permission) to override retention settings or delete objects before the retention period expires. This is useful in environments where operational flexibility is needed, but does not meet regulatory immutability requirements because deletions are still technically possible. Therefore, Governance Mode is not sufficient for legally enforced retention.\n\nEnable S3 Versioning and set a bucket policy that denies s3:DeleteObject to all users during the retention period - While enabling versioning and denying s3:DeleteObject through a bucket policy can prevent casual deletion, this approach is not tamper-proof and does not meet regulatory compliance for WORM storage. A user with sufficient IAM permissions (e.g., policy modification rights) can update or remove the bucket policy, making this control circumventable. Object Lock in Compliance Mode provides enforced immutability, which cannot be achieved using only versioning and bucket policies.\n\nUse S3 Lifecycle Policies to transition data to Glacier Deep Archive and treat it as immutable during the archival period - S3 Lifecycle Policies are used to transition objects between storage classes or delete them after a specified time. Transitioning data to Glacier Deep Archive does not make the objects immutable. Users can still delete archived objects unless Object Lock is also enabled. Lifecycle configurations are not designed for WORM enforcement and do not prevent deletion or modification during archival.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html#object-lock-retention-modes",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q611",
    "questionText": "The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required.\n\nWhich solution is the MOST cost-effective for the given use case?",
    "options": [
      {
        "text": "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nSet up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation\n\nAmazon S3 Standard-IA class is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per gigabyte storage price and per GB retrieval charge.\n\n\nvia - https://aws.amazon.com/s3/storage-classes/\n\nFor the given use case, you can set up an Amazon S3 lifecycle configuration and create a transition action to move objects from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. You can set up an expiration action to delete the object 5 years after object creation.\n\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\n\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\n\nIncorrect options:\n\nSet up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation - Amazon S3 Glacier Flexible Retrieval storage class has the best case retrieval time of the order of minutes, so this option is incorrect for the given requirement.\n\nSet up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation - The files can simply be deleted 5 years after object creation instead of archiving the files to Amazon S3 Glacier Deep Archive. There is no need to incur the cost of archival.\n\nSet up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation - Unlike other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3 One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA is a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. The given scenario clearly states that the business-critical data is not easy to reproduce, so this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/s3/storage-classes/\n\nhttps://aws.amazon.com/s3/storage-classes/glacier/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q612",
    "questionText": "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data.\n\nAs a Solutions Architect, which set of services will you recommend to meet these requirements?",
    "options": [
      {
        "text": "Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage",
        "isCorrect": false
      },
      {
        "text": "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
        "isCorrect": true
      },
      {
        "text": "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
        "isCorrect": false
      },
      {
        "text": "Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage\n\nAn instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.\n\nYou can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance.\n\nSome instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.\n\nAmazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, Amazon S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\n\nAmazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, Amazon S3 Glacier provides three retrieval options that range from a few minutes to hours. You can upload objects directly to Amazon S3 Glacier, or use S3 Lifecycle policies to transfer data between any of the Amazon S3 Storage Classes for active data (S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA) and S3 Glacier.\n\nIncorrect options:\n\nAmazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage - Amazon EC2 instance store volumes provide the best I/O performance for low latency requirement, as in the current use case. The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.\n\nAmazon S3 Glacier Deep Archive is Amazon S3â€™s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice a year. It is designed for customers â€” particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors â€” that retain data sets for 7-10 years or longer to meet regulatory compliance requirements.\n\nAmazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage - Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. Amazon EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. For high I/O performance, instance store volumes are a better option.\n\nAmazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. AWS Storage Gateway will be the right answer if the customer wanted to retain the on-premises data storage and just move the applications to AWS Cloud. In the absence of such requirements, instance store is a better option for high performance and Amazon S3 for durable storage.\n\nReference:\n\nhttps://aws.amazon.com/s3/storage-classes/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q613",
    "questionText": "You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting a ProvisionedThroughputExceededException exception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate.\n\nWhich of the following options will help with the exception while keeping costs at a minimum?",
    "options": [
      {
        "text": "Use batch messages",
        "isCorrect": true
      },
      {
        "text": "Decrease the Stream retention duration",
        "isCorrect": false
      },
      {
        "text": "Increase the number of shards",
        "isCorrect": false
      },
      {
        "text": "Use Exponential Backoff",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse batch messages\n\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\n\nAmazon Kinesis Data Streams Overview:\n\nvia - https://aws.amazon.com/kinesis/data-streams/\n\nWhen a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards.\n\nIncorrect options:\n\nUse Exponential Backoff - While this may help in the short term, as soon as the request rate increases, you will see the ProvisionedThroughputExceededException exception again.\n\nIncrease the number of shards - Increasing shards could be a short term fix but will substantially increase the cost, so this option is ruled out.\n\nDecrease the Stream retention duration - This operation may result in data loss and won't help with the exceptions, so this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/\n\nhttps://aws.amazon.com/kinesis/data-streams/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q614",
    "questionText": "A digital design company has migrated its project archiving platform to AWS. The application runs on Amazon EC2 Linux instances in an Auto Scaling group that spans multiple Availability Zones. Designers upload and retrieve high-resolution image files from a shared file system, which is currently configured to use Amazon EFS Standard-IA. Metadata for these files is stored and indexed in an Amazon RDS for PostgreSQL database. The company's cloud engineering team has been asked to optimize storage costs for the image archive without compromising reliability. They are open to refactoring the application to use managed AWS services when necessary.\n\nWhich solution offers the most cost-effective architecture?",
    "options": [
      {
        "text": "Replace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost",
        "isCorrect": false
      },
      {
        "text": "Create an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API",
        "isCorrect": true
      },
      {
        "text": "Use AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export",
        "isCorrect": false
      },
      {
        "text": "Replace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an Amazon S3 bucket with Intelligent-Tiering enabled. Update the application to store and retrieve project files using the Amazon S3 API\n\nAmazon S3 Intelligent-Tiering is an ideal cost-optimization choice for datasets with unpredictable access patterns. It automatically moves objects between frequent and infrequent access tiers, optimizing cost without compromising latency or availability. Compared to EFS Standard-IA, S3 Intelligent-Tiering offers lower cost per GB for archival-like workloads and doesnâ€™t require manual monitoring or tuning. Refactoring the application to use the S3 API introduces some development overhead but enables the most cost-effective, durable, and scalable storage.\n\nIncorrect options:\n\nReplace the EFS file system with Amazon FSx for Lustre. Mount the file system to EC2 instances and store project files there to reduce access latency and cost - Amazon FSx for Lustre is designed for high-performance compute (HPC) use cases such as machine learning, simulations, or big data analyticsâ€”not general-purpose file storage or archival. While it offers low-latency access to frequently accessed files, it does not offer automated lifecycle management, nor is it optimized for cost reduction in infrequently accessed workloads. It also lacks direct integration with database indexing use cases unless paired with Amazon S3.\n\nUse AWS Backup to export all EFS files daily to an Amazon S3 bucket. Retain the EFS file system in Standard-IA class for occasional real-time access and route all archival queries to the S3 export - While AWS Backup can export EFS file system backups to S3, this approach introduces duplication and complexity rather than eliminating cost. The company would now pay for EFS storage, S3 storage, and backup operations, which could increase rather than reduce overall storage costs. Additionally, this setup would require the application to differentiate between S3 and EFS reads, complicating data retrieval paths.\n\nReplace the EFS file system with Amazon FSx for NetApp ONTAP. Use volume tiering to move cold data to lower-cost capacity pool storage. Update the application to use the ONTAP mount path - Amazon FSx for NetApp ONTAP supports volume tiering to cost-efficient storage pools, which may sound appealing for cold data. However, FSx for ONTAP is more suited for enterprise NAS use cases, often tied to lift-and-shift migrations of existing NetApp systems. Its pricing model and operational complexity are less cost-effective for infrequent file access when compared to S3 Intelligent-Tiering. It also introduces the need to manage storage efficiency features (e.g., snapshots, quotas) that may be unnecessary in this context.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html\n\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html\n\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html\n\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q615",
    "questionText": "An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancing (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the company significant time and resulted in major revenue loss.\n\nWhat should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?",
    "options": [
      {
        "text": "Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed",
        "isCorrect": false
      },
      {
        "text": "Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user",
        "isCorrect": false
      },
      {
        "text": "Set up an Amazon Route 53 geoproximity routing policy to route traffic",
        "isCorrect": false
      },
      {
        "text": "Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nSet up AWS Global Accelerator and add endpoints to cater to users in different geographic locations\n\nAs your application architecture grows, so does the complexity, with longer user-facing IP lists and more nuanced traffic routing logic. AWS Global Accelerator solves this by providing you with two static IPs that are anycast from our globally distributed edge locations, giving you a single entry point to your application, regardless of how many AWS Regions itâ€™s deployed in. This allows you to add or remove origins, Availability Zones or Regions without reducing your application availability. Your traffic routing is managed manually, or in console with endpoint traffic dials and weights. If your application endpoint has a failure or availability issue, AWS Global Accelerator will automatically redirect your new connections to a healthy endpoint within seconds.\n\nBy using AWS Global Accelerator, you can:\n\n\nAssociate the static IP addresses provided by AWS Global Accelerator to regional AWS resources or endpoints, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IP addresses. The IP addresses are anycast from AWS edge locations so they provide onboarding to the AWS global network close to your users.\nEasily move endpoints between Availability Zones or AWS Regions without needing to update your DNS configuration or change client-facing applications.\nDial traffic up or down for a specific AWS Region by configuring a traffic dial percentage for your endpoint groups. This is especially useful for testing performance and releasing updates.\nControl the proportion of traffic directed to each endpoint within an endpoint group by assigning weights across the endpoints.\n\n\nAWS Global Accelerator for Multi-Region applications:\n\nvia - https://aws.amazon.com/global-accelerator/\n\nIncorrect options:\n\nSet up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed - AWS Direct Connect can reduce latency to great extent. Direct Connect is used to connect on-premises systems to AWS Cloud for extremely low latency use cases. It cannot be used to serve users directly.\n\nCreate Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the nearest edge location to the user - If most of the content is static, we can configure Amazon CloudFront to improve performance. In the current scenario, the architecture has ELBs, Amazon EC2 instances too that need to be covered in the automatic failover plan.\n\nSet up an Amazon Route 53 geoproximity routing policy to route traffic - Geoproximity routing lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. Unlike AWS Global Accelerator, managing and routing to different instances, ELBs and other AWS resources will become an operational overhead as the resource count reaches into the hundreds. With inbuilt features like Static anycast IP addresses, fault tolerance using network zones, Global performance-based routing, TCP Termination at the Edge - AWS Global Accelerator is the right choice for multi-region, low latency use cases.\n\nReferences:\n\nhttps://aws.amazon.com/global-accelerator/features/\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q616",
    "questionText": "A company's cloud architect has set up a solution that uses Amazon Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website.\n\nWhich configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
    "options": [
      {
        "text": "Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket",
        "isCorrect": true
      },
      {
        "text": "Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket\n\nUse an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Amazon Route 53 includes only healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.\n\nIncorrect options:\n\nSet up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket - This option has been added as a distractor as there is no such thing as an active-active failover routing policy in Amazon Route 53. You can configure active-active failover using any routing policy (or combination of routing policies) other than failover routing policy and you configure active-passive failover only using the failover routing policy. In active-active failover configuration, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Amazon Route 53 considers them unhealthy. Amazon Route 53 can respond to a DNS query using any healthy record.\n\nUse Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency - this is Latency-based routing and is not helpful for the current use case.\n\nUse Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. This is not useful for the current use case.\n\nReferences:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q617",
    "questionText": "An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application Firewall (AWS WAF) to handle this requirement.\n\nCan you identify the correct solution leveraging the capabilities of AWS WAF?",
    "options": [
      {
        "text": "Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures",
        "isCorrect": true
      },
      {
        "text": "Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too",
        "isCorrect": false
      },
      {
        "text": "AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data",
        "isCorrect": false
      },
      {
        "text": "AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures\n\nWhen you use AWS WAF with Amazon CloudFront, you can protect your applications running on any HTTP webserver, whether it's a webserver that's running in Amazon Elastic Compute Cloud (Amazon EC2) or a web server that you manage privately. You can also configure Amazon CloudFront to require HTTPS between CloudFront and your own webserver, as well as between viewers and Amazon CloudFront.\n\nAWS WAF is tightly integrated with Amazon CloudFront and the Application Load Balancer (ALB), services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesnâ€™t come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on Application Load Balancer, your rules run in the region and can be used to protect internet-facing as well as internal load balancers.\n\nIncorrect options:\n\nConfigure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2 instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF cannot be directly configured on ALB. This configuration not only provides necessary safety but is scalable too - This statement is wrong. You can configure AWS WAF on Application Load Balancers (ALB).\n\nAWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data - AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), and Amazon API Gateway. It cannot be configured directly on an Amazon EC2 instance.\n\nAWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway. One of these two services can then be configured with Amazon EC2 to build the needed secure architecture - This statement is only partially correct. AWS WAF can also be deployed on Amazon CloudFront service.\n\nReferences:\n\nhttps://aws.amazon.com/waf/faqs/\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q618",
    "questionText": "A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries.\n\nAs a Solutions Architect, which of the following solutions would you recommend?",
    "options": [
      {
        "text": "Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments",
        "isCorrect": true
      },
      {
        "text": "Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments\n\nAmazon Transcribe is an automatic speech recognition (ASR) service that makes it easy to convert audio to text. One key feature of the service is called speaker identification, which you can use to label each individual speaker when transcribing multi-speaker audio files. You can specify Amazon Transcribe to identify 2â€“10 speakers in the audio clip.\n\nAmazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. To leverage Athena, you can simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds.\n\nAnalyzing multi-speaker audio files using Amazon Transcribe and Amazon Athena:\n\nvia - https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena\n\nIncorrect options:\n\nUse Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis - Amazon Kinesis can be used to stream real-time data for further analysis and storage. Kinesis Data Streams cannot read audio files. You will still need to use AWS Transcribe for ASR services.\n\nUse Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output -  Amazon Kinesis Data Streams cannot read audio files. Amazon Alexa cannot be used as an Automatic Speech Recognition (ASR) service, though Alexa internally uses ASR for its working.\n\nUse Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes - Amazon Quicksight is used for the visual representation of data through dashboards. However, it is not an SQL query based analysis tool like Amazon Athena. So, this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena\n\nhttps://aws.amazon.com/athena",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q619",
    "questionText": "A global e-commerce platform currently operates its order processing system in a single on-premises data center located in Europe. As the company grows its customer base across Asia and North America, it plans to deploy the application across multiple AWS Regions to improve availability and reduce latency. The company requires that updates to the central order database be completed in under one second with global consistency. The application layer will be deployed separately in each Region, but the order management data must remain centrally managed and globally synchronized.\n\nWhich solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "text": "Migrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access",
        "isCorrect": true
      },
      {
        "text": "Use Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS.",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nMigrate the order data to Amazon DynamoDB and create a global table. Deploy the application in each Region and connect to the local DynamoDB replica for low-latency access\n\nAmazon DynamoDB global tables are a fully managed, multi-Region, multi-writer database solution designed to deliver low-latency performance and high availability for globally distributed applications. In this architecture, a DynamoDB table is created in one Region and replicated automatically to other specified AWS Regions. Each replica behaves as a fully functional table, capable of handling both reads and writes locally. This setup allows regional deployments of the application to interact with the local replica of the database, minimizing cross-Region latency and avoiding the need to route write requests to a single centralized Region. Updates made to any Regionâ€™s table are asynchronously propagated to all other Regions with typical replication latency under 1 second, satisfying the application's performance requirement.\n\nThis solution also eliminates the need to manage complex replication logic, ensures high resilience against Regional failures, and provides seamless multi-active availabilityâ€”allowing each Region to continue operating independently even if another Region experiences disruption.\n\nIncorrect options:\n\nUse Amazon RDS for MySQL with a cross-Region read replica. Route all writes to the primary Region and use read replicas for local access in other Regions - Using RDS MySQL with read replicas in different Regions allows users to perform read queries from geographically closer endpoints. However, all write operations must be handled by the single primary instance located in the original Region, which violates the latency requirement for updates. Furthermore, replication across Regions is asynchronous, which introduces delay and potential data consistency issues. This approach does not fulfill the global consistency and low-latency write requirement.\n\nUse Amazon Neptune to store tracking updates as graph data. Deploy clusters in each Region and replicate changes using custom-built Lambda functions and Amazon SQS. - Amazon Neptune is a managed graph database, suitable for highly connected datasets such as social graphs or fraud detection. It does not support multi-Region deployment out of the box, and using custom replication via Lambda and SQS introduces complexity, increased latency, and potential consistency issues. This solution would not meet the companyâ€™s needs for a low-latency, replicated write path across Regions.\n\nUse Amazon Aurora database with MySQL engine, and configure read-only nodes in other Regions to handle local traffic while routing all write operations to the central Region - This architecture enables low-latency reads from secondary Regions by using Aurora Global Database read-only replicas. However, it restricts write operations to the primary Region only, meaning any update to the order database must still be processed in the central Region. This introduces latency for global users and fails to meet the requirement for sub-second update performance from all Regions. Additionally, the system doesn't support multi-Region active-active writes or built-in conflict resolution.\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.XRgn.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q620",
    "questionText": "A financial data processing company runs a workload on Amazon EC2 instances that fetch and process real-time transaction batches from an Amazon SQS queue. The application needs to scale based on unpredictable message volume, which fluctuates significantly throughout the day. The system must process messages with minimal delay and no downtime, even during peak spikes. The company is seeking a solution that balances cost-efficiency with availability and elasticity.\n\nWhich EC2 purchasing strategy best meets these requirements in the most cost-effective manner?",
    "options": [
      {
        "text": "Purchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations",
        "isCorrect": false
      },
      {
        "text": "Use EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs",
        "isCorrect": false
      },
      {
        "text": "Use Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume",
        "isCorrect": true
      },
      {
        "text": "Use EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Reserved Instances for the baseline level of traffic and configure EC2 Auto Scaling with Spot Instances to handle spikes in message volume\n\nThis blended approach is ideal for workloads that are partially predictable and partially variable. Reserved Instances ensure cost-efficient baseline capacity for steady message ingestion, while Spot Instances are used to handle unpredictable surges in traffic, significantly reducing costs compared to On-Demand. If Spot capacity is unavailable, Auto Scaling can be configured with fallback instance types or prioritized mixed instance policies. This model maximizes cost savings without compromising on availability.\n\nIncorrect options:\n\nPurchase EC2 Reserved Instances to match peak capacity and assign all message processing tasks to these instances regardless of load variations - While Reserved Instances (RIs) offer significant cost savings compared to On-Demand pricing, they are best suited for predictable, steady-state workloads. Using RIs to provision maximum capacity at all times, regardless of actual load, results in over-provisioning and wasted cost during low-traffic periods. This approach lacks elasticity and is not suitable for workloads with unpredictable and bursty traffic patterns.\n\nUse EC2 Spot Instances exclusively with Auto Scaling enabled to match message volume fluctuations and save on compute costs - Spot Instances provide deep cost savings, but they can be interrupted by AWS with only two minutes of warning if capacity is reclaimed. Using them exclusively for a mission-critical workload with real-time, parallel processing introduces risk of downtime and message loss, especially if traffic spikes coincide with Spot interruptions. For fault-tolerant or batch jobs, this could workâ€”but not for always-on processing with strict uptime requirements.\n\nUse EC2 Reserved Instances for the baseline workload and configure EC2 Auto Scaling to launch On-Demand Instances for all traffic spikes - While combining Reserved Instances for baseline capacity with On-Demand Instances for additional traffic is a functional and reliable solution, it is not the most cost-effective choice for a workload with unpredictable, intermittent spikes like real-time message processing. On-Demand Instances are priced at a premium and can lead to significantly higher costs when scaling frequently. A blended model using Reserved + Spot Instances would better balance availability and cost.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-groups.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\n\nhttps://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q621",
    "questionText": "A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic increases.\n\nAs a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?",
    "options": [
      {
        "text": "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6",
        "isCorrect": true
      },
      {
        "text": "The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6",
        "isCorrect": false
      },
      {
        "text": "The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone",
        "isCorrect": false
      },
      {
        "text": "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nThe Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6\n\nYou configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.\n\nAmazon EC2 Auto Scaling enables you to take advantage of the safety and reliability of geographic redundancy by spanning Auto Scaling groups across multiple Availability Zones within a Region. When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones. Since the application is extremely critical and needs to have a reliable architecture to support it, the Amazon EC2 instances should be maintained in at least two Availability Zones (AZs) for uninterrupted service.\n\nAmazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. This is why the minimum capacity should be 4 instances and not 2. Auto Scaling group will launch 2 instances each in both the AZs and this redundancy is needed to keep the service available always.\n\nIncorrect options:\n\nThe Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to 6\n\nThe Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone\n\nThe explanation above gives the correct rationale for minimum capacity as well as the instance distribution across AZs, so both these options are incorrect.\n\nThe Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6 - An Auto Scaling group can contain Amazon EC2 instances in one or more Availability Zones within the same region. However, Auto Scaling groups cannot span multiple Regions.\n\nReference:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q622",
    "questionText": "A biomedical research firm operates a file exchange system for external research partners to upload and download experimental data. Currently, the system runs on two Amazon EC2 Linux instances, each configured with Elastic IP addresses to allow access from trusted IPs. File transfers use the SFTP protocol, and Linux user accounts are manually provisioned to enforce file-level access control. Data is stored on a shared file system mounted to both EC2 instances. The firm wants to modernize the solution to a fully managed, serverless model with high IOPS, fine-grained user permission control, and strict IP-based access restrictions. They also want to reduce operational overhead without sacrificing performance or security.\n\nWhich solution best meets these requirements?",
    "options": [
      {
        "text": "Use Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies",
        "isCorrect": true
      },
      {
        "text": "Use Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses",
        "isCorrect": false
      },
      {
        "text": "Use AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF",
        "isCorrect": false
      },
      {
        "text": "Use Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon EFS with encryption enabled. Create an AWS Transfer Family SFTP endpoint in a VPC with Elastic IP addresses. Restrict access using a security group that allows traffic only from known IPs. Manage user access using POSIX identity mappings and IAM policies\n\nThis solution uses Amazon EFS, which is natively supported by AWS Transfer Family and provides shared, low-latency, and scalable file storage, ideal for high IOPS use cases. Creating the SFTP endpoint within a VPC with Elastic IP addresses allows secure, internet-facing access. Using IAM policies and POSIX user ID mappings, administrators can enforce per-user access control, similar to Linux user permissions. The security group restricts SFTP access to trusted IP addresses. This configuration satisfies all the companyâ€™s performance, security, and manageability goals in a serverless architecture.\n\nIncorrect options:\n\nUse Amazon FSx for Lustre as the backend storage. Create an AWS Transfer Family SFTP service with a public endpoint. Configure IAM policies to manage user access and attach a security group that restricts access to trusted IP addresses - While Amazon FSx for Lustre offers high-performance, POSIX-compliant storage, it is not natively integrated with AWS Transfer Family as a backend for SFTP services. Transfer Family only supports Amazon S3 and Amazon EFS as data stores. This option would require complex, unsupported workarounds to integrate FSx, making it operationally risky and unsupported.\n\nUse AWS Storage Gateway in file gateway mode to expose an NFS file share. Deploy AWS Transfer Family with a public endpoint and map user identities using IAM roles. Configure IP allow lists using AWS WAF - AWS Storage Gateway (file gateway mode) allows access to S3 via NFS or SMB but is not natively integrated with AWS Transfer Family. Although it supports on-premises caching and hybrid workloads, using it in this context introduces unnecessary complexity and cannot meet the requirement for native SFTP integration with a serverless backend. AWS WAF is also not applicable for SFTP traffic, as it operates on Layer 7 HTTP(S) protocols.\n\nUse Amazon S3 with server-side encryption enabled. Create an AWS Transfer Family SFTP endpoint with a VPC endpoint in a private subnet. Restrict access to known IPs using security group rules. Manage user-level permissions using IAM role-based access mappings - While Amazon S3 is a supported backend for AWS Transfer Family and offers excellent scalability and durability, it does not offer file system semantics or the high IOPS required by workloads involving frequent reads/writes like SFTP-based workflows. S3 performance is sufficient for bulk transfer or archiving, but EFS is a better choice for interactive file exchange with low latency and high throughput. This solution satisfies access control requirements but falls short on performance.\n\nReferences:\n\nhttps://docs.aws.amazon.com/transfer/latest/userguide/what-is-aws-transfer-family.html\n\nhttps://docs.aws.amazon.com/filegateway/latest/files3/what-is-file-s3.html\n\nhttps://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q623",
    "questionText": "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway.\n\nWhich conditions should be met for internet connectivity to be established? (Select two)",
    "options": [
      {
        "text": "The instance's subnet is not associated with any route table",
        "isCorrect": false
      },
      {
        "text": "The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic",
        "isCorrect": true
      },
      {
        "text": "The instance's subnet is associated with multiple route tables with conflicting configurations",
        "isCorrect": false
      },
      {
        "text": "The route table in the instanceâ€™s subnet should have a route to an Internet Gateway",
        "isCorrect": true
      },
      {
        "text": "The subnet has been configured to be public and has no access to the internet",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nThe network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic\n\nThe network access control list (network ACL) that is associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity.\n\nThe route table in the instanceâ€™s subnet should have a route to an Internet Gateway\n\nA route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instanceâ€™s subnet should have a route defined to the Internet Gateway.\n\nIncorrect options:\n\nThe instance's subnet is not associated with any route table - This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table.\n\nThe instance's subnet is associated with multiple route tables with conflicting configurations - This is an incorrect statement. A subnet can only be associated with one route table at a time.\n\nThe subnet has been configured to be public and has no access to the internet - This is an incorrect statement. Public subnets have access to the internet via Internet Gateway.\n\nReference:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q624",
    "questionText": "A financial analytics firm runs performance-intensive modeling software on Amazon EC2 instances backed by Amazon EBS volumes. The production data resides on EBS volumes attached to EC2 instances in the same AWS Region where the testing environment is hosted. To maintain data integrity, any changes made during testing must not affect production data. The development team needs to frequently create clones of this production data for simulations. The modeling software requires high and consistent I/O performance, and the firm wants to minimize the time required to provision test data.\n\nWhich solution should a solutions architect recommend to meet these requirements?",
    "options": [
      {
        "text": "Create Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data",
        "isCorrect": false
      },
      {
        "text": "Create new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations",
        "isCorrect": false
      },
      {
        "text": "Take snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment",
        "isCorrect": true
      },
      {
        "text": "Use Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nTake snapshots of the production EBS volumes. Enable EBS fast snapshot restore on the snapshots. Create new EBS volumes from the snapshots and attach them to EC2 instances in the test environment\n\nThis solution uses EBS fast snapshot restore (FSR), which allows immediate, full-performance access to new EBS volumes created from snapshotsâ€”eliminating the usual initialization delay. Creating volumes from snapshots ensures that test data is logically cloned without impacting the source, and the FSR feature guarantees the I/O performance required by the analytics workloads. This method is efficient, scalable, and preserves isolation between test and production environments.\n\nIncorrect options:\n\nCreate Amazon EBS-backed Amazon Machine Images (AMIs) from the production EC2 instances. Launch new EC2 instances in the test environment from the AMIs. Use Amazon EC2 instance store volumes for temporary simulation data - While AMIs do provide a fast way to clone EC2 instances, EBS-backed AMIs are optimized for instance bootstrapping, not large-scale data cloning or simulation. Additionally, using instance store volumes for high I/O operations is risky, as they are ephemeral and can be lost on stop/terminate. This setup doesn't offer the persistent high-performance storage required for consistent simulation workloads and would not scale well for repeated cloning needs.\n\nCreate new EBS volumes in the test environment and use AWS Backup to perform a backup job of the production volumes. Restore the backup directly to the test EBS volumes to begin simulations - While AWS Backup supports backing up and restoring EBS volumes, it is designed for data protection and compliance use cases, not for rapid, low-latency cloning. The process of creating a backup job and restoring to new volumes adds significant delay and operational overhead compared to using native EBS snapshots with fast snapshot restore. Additionally, AWS Backup does not provide the same flexibility or speed when initiating volume restoration, especially when frequent or automated cloning is required. This approach does not meet the goal of minimizing the time to clone data for performance-critical testing.\n\nUse Amazon EBS io2 volumes with Multi-Attach enabled. Attach the same production EBS volumes to both the production and test EC2 instances simultaneously to avoid cloning delays and ensure high IOPS performance - While Amazon EBS Multi-Attach allows an io1 or io2 volume to be attached to multiple EC2 instances within the same Availability Zone, it is designed for use in clustered, coordinated applications (such as shared-disk file systems or clustered databases). Attaching the same production volume to both production and test environments introduces the risk of uncoordinated writes, data corruption, and unexpected state changes, especially since the requirement explicitly states that test modifications must not impact production data. Additionally, Multi-Attach does not eliminate the need for cloning, it merely provides concurrent access, which is unsuitable for isolated test simulations.\n\nReferences:\n\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-fast-snapshot-restore.html\n\nhttps://docs.aws.amazon.com/ebs/latest/userguide/ebs-volumes-multi.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q625",
    "questionText": "The engineering team at a weather tracking company wants to enhance the performance of its relational database and is looking for a caching solution that supports geospatial data.\n\nAs a solutions architect, which of the following solutions will you suggest?",
    "options": [
      {
        "text": "Use Amazon ElastiCache for Memcached",
        "isCorrect": false
      },
      {
        "text": "Use Amazon ElastiCache for Redis",
        "isCorrect": true
      },
      {
        "text": "Use Amazon DynamoDB Accelerator (DAX)",
        "isCorrect": false
      },
      {
        "text": "Use AWS Global Accelerator",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon ElastiCache for Redis\n\nAmazon ElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud. Redis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps.\n\nAll Redis data resides in the serverâ€™s main memory, in contrast to databases such as PostgreSQL, Cassandra, MongoDB and others that store most data on disk or on SSDs. In comparison to traditional disk based databases where most operations require a roundtrip to disk, in-memory data stores such as Redis donâ€™t suffer the same penalty. They can therefore support an order of magnitude more operations and faster response times. The result is â€“ blazing fast performance with average read or write operations taking less than a millisecond and support for millions of operations per second.\n\nRedis has purpose-built commands for working with real-time geospatial data at scale. You can perform operations like finding the distance between two elements (for example people or places) and finding all elements within a given distance of a point.\n\nIncorrect options:\n\nUse Amazon ElastiCache for Memcached - Both Redis and MemCached are in-memory, open-source data stores. Memcached, a high-performance distributed memory cache service, is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Memcached does not offer support for geospatial data.\n\n\nvia - https://aws.amazon.com/elasticache/redis-vs-memcached/\n\nUse Amazon DynamoDB Accelerator (DAX) - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases.\n\nUse AWS Global Accelerator - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. This option has been added as a distractor, it has nothing to do with database caching.\n\nReference:\n\nhttps://aws.amazon.com/elasticache/redis-vs-memcached/",
    "awsService": "DynamoDB",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q626",
    "questionText": "A tech enterprise operates several workloads using Amazon EC2, AWS Fargate, and AWS Lambda across various teams. To optimize compute costs, the company has purchased Compute Savings Plans. The cloud operations team needs to implement a solution that not only monitors utilization but also sends automated alerts when coverage levels of the Compute Savings Plans fall below a defined threshold.\n\nWhat is the MOST operationally efficient way to achieve this?",
    "options": [
      {
        "text": "Use AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders",
        "isCorrect": true
      },
      {
        "text": "Configure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly",
        "isCorrect": false
      },
      {
        "text": "Enable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops",
        "isCorrect": false
      },
      {
        "text": "Create a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Budgets to create a daily coverage budget specifically for Compute Savings Plans. Define a coverage threshold and configure notifications to alert relevant stakeholders\n\nAWS Budgets allows organizations to create Savings Plans coverage budgets, which track how much of their compute usage is covered by active Savings Plans. These budgets can be configured to monitor coverage percentages on a daily basis and can trigger alerts via Amazon SNS or email when the coverage drops below a defined threshold (e.g., 90%). This approach requires no custom scripting or infrastructure maintenance and integrates seamlessly with AWS services. It ensures that stakeholders are notified proactively about potential cost inefficiencies without adding administrative overhead.\n\n\nvia - https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html\n\nIncorrect options:\n\nConfigure a custom script that queries the Savings Plans utilization API and pushes results to an Amazon S3 bucket. Use Amazon QuickSight to visualize coverage and email reports weekly - While technically feasible, this solution introduces unnecessary complexity by requiring custom scripting, manual reporting, and delayed insights via weekly visualizations. This approach does not provide real-time alerting or low operational overhead compared to native features in AWS Budgets. It's less efficient and more difficult to maintain.\n\nEnable Compute Optimizer recommendations for EC2 and Fargate. Configure automatic notifications for cost optimization opportunities and Savings Plans coverage drops - AWS Compute Optimizer offers instance type recommendations based on performance metrics but does not track or notify on Savings Plans coverage. It helps right-size workloads but does not integrate with Savings Plans metrics or budgets. Therefore, it cannot fulfill the requirement of alerting on under-coverage.\n\nCreate a standalone dashboard in Amazon CloudWatch to track EC2 and Fargate usage. Use metric math to estimate coverage and trigger alarms - Although Amazon CloudWatch is powerful for metrics and alarms, Savings Plans coverage metrics are not directly available in CloudWatch. You would need to implement a complex estimation process using usage metrics, which is both error-prone and operationally inefficient. AWS Budgets is purpose-built for this use case and is much simpler to implement.\n\nReference:\n\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q627",
    "questionText": "A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances and Spot fleets.\n\nCan you help the intern by selecting the correct options that identify the key characteristics of these two types of Spot entities? (Select two)",
    "options": [
      {
        "text": "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "isCorrect": true
      },
      {
        "text": "Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted",
        "isCorrect": false
      },
      {
        "text": "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity",
        "isCorrect": true
      },
      {
        "text": "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity",
        "isCorrect": false
      },
      {
        "text": "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nSpot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification\n\nSpot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices that Amazon Web Services can interrupt with a 2-minute notification. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted.\n\nA Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity\n\nA Spot fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances. The Spot fleet attempts to launch the number of Spot Instances and On-Demand Instances to meet the target capacity that you specified in the Spot fleet request. A Spot fleet allows you to automatically request and manage multiple Spot instances that provide the lowest price per unit of capacity for your cluster or application, like a batch processing job, a Hadoop workflow, or an HPC grid computing job.\n\n\nvia - https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html\n\nIncorrect options:\n\nA Spot fleet can only consist of a set of Spot Instances that are launched to meet your target capacity\n\nSpot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification\n\nThese two options contradict the explanation provided above, so these options are incorrect.\n\nSpot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted - You could use Spot blocks (now deprecated) to request Amazon EC2 Spot instances for 1 to 6 hours to avoid being interrupted. So, Spot fleets cannot be used for this purpose.\n\nReferences:\n\nhttps://www.amazonaws.cn/en/ec2/spot-instances/faqs/\n\nhttps://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q628",
    "questionText": "The data engineering team at a company wants to analyze Amazon S3 storage access patterns to decide when to transition the right data to the right storage class.\n\nWhich of the following represents a correct option regarding the capabilities of Amazon S3 Analytics storage class analysis?",
    "options": [
      {
        "text": "Storage class analysis only provides recommendations for Standard to Standard One-Zone IA classes",
        "isCorrect": false
      },
      {
        "text": "Storage class analysis only provides recommendations for Standard to Glacier Deep Archive classes",
        "isCorrect": false
      },
      {
        "text": "Storage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes",
        "isCorrect": false
      },
      {
        "text": "Storage class analysis only provides recommendations for Standard to Standard IA classes",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nStorage class analysis only provides recommendations for Standard to Standard IA classes\n\nBy using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class.\n\nStorage class analysis only provides recommendations for Standard to Standard IA classes.\n\nAfter storage class analysis observes the infrequent access patterns of a filtered set of data over a period of time, you can use the analysis results to help you improve your lifecycle configurations. You can configure storage class analysis to analyze all the objects in a bucket. Or, you can configure filters to group objects together for analysis by common prefix (that is, objects that have names that begin with a common string), by object tags, or by both prefix and tags.\n\nIncorrect options:\n\nStorage class analysis only provides recommendations for Standard to Standard One-Zone IA classes\n\nStorage class analysis only provides recommendations for Standard to Glacier Deep Archive classes\n\nStorage class analysis only provides recommendations for Standard to Glacier Flexible Retrieval classes\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q629",
    "questionText": "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations.\n\nWhich of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
    "options": [
      {
        "text": "AWS Web Application Firewall (AWS WAF)",
        "isCorrect": true
      },
      {
        "text": "Amazon GuardDuty",
        "isCorrect": false
      },
      {
        "text": "Amazon Inspector",
        "isCorrect": false
      },
      {
        "text": "AWS Shield Advanced",
        "isCorrect": true
      },
      {
        "text": "VPC Route Table",
        "isCorrect": false
      },
      {
        "text": "VPC Security Group",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nAWS Web Application Firewall (AWS WAF)\n\nAWS Shield Advanced\n\nVPC Security Group\n\nAWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure.\n\nUsing AWS Firewall Manager, you can centrally configure AWS WAF rules, AWS Shield Advanced protection, Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of today.\n\n\nvia - https://aws.amazon.com/firewall-manager/faqs/\n\nIncorrect options:\n\nAmazon GuardDuty - Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. Amazon GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs.\n\nHow Amazon GuardDuty Works:\n\n\nAmazon Inspector - Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.\n\nVPC Route Table - A route table serves as the traffic controller for your virtual private cloud (VPC). Each route table contains a set of rules, called routes, that determine where network traffic from your subnet or gateway is directed.\n\nThese three options are not in the list of AWS resources supported by AWS Firewall Manager, so these options are incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/firewall-manager/faqs/\n\nhttps://aws.amazon.com/guardduty/\n\nhttps://aws.amazon.com/inspector/",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q630",
    "questionText": "A company manages a multi-tier social media application that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones (AZs) and use an Amazon Aurora database. As an AWS Certified Solutions Architect â€“ Associate, you have been tasked to make the application more resilient to periodic spikes in request rates.\n\nWhich of the following solutions would you recommend for the given use-case? (Select two)",
    "options": [
      {
        "text": "Use Amazon Aurora Replica",
        "isCorrect": true
      },
      {
        "text": "Use AWS Shield",
        "isCorrect": false
      },
      {
        "text": "Use AWS Global Accelerator",
        "isCorrect": false
      },
      {
        "text": "Use AWS Direct Connect",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudFront distribution in front of the Application Load Balancer",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nYou can use Amazon Aurora replicas and Amazon CloudFront distribution to make the application more resilient to spikes in request rates.\n\nUse Amazon Aurora Replica\n\nAmazon Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Amazon Aurora Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. Up to 15 Aurora Replicas can be distributed across the Availability Zones (AZs) that a DB cluster spans within an AWS Region.\n\nUse Amazon CloudFront distribution in front of the Application Load Balancer\n\nAmazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. Amazon CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.\n\nAmazon CloudFront offers an origin failover feature to help support your data resiliency needs. Amazon CloudFront is a global service that delivers your content through a worldwide network of data centers called edge locations or points of presence (POPs). If your content is not already cached in an edge location, Amazon CloudFront retrieves it from an origin that you've identified as the source for the definitive version of the content.\n\nIncorrect options:\n\nUse AWS Shield - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency. There are two tiers of AWS Shield - Standard and Advanced. AWS Shield cannot be used to improve application resiliency to handle spikes in traffic.\n\nUse AWS Global Accelerator - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. Amazon Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Since Amazon CloudFront is better for improving application resiliency to handle spikes in traffic, so this option is ruled out.\n\nUse AWS Direct Connect - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC. AWS Direct Connect cannot be used to improve application resiliency to handle spikes in traffic.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/disaster-recovery-resiliency.html\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\n\nhttps://aws.amazon.com/global-accelerator/faqs/\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/disaster-recovery-resiliency.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q631",
    "questionText": "A company uses Amazon DynamoDB as a data store for various kinds of customer data, such as user profiles, user events, clicks, and visited links. Some of these use-cases require a high request rate (millions of requests per second), low predictable latency, and reliability. The company now wants to add a caching layer to support high read volumes.\n\nAs a solutions architect, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)",
    "options": [
      {
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "isCorrect": true
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": true
      },
      {
        "text": "Amazon Relational Database Service (Amazon RDS)",
        "isCorrect": false
      },
      {
        "text": "Amazon OpenSearch Service",
        "isCorrect": false
      },
      {
        "text": "Amazon Redshift",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nAmazon DynamoDB Accelerator (DAX)\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement â€“ from milliseconds to microseconds â€“ even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option.\n\nDAX Overview:\n\nvia - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\n\nAmazon ElastiCache\n\nAmazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option.\n\nIncorrect options:\n\nAmazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Amazon RDS cannot be used as a caching layer for Amazon DynamoDB.\n\nAmazon OpenSearch Service - Amazon OpenSearch Service is a managed service that makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. It cannot be used as a caching layer for Amazon DynamoDB.\n\nAmazon Redshift - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used as a caching layer for Amazon DynamoDB.\n\nReferences:\n\nhttps://aws.amazon.com/dynamodb/dax/\n\nhttps://aws.amazon.com/elasticache/faqs/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q632",
    "questionText": "A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load Balancing?",
    "options": [
      {
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region",
        "isCorrect": false
      },
      {
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region",
        "isCorrect": true
      },
      {
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region",
        "isCorrect": false
      },
      {
        "text": "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region\n\nElastic Load Balancer automatically distributes incoming traffic across multiple targets â€“ Amazon EC2 instances, containers, IP addresses, and Lambda functions â€“ in multiple Availability Zones and ensures only healthy targets receive traffic.\nELB cannot distribute incoming traffic for targets deployed in different regions. This configuration is NOT allowed for the Elastic Load Balancer and therefore this is the correct option.\n\nIncorrect options:\n\nUse the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region\n\nUse the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region\n\nUse the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region\n\nThese three options are valid configurations for the Elastic Load Balancing to distribute traffic (either within an Availability Zone or between two Availability Zones).\n\nReference:\n\nhttps://aws.amazon.com/elasticloadbalancing/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q633",
    "questionText": "A developer has configured inbound traffic for the relevant ports in both the Security Group of the Amazon EC2 instance as well as the network access control list (network ACL) of the subnet for the Amazon EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance.\n\nAs a solutions architect, how will you fix this issue?",
    "options": [
      {
        "text": "Network access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic",
        "isCorrect": false
      },
      {
        "text": "IAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL)",
        "isCorrect": false
      },
      {
        "text": "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic",
        "isCorrect": true
      },
      {
        "text": "Rules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSecurity Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network access control list (network ACL) are stateless, so you must allow both inbound and outbound traffic\n\nSecurity groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.\n\nTo enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.\n\nThe designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.\n\nBy default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.\n\nIf you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway.\n\nIncorrect options:\n\nNetwork access control list (network ACL) are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic - This is incorrect as already discussed.\n\nIAM Role defined in the Security Group is different from the IAM Role that is given access in the network access control list (network ACL) - This is a made-up option and just added as a distractor.\n\nRules associated with network access control list (network ACL) should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool.\n\nReference:\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q634",
    "questionText": "The engineering team at a social media company has noticed that while some of the images stored in Amazon S3 are frequently accessed, others sit idle for a considerable span of time.\n\nAs a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
    "options": [
      {
        "text": "Store the images using the Amazon S3 Intelligent-Tiering storage class",
        "isCorrect": true
      },
      {
        "text": "Store the images using the Amazon S3 Standard-IA storage class",
        "isCorrect": false
      },
      {
        "text": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "isCorrect": false
      },
      {
        "text": "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nStore the images using the Amazon S3 Intelligent-Tiering storage class\n\nThe Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.\n\nFor a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. Therefore using the Amazon S3 Intelligent-Tiering storage class is the correct solution for the given problem statement.\n\nAmazon S3 Storage Classes Overview:\n\n\nIncorrect options:\n\nStore the images using the Amazon S3 Standard-IA storage class\n\nAmazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As some of the objects are frequently accessed, the per GB retrieval fee for Amazon S3 Standard-IA can cause the costs to shoot up, hence this option is incorrect.\n\nCreate a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are migrated to Amazon S3 Standard class\n\nCreate a data monitoring application on an Amazon EC2 instance in the same region as the bucket storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are migrated to Amazon S3 Standard class\n\nCreating a data monitoring application on an Amazon EC2 instance for managing the desired Amazon S3 storage class entails significant development cost as well as infrastructure maintenance effort. The Amazon S3 Intelligent-Tiering storage class does the job in a cost-effective way. Therefore both these options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/s3/storage-classes/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q635",
    "questionText": "A healthcare startup is deploying an AWS-based analytics platform that processes sensitive patient records. The application backend uses Amazon RDS for structured data and Amazon S3 for storing medical files. S3 Event Notifications trigger AWS Lambda for real-time data classification and alerting. The startup uses AWS IAM Identity Center to manage federated access from their enterprise directory. Development, operations, and compliance teams require granular and secure access to RDS and S3 resources, based strictly on their job roles. The company must follow the principle of least privilege while minimizing manual administrative work.\n\nWhich solution should the company implement to meet these requirements with the least operational overhead?",
    "options": [
      {
        "text": "Create individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically",
        "isCorrect": false
      },
      {
        "text": "Create an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3",
        "isCorrect": false
      },
      {
        "text": "Use AWS IAM Identity Center integrated with the organizationâ€™s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets",
        "isCorrect": true
      },
      {
        "text": "Use AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each teamâ€™s responsibilities. Assign users to accounts and let SCPs enforce the required access",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS IAM Identity Center integrated with the organizationâ€™s directory. Define permission sets with least-privilege policies for Amazon RDS and Amazon S3. Assign users to groups based on their team roles and map those groups to the appropriate permission sets\n\nThis solution provides a scalable and centralized way to manage user access by leveraging AWS IAM Identity Center (formerly AWS SSO), which integrates directly with external identity providers (e.g., Active Directory). By defining granular permission sets, administrators can assign precise AWS resource access (e.g., RDS read-only, S3 write) to each team according to their job functions, ensuring compliance with the principle of least privilege. Group memberships in the identity source can be mapped directly to those permission sets, allowing seamless onboarding, offboarding, and role changes without managing individual IAM users or credentials. This minimizes operational overhead while providing centralized visibility, governance, and security over access permissions.\n\n\nvia - https://docs.aws.amazon.com/singlesignon/latest/userguide/permissionsetsconcept.html\n\nIncorrect options:\n\nCreate an IAM identity provider that integrates with the company's IdP (e.g., Azure AD or Okta). Use SAML federation to grant access to IAM roles that are manually assigned to each user. Create and maintain inline IAM policies for each role to access RDS and S3 - Although SAML-based federation provides a secure way to authenticate users, it lacks the automation and manageability offered by IAM Identity Center. Managing inline policies for individual roles increases administrative burden and does not scale well. This solution partially overlaps with IAM Identity Centerâ€™s capabilities but delivers less efficiency and more manual effort.\n\nCreate individual IAM users for each team member. Attach role-based IAM policies granting permissions to RDS and S3 based on team roles. Use AWS IAM Access Analyzer to monitor for unused permissions and rotate access keys periodically -  While this setup enforces the principle of least privilege at the user level, it introduces high operational overhead: individual IAM users must be managed, keys rotated, and policies updated manually. Even with IAM Access Analyzer, this model does not scale well for larger teams or frequent access changes, and does not utilize IAM Identity Center, which is already part of the organizationâ€™s identity strategy.\n\nUse AWS Organizations to group team accounts under a single organizational unit (OU). Attach Service Control Policies (SCPs) to the OU that define access boundaries for Amazon RDS and Amazon S3 based on each teamâ€™s responsibilities. Assign users to accounts and let SCPs enforce the required access - While SCPs are useful for setting permission guardrails at the organizational level, they do not grant permissions themselves â€” they only define what actions are allowed or denied for IAM identities within an account. To actually grant access to Amazon RDS and Amazon S3, you still need IAM roles, users, or permission sets configured within each account. Relying solely on SCPs to enforce access violates the principle of least privilege because it provides no actual identity-level permission management, and would lead to incomplete or nonfunctional access control unless paired with proper IAM configurations. This also introduces unnecessary operational overhead and does not leverage AWS IAM Identity Center, which is already in use.\n\nReferences:\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/getting-started.html\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/permissionsetsconcept.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q636",
    "questionText": "A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application.\n\nWhich of the following represents the best way of configuring a solution for this requirement with minimal effort?",
    "options": [
      {
        "text": "Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process",
        "isCorrect": false
      },
      {
        "text": "Run the custom scripts as user data scripts on the Amazon EC2 instances",
        "isCorrect": true
      },
      {
        "text": "Run the custom scripts as instance metadata scripts on the Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Use AWS CLI to run the user data scripts only once while launching the instance",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nRun the custom scripts as user data scripts on the Amazon EC2 instances\n\nWhen you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives.\n\nBy default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. Hence, no extra configuration is needed, apart from including the custom scripts in user data scripts.\n\nIncorrect options:\n\nUpdate Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process - You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. By default, the scripts are run, only once during the boot process while first launching the instance.\n\nRun the custom scripts as instance metadata scripts on the Amazon EC2 instances- Instance metadata is data about your instance that you can use to configure or manage the running instance. Metadata cannot be used to run custom scripts.\n\nUse AWS CLI to run the user data scripts only once while launching the instance - This statement is incorrect and used only as a distractor.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q637",
    "questionText": "A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect.\n\nWhat do you recommend?",
    "options": [
      {
        "text": "VPC Peering Connection",
        "isCorrect": false
      },
      {
        "text": "Virtual private gateway (VGW)",
        "isCorrect": false
      },
      {
        "text": "AWS Transit Gateway",
        "isCorrect": true
      },
      {
        "text": "AWS PrivateLink",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAWS Transit Gateway\n\nAWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes.\nSo, this is a perfect use-case for the Transit Gateway.\n\nWithout AWS Transit Gateway\n\nvia - https://aws.amazon.com/transit-gateway/\n\nWith AWS Transit Gateway\n\nvia - https://aws.amazon.com/transit-gateway/\n\nIncorrect options:\n\nVPC Peering Connection - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).\n\nVPC Peering helps connect two VPCs and is not transitive. It would require to create many peering connections between all the VPCs to have them connect. This alone wouldn't work, because we would need to also connect the on-premises data center through Direct Connect and Direct Connect Gateway, but that's not mentioned in this answer.\n\nVirtual private gateway (VGW) - A virtual private gateway (VGW), also known as a VPN Gateway, is the endpoint on the VPC side of your VPN connection. You can create a virtual private gateway before creating the VPC itself.\nVPN Gateway is a distractor here because we haven't mentioned a VPN.\n\nAWS PrivateLink - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.\nPrivate Link is utilized to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering, and allowing the connections between the two to remain within the AWS network.\n\nReferences:\n\nhttps://aws.amazon.com/transit-gateway/\n\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_CreateVpnGateway.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q638",
    "questionText": "A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for operating system patch management and third-party software maintenance. To facilitate this, the engineering team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly available configuration.\n\nWhich of the following options would you suggest?",
    "options": [
      {
        "text": "Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2",
        "isCorrect": false
      },
      {
        "text": "Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2",
        "isCorrect": false
      },
      {
        "text": "Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2",
        "isCorrect": false
      },
      {
        "text": "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nSet up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2\n\nA NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances.\n\nFor the given use case, the Amazon EC2 instances in the private subnets can connect to the internet through public NAT gateways in their respective Availability Zones (AZ). You should create public NAT gateway in the public subnet of each AZ and must associate an elastic IP address with the NAT gateway at creation. Then, you can route traffic from the NAT gateway to the internet gateway for the VPC.\n\nIf you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gatewayâ€™s Availability Zone is down, resources in the other Availability Zones lose internet access. To create a highly available or an Availability Zone independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.\n\n\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nIncorrect options:\n\nSet up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2 - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create NAT gateways in the private subnet for the given use case.\n\nSet up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public subnet PU1 in any of the Availability Zones A1 or A2 - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create both NAT gateways in a single public subnet, as this configuration would not be highly available.\n\nSet up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of the Availability Zones A1 or A2 - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of each AZ. You cannot create a single NAT gateway, as this configuration would not be highly available.\n\nReference:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q639",
    "questionText": "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to cryptocurrency mining.\n\nWhich AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the future?",
    "options": [
      {
        "text": "AWS Web Application Firewall (AWS WAF)",
        "isCorrect": false
      },
      {
        "text": "AWS Shield Advanced",
        "isCorrect": false
      },
      {
        "text": "AWS Firewall Manager",
        "isCorrect": false
      },
      {
        "text": "Amazon GuardDuty",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nAmazon GuardDuty\n\nAmazon GuardDuty continuously monitors for malicious or unauthorized behavior to help protect your AWS resources, including your AWS accounts and access keys. Amazon GuardDuty identifies any unusual or unauthorized activity, like cryptocurrency mining or infrastructure deployments in a region that has never been used. Powered by threat intelligence and machine learning, GuardDuty is continuously evolving to help you protect your AWS environment.\n\nThe cryptocurrency finding expands the serviceâ€™s ability to detect Amazon EC2 instances querying IP addresses associated with the cryptocurrency-related activity. The finding type is: CryptoCurrency:EC2/BitcoinTool.B, CryptoCurrency:EC2/BitcoinTool.B!DNS.\n\nThis finding informs you that the listed Amazon EC2 instance in your AWS environment is querying a domain name that is associated with Bitcoin or other cryptocurrency-related activity. Bitcoin is a worldwide cryptocurrency and digital payment system that can be exchanged for other currencies, products, and services. Bitcoin is a reward for bitcoin mining and is highly sought after by threat actors.\n\nIf you use the Amazon EC2 instance to mine or manage cryptocurrency, or this instance is otherwise involved in blockchain activity, this finding could represent expected activity for your environment. If this is the case in your AWS environment, AWS recommends that you set up a suppression rule for this finding.\n\nIncorrect options:\n\nAWS Web Application Firewall (AWS WAF) - AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting.\n\nAWS Shield Advanced - For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS-related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 charges.\n\nAWS Firewall Manager - AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure, from a central administrator account.\n\nNone of these three services can detect unauthorized cryptocurrency mining activity on EC2 instances, so these options are incorrect.\n\nReference:\n\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q640",
    "questionText": "A company is developing a document management application on AWS. The application runs on Amazon EC2 instances in multiple Availability Zones (AZs). The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to consider other options to meet the availability requirement.\n\nAs a solutions architect, which of the following will you recommend?",
    "options": [
      {
        "text": "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store",
        "isCorrect": false
      },
      {
        "text": "Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones",
        "isCorrect": false
      },
      {
        "text": "Provision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nSet up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 as the document store\n\nInstances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. When you launch an Amazon EBS-backed instance, AWS creates an Amazon EBS volume for each Amazon EBS snapshot referenced by the AMI you use. An Amazon EBS-backed instance can be stopped and later restarted without affecting data stored in the attached volumes.\n\nAmazon S3 provides access to reliable, fast, and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web. S3 is highly available and can be configured to work as a document store for the given use case.\n\nIncorrect options:\n\nSet up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use Amazon S3 Glacier as the document store - As the documents need to be returned immediately when requested, Amazon S3 Glacier is not the right fit, since there is a lag of several minutes/hours when you want to read data from Glacier.\n\nCreate snapshots for the Amazon EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones - You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. Hence, using Amazon EBS volumes as a primary storage solution is ineffective, and creating recurring snapshots is a management nightmare for the current use case.\n\nProvision at least three Provisioned IOPS Amazon Instance Store volumes for the Amazon EC2 instances and then mount these volumes to multiple Amazon EC2 instances - You cannot mount Instance Store volumes to multiple Amazon EC2 instances. An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\n\nhttps://aws.amazon.com/s3/storage-classes/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q641",
    "questionText": "A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern.\n\nWhich of the following Amazon SQS queue types is the best fit to meet this requirement?",
    "options": [
      {
        "text": "Amazon Simple Queue Service (Amazon SQS) dead-letter queues",
        "isCorrect": false
      },
      {
        "text": "Amazon Simple Queue Service (Amazon SQS) FIFO queues",
        "isCorrect": false
      },
      {
        "text": "Amazon Simple Queue Service (Amazon SQS) temporary queues",
        "isCorrect": true
      },
      {
        "text": "Amazon Simple Queue Service (Amazon SQS) delay queues",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon Simple Queue Service (Amazon SQS) temporary queues\n\nTemporary queues help you save development time and deployment costs when using common message patterns such as request-response. You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues.\n\nThe client maps multiple temporary queuesâ€”application-managed queues created on demand for a particular processâ€”onto a single Amazon SQS queue automatically. This allows your application to make fewer API calls and have a higher throughput when the traffic to each temporary queue is low. When a temporary queue is no longer in use, the client cleans up the temporary queue automatically, even if some processes that use the client aren't shut down cleanly.\n\nThe following are the benefits of temporary queues:\n\n\nThey serve as lightweight communication channels for specific threads or processes.\nThey can be created and deleted without incurring additional costs.\nThey are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues.\n\n\nTo better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client. This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. The key concept behind the client is the virtual queue. Virtual queues let you multiplex many low-traffic queues onto a single Amazon SQS queue. Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue.\n\nEnd-to-end process for sending messages through virtual queues:\n\nvia - https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/\n\nIncorrect options:\n\nAmazon Simple Queue Service (Amazon SQS) dead-letter queues - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. Amazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue.\n\nAmazon Simple Queue Service (Amazon SQS) FIFO queues - Amazon SQS FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. FIFO queues also provide exactly-once processing but have a limited number of transactions per second (TPS).\n\nAmazon Simple Queue Service (Amazon SQS) delay queues - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html\n\nhttps://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/",
    "awsService": "SQS",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q642",
    "questionText": "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that needs to support relational queries. The application will have unpredictable spikes of usage that the team does not know in advance.\n\nWhich database would you recommend using?",
    "options": [
      {
        "text": "Amazon Aurora Serverless",
        "isCorrect": true
      },
      {
        "text": "Amazon ElastiCache",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB with Provisioned Capacity and Auto Scaling",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB with On-Demand Capacity",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon Aurora Serverless\n\nAmazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. The database design for an OLTP application fits the relational model, therefore you can infer an OLTP system as a Relational Database.\n\nAmazon Aurora Serverless is the perfect way to create a database that can scale down to 0 servers, and scale up to many servers, as an OLTP database. So this is the correct option.\n\nIncorrect options:\n\nAmazon DynamoDB with Provisioned Capacity and Auto Scaling\n\nAmazon DynamoDB with On-Demand Capacity\n\nAmazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.\n\nAmazon DynamoDB is a NoSQL database and doesn't do relational queries, therefore it's a choice we have to eliminate, even though the two modes proposed here help us cope with an unpredictable amount of usage. So both these options are incorrect.\n\nAmazon ElastiCache - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Amazon Elasticache is used as a caching layer in front of relational databases. Amazon ElastiCache is a NoSQL database and doesn't facilitate relational queries, so this option is ruled out.\n\nReferences:\n\nhttps://aws.amazon.com/rds/aurora/serverless/\n\nhttps://aws.amazon.com/rds/",
    "awsService": "Auto Scaling",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q643",
    "questionText": "The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The process can withstand any interruptions in its execution and start over again. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS.\n\nWhich of the following options do you recommend as the MOST cost-effective solution?",
    "options": [
      {
        "text": "Run on a Spot Instance with a persistent request type",
        "isCorrect": true
      },
      {
        "text": "Run on Amazon EMR",
        "isCorrect": false
      },
      {
        "text": "Run on AWS Lambda",
        "isCorrect": false
      },
      {
        "text": "Run on an Application Load Balancer",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nRun on a Spot Instance with a persistent request type\n\nA Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The request type (one-time or persistent) determines whether the request is opened again when Amazon EC2 interrupts a Spot Instance or if you stop a Spot Instance. If the request is persistent, the request is opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance.\n\nIncorrect options:\n\nRun on an Application Load Balancer - Application Load Balancer operates at the request level (layer 7), routing traffic to targets â€“ Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nApplication Load Balancer helps distribute load for HTTP(S) requests. This option has been added as a distractor.\n\nRun on Amazon EMR - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\n\nAmazon EMR is to run Big Data load that is meant to be run on Hadoop, this is also a distractor.\n\nRun on AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\n\nAWS Lambda would be the perfect fit if our script could run in less than 15 minutes, as this is the maximum timeout for AWS Lambda.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q644",
    "questionText": "Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales.\n\nWhich of the following is the MOST cost-optimal solution to fix this issue?",
    "options": [
      {
        "text": "Create a Read Replica in the same Region as the Master database and point the analytics workload there",
        "isCorrect": true
      },
      {
        "text": "Migrate the analytics application to AWS Lambda",
        "isCorrect": false
      },
      {
        "text": "Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database",
        "isCorrect": false
      },
      {
        "text": "Create a Read Replica in another Region as the Master database and point the analytics workload there",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a Read Replica in the same Region as the Master database and point the analytics workload there\n\nAmazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source database instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.\n\nCreating a Read Replica is the answer. As we want to minimize the costs, we need to launch the Read Replica in the same Region as you are not charged for the data transfer incurred in replicating data between your source database instance and read replica within the same AWS Region.\n\nExam Alert:\n\nPlease review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS:\n\nvia - https://aws.amazon.com/rds/features/multi-az/\n\nIncorrect options:\n\nEnable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.\n\nEnabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure. So this option is not correct.\n\nMigrate the analytics application to AWS Lambda- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\n\nRunning the application on AWS Lambda will not help, as it will still run against the main database and slow down our e-commerce application.\n\nCreate a Read Replica in another Region as the Master database and point the analytics workload there - This is incorrect because we have to pay for inter-Region data replication charges for the Read Replica, whereas the replication of data within a single Region is free.\n\nReferences:\n\nhttps://aws.amazon.com/rds/features/multi-az/\n\nhttps://aws.amazon.com/rds/features/read-replicas/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q645",
    "questionText": "You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50 terabytes of data.\n\nHow can you build this index efficiently?",
    "options": [
      {
        "text": "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS",
        "isCorrect": true
      },
      {
        "text": "Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index",
        "isCorrect": false
      },
      {
        "text": "Create an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS",
        "isCorrect": false
      },
      {
        "text": "Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in Amazon RDS\n\nAmazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.\n\nUsing the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted.\n\nA byte-range request is a perfect way to get the beginning of a file and ensuring we remain efficient during our scan of our Amazon S3 bucket. So this is the correct option.\n\nIncorrect options:\n\nUse the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL query to build the index - You cannot import data from Amazon S3 into Amazon RDS, so this option is incorrect.\n\nCreate an application that will traverse the Amazon S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in Amazon RDS - If you build an application that loads all the files from Amazon S3, that would work, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect.\n\nCreate an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in Amazon RDS - Amazon S3 Select is a new Amazon S3 capability designed to pull out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in Amazon S3. You cannot use Byte Range Fetch parameter with S3 Select to traverse the Amazon S3 bucket and get the first bytes of a file. So this option is incorrect.\n\nExam Alert:\n\nPlease note that with Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte).\n\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range",
    "awsService": "S3",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q646",
    "questionText": "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines.\n\nWhich of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?",
    "options": [
      {
        "text": "Dedicated Instances",
        "isCorrect": true
      },
      {
        "text": "Spot Instances",
        "isCorrect": false
      },
      {
        "text": "Dedicated Hosts",
        "isCorrect": false
      },
      {
        "text": "On-Demand Instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDedicated Instances\n\nDedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.\n\nA Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.\n\nDifferences between Dedicated Hosts and Dedicated Instances:\n\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\n\nIncorrect options:\n\nSpot Instances -  A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.\n\nDedicated Hosts - An Amazon EC2 Dedicated Host is a physical server with Amazon EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on Amazon EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.\n\nOn-Demand Instances - With On-Demand Instances, you pay for the compute capacity by the second with no long-term commitments. You have full control over its lifecycleâ€”you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.\n\nHigh Level Overview of Amazon EC2 Instance Purchase Options:\n\nvia - https://aws.amazon.com/ec2/pricing/\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q647",
    "questionText": "An organization has rolled out a multi-account architecture using AWS Control Tower to isolate development environments. Each developer has their own dedicated AWS account to provision and test workloads. However, the company is concerned about unexpected spikes in resource usage and AWS spending from individual developer accounts. The leadership team wants to implement a cost control mechanism that can proactively enforce budget limits, ensure automatic responses to overspending, and require minimal ongoing administrative effort.\n\nWhat is the most efficient solution to meet this goal with the least operational overhead?",
    "options": [
      {
        "text": "Deploy an AWS Lambda function to run daily in each developerâ€™s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule",
        "isCorrect": false
      },
      {
        "text": "Use AWS Budgets to define spending thresholds for each developerâ€™s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developerâ€™s primary IAM role when the budget threshold is crossed",
        "isCorrect": true
      },
      {
        "text": "Use AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day",
        "isCorrect": false
      },
      {
        "text": "Use AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Budgets to define spending thresholds for each developerâ€™s account. Configure budget alerts to notify developers when actual or forecasted usage exceeds the set limit. Attach Budgets actions to automatically apply a restrictive DenyAll IAM policy to the developerâ€™s primary IAM role when the budget threshold is crossed\n\nThis solution is optimal because it combines cost visibility, real-time notifications, and automated enforcement in a single, low-maintenance workflow. AWS Budgets allows administrators to define both actual and forecasted budget thresholds at the account level and set up alerts that are automatically triggered when spending exceeds defined limits. By attaching Budgets actions, administrators can enforce access control dynamically â€” for example, by attaching a DenyAll IAM policy to a user or role when their budget is breached. This ensures that developers cannot launch new costly resources beyond their allocation, providing a proactive and automated way to control costs. Since the setup is centralized and integrated with AWS Organizations and Control Tower, it minimizes operational overhead while maintaining governance.\n\nThis option combines budget tracking, notification, and enforcement using native AWS Budgets capabilities. With Budgets actions, administrators can define automated responses (e.g., applying a DenyAll policy) when a userâ€™s spend exceeds the defined budget. This solution is fully serverless, scalable, and low maintenance â€” ideal for controlling developer costs without requiring daily manual intervention.\n\nIncorrect options:\n\nDeploy an AWS Lambda function to run daily in each developerâ€™s account. Use the function to analyze cost usage reports via the Cost Explorer API. If costs exceed a predefined threshold, the function invokes an AWS Config remediation rule - This approach is operationally intensive and not scalable. It relies on building and maintaining custom Lambda logic in each account and introduces complexity with parsing cost data via API. AWS Budgets provides a better way to control cost thresholds and actions using native automation.\n\nUse AWS Service Catalog to restrict developers to predefined resource templates with pricing limits. In each developer account, create a scheduled Lambda function that stops all running resources at the end of the day and and restart these resources at the start of next business day - Although Service Catalog can enforce standardized deployments, it doesn't prevent developers from incurring costs outside the template constraints. Scheduled resource stoppage helps reduce idle costs but does not prevent overspending due to resource scaling or intensive workloads. Also, maintaining separate Lambda functions per account adds operational complexity.\n\nUse AWS Cost Explorer to enable detailed usage and cost reports for each developer account. Configure daily usage reports to be emailed to developers. Create dashboards for each developer in Cost Explorer, and require them to monitor their resource consumption and take action if they approach spending thresholds - While AWS Cost Explorer is useful for visualizing and analyzing cost data, it is a passive monitoring tool â€” it does not provide enforcement capabilities. There is no built-in way to automatically restrict or deny actions when budgets are exceeded. Requiring developers to monitor their own spending is also not scalable and introduces high operational risk. This approach violates the requirement for automated budget enforcement with minimal overhead.\n\nReferences:\n\nhttps://aws.amazon.com/blogs/aws-cloud-financial-management/get-started-with-aws-budgets-actions/\n\nhttps://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q648",
    "questionText": "A financial services firm operates a mission-critical transaction processing platform hosted in the AWS us-east-2 Region. The backend is powered by a MySQL-compatible Amazon Aurora cluster, with high transaction volumes throughout the day. As part of its business continuity planning, the firm has selected us-west-2 as its designated disaster recovery (DR) Region.\n\nThe firm has defined strict DR objectives:\n\nRecovery Point Objective (RPO): â‰¤ 5 minutes\n\nRecovery Time Objective (RTO): â‰¤ 15 minutes\n\nLeadership has asked for a DR solution that ensures fast cross-regional failover with minimal operational overhead and configuration effort. What do you recommend?",
    "options": [
      {
        "text": "Convert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives",
        "isCorrect": true
      },
      {
        "text": "Provision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events",
        "isCorrect": false
      },
      {
        "text": "Create an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover",
        "isCorrect": false
      },
      {
        "text": "Deploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConvert the Aurora cluster to an Aurora global database, with the secondary cluster deployed in us-west-2. Rely on Aurora global database managed failover to meet RTO and RPO objectives\n\nThis solution is the most operationally efficient and fully meets the companyâ€™s disaster recovery (DR) objectives. Aurora global databases are specifically designed for cross-Region deployments that require low-latency replication and high availability. By converting the existing Aurora MySQL cluster to a global database, the primary Region (us-east-2) replicates data to a secondary Aurora cluster in the DR Region (us-west-2) with typically less than 1 second of replication lag, ensuring the companyâ€™s RPO of 5 minutes is comfortably met. Additionally, managed failover capabilities allow for a fast, coordinated promotion of the secondary cluster to primary with minimal downtime, satisfying the RTO of 15 minutes. This approach requires minimal configuration changes and automates much of the failover process, significantly reducing operational burden while maintaining data consistency and business continuity.\n\n\nvia - https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/\n\nIncorrect options:\n\nProvision a separate Aurora MySQL-compatible cluster in us-west-2, and configure AWS Database Migration Service (AWS DMS) to replicate data from the primary database to the DR cluster continuously. Perform manual failover during DR events - While DMS supports near-real-time replication, it does not guarantee low-latency replication suitable for a 5-minute RPO, especially during high loads. DMS also introduces additional operational complexity and lacks native Aurora integration for seamless failover. AWS Database Migration Service (AWS DMS) is generally designed for database migrations or light-weight replication tasks, not for high-throughput, low-latency disaster recovery needs. While DMS can perform ongoing replication, it does not guarantee latency under 5 minutes, especially during peak traffic or write-heavy workloads. Moreover, DMS lacks the tight native integration with Amazon Aurora that global databases offer â€” meaning failover must be manually orchestrated, increasing operational complexity and potentially pushing the RTO beyond 15 minutes. Additionally, managing DMS tasks at scale (monitoring, restarting, troubleshooting) requires more administrative effort compared to the managed, built-in replication and failover mechanisms of Aurora global databases.\n\nCreate an Aurora read replica in us-west-2 with equivalent capacity to the primary cluster's writer node in us-east-2. Monitor replication health and configure a manual promotion process for failover - Aurora cross-Region read replicas do not offer managed failover and require manual intervention and additional orchestration during a disaster. Although replication latency is low, managing failover manually adds operational burden and increases risk of human error, affecting RTO.\n\nDeploy a separate Aurora cluster in us-west-2, and use scheduled AWS Lambda functions with custom scripts to export and import snapshots from us-east-2 every 5 minutes - Snapshot export/import is not near real-time, and even with scripting, this method cannot guarantee a 5-minute RPO. Snapshot-based DR also requires manual failover orchestration, violating both the RPO and RTO requirements.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/implement-cross-region-disaster-recovery-with-aws-dms-and-amazon-aurora.html\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/Welcome.html",
    "awsService": "Lambda",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q649",
    "questionText": "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes.\n\nWhich Amazon EBS volume type is the correct choice for these Amazon EC2 instances?",
    "options": [
      {
        "text": "Provisioned IOPS SSD Amazon EBS volumes",
        "isCorrect": true
      },
      {
        "text": "General-purpose SSD-based Amazon EBS volumes",
        "isCorrect": false
      },
      {
        "text": "Throughput Optimized HDD Amazon EBS volumes",
        "isCorrect": false
      },
      {
        "text": "Cold HDD Amazon EBS volumes",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nProvisioned IOPS SSD Amazon EBS volumes\n\nAmazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an instance or set of instances. Each instance to which the volume is attached has full read and write permission to the shared volume. Multi-Attach makes it easier for you to achieve higher application availability in clustered Linux applications that manage concurrent write operations.\n\nMulti-Attach is supported exclusively on Provisioned IOPS SSD volumes.\n\nIncorrect options:\n\nGeneral-purpose SSD-based Amazon EBS volumes - These SSD-backed Amazon EBS volumes provide a balance of price and performance. AWS recommends these volumes for most workloads. These volume types are not supported for Multi-Attach functionality.\n\nThroughput Optimized HDD Amazon EBS volumes - These HDD-backed volumes provide a low-cost HDD designed for frequently accessed, throughput-intensive workloads. These volume types are not supported for Multi-Attach functionality.\n\nCold HDD Amazon EBS volumes - These HDD-backed volumes provide a lowest-cost HDD design for less frequently accessed workloads. These volume types are not supported for Multi-Attach functionality.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q650",
    "questionText": "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection.\n\nWhat is the MOST cost-effective way to establish such a connection?",
    "options": [
      {
        "text": "Set up a bastion host on Amazon EC2",
        "isCorrect": false
      },
      {
        "text": "Set up AWS Direct Connect",
        "isCorrect": false
      },
      {
        "text": "Set up an AWS Site-to-Site VPN connection",
        "isCorrect": true
      },
      {
        "text": "Set up an Internet Gateway between the on-premises data center and AWS cloud",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up an AWS Site-to-Site VPN connection\n\nBy default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. A VPN connection refers to the connection between your VPC and your own on-premises network.\n\nAn AWS Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises) side.\n\nA virtual private gateway (VGW) is the VPN concentrator on the Amazon side of the AWS Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the AWS Site-to-Site VPN connection.\n\nHow virtual private gateway works:\n\nvia - https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\n\nAn AWS transit gateway is a transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. For more information, see Amazon VPC Transit Gateways. You can create a Site-to-Site VPN connection as an attachment on a transit gateway.\n\nHow AWS transit gateway works:\n\nvia - https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\n\nIncorrect options:\n\nSet up a bastion host on Amazon EC2 - A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. The bastion host runs on an Amazon EC2 instance that is typically in a public subnet of your Amazon VPC. Other Amazon EC2 instances can be in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying Amazon EC2 instance running the bastion host. A bastion host cannot be used to set up a connection between its on-premises data center and AWS Cloud.\n\nSet up AWS Direct Connect - AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services. AWS Direct Connect enables customers to have low latency, secure and private connections to AWS for workloads that require higher speed or lower latency than the internet. A Dedicated Connection is made through a 1 Gbps, 10 Gbps, or 100 Gbps Ethernet port dedicated to a single customer. AWS Direct Connect takes about a month to provision the connection, so this option is ruled out for the given use case.\n\nSet up an Internet Gateway between the on-premises data center and AWS cloud - An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An Internet Gateway cannot be used to set up a connection between its on-premises data center and AWS Cloud.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q651",
    "questionText": "Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Amazon Redshift.\n\nWhat do you recommend? (Select two)",
    "options": [
      {
        "text": "Migrate the Amazon Redshift underlying storage to Amazon S3 IA",
        "isCorrect": false
      },
      {
        "text": "Create a smaller Amazon Redshift Cluster with the cold data",
        "isCorrect": false
      },
      {
        "text": "Move the data to Amazon S3 Glacier Deep Archive after 30 days",
        "isCorrect": false
      },
      {
        "text": "Move the data to Amazon S3 Standard IA after 30 days",
        "isCorrect": true
      },
      {
        "text": "Analyze the cold data with Amazon Athena",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nMove the data to Amazon S3 Standard IA after 30 days\n\nAmazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. Amazon S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.\n\nAnalyze the cold data with Amazon Athena\n\nAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Amazon Athena to process logs, perform ad-hoc analysis, and run interactive queries.\n\nMoving the data to Amazon S3 glacier will prevent us from being able to query it.  Therefore, we should migrate the data to Amazon S3 Standard IA and use Amazon Athena to analyze the cold data.\n\nIncorrect options:\n\nMigrate the Amazon Redshift underlying storage to Amazon S3 IA - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.\n\nRedshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also ruled out.\n\nCreate a smaller Amazon Redshift Cluster with the cold data - Creating a smaller cluster with the cold data would not decrease the storage cost of Amazon Redshift, which will only increase with time as we keep on creating data. Therefore this option is ruled out.\n\nMove the data to Amazon S3 Glacier Deep Archive after 30 days - Amazon S3 Glacier Deep Archive (GDA) is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. GDA has a first-byte latency of several hours, so this option is incorrect.\n\nReferences:\n\nhttps://aws.amazon.com/athena/\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q652",
    "questionText": "A media company is relocating its legacy infrastructure to AWS. The on-premises environment consists of multiple virtualized workloads that are tightly coupled to their host operating systems and cannot be containerized or re-architected due to software constraints. Each workload currently runs on a standalone virtual machine. The engineering team plans to run these workloads on Amazon EC2 instances without modifying their core design. The company needs a solution that ensures high availability and fault tolerance in the AWS Cloud.\n\nWhich solution will meet these requirements?",
    "options": [
      {
        "text": "Create Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover.",
        "isCorrect": false
      },
      {
        "text": "Generate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure",
        "isCorrect": true
      },
      {
        "text": "Use AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure",
        "isCorrect": false
      },
      {
        "text": "Containerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nGenerate an Amazon Machine Image (AMI) for each legacy server. Launch two EC2 instances from this AMI, placing one instance in each of two different Availability Zones. Set up a Network Load Balancer (NLB) to route traffic to the instances and to monitor instance health for automatic traffic redirection in case of failure\n\nThis option is correct as it recommends creating an Amazon Machine Image (AMI) of each legacy application server and launching two EC2 instances from that AMI, each in a different Availability Zone. This setup ensures high availability and fault tolerance by distributing the workload across multiple fault-isolated zones. A Network Load Balancer (NLB) is configured to route incoming traffic to the healthy instances based on real-time health checks, providing seamless failover in case one instance becomes unavailable. This solution is ideal for legacy applications that cannot be re-architected, and it complies with the requirement to run on EC2 while minimizing disruption and ensuring resiliency in a cost-effective and operationally efficient manner.\n\nIncorrect option:\n\nCreate Amazon Machine Images (AMIs) for each legacy workload. Use the AMIs to launch Auto Scaling groups with a minimum and maximum capacity of 1 EC2 instance. Place an Application Load Balancer (ALB) in front of the Auto Scaling group to provide routing and health check-based failover. - This option is incorrect because it configures only one EC2 instance in the Auto Scaling group, which does not provide multi-AZ fault tolerance. If the Availability Zone fails, the application becomes unavailable, violating the requirement for high availability.\n\nUse AWS Backup to schedule hourly backups of each EC2 instance to Amazon S3 in a separate Availability Zone. Create a recovery plan that includes manual restoration of instances from backup in the event of a failure - While this solution offers point-in-time recovery, it does not provide fault tolerance or high availability. Restoration from backup is a manual and time-consuming process that can lead to significant downtime â€” violating the requirement for reliable, resilient operation.\n\nContainerize the legacy applications and deploy them to Amazon ECS using the Fargate launch type. Define a task for each workload, and use an Application Load Balancer to route traffic across multiple Fargate tasks running in separate Availability Zones - This option proposes containerizing the legacy applications and running them on ECS with the Fargate launch type, which is a serverless compute engine for containers. However, the question explicitly states that the legacy applications must run on EC2 instances and cannot be modified. Fargate requires applications to be containerized, which involves re-architecting or packaging the workloads into Docker containers â€” a process not feasible in this case due to application constraints. Furthermore, Fargate does not provide direct control over EC2 instances, which violates the architectural requirement. This makes the option technically inappropriate for the scenario.\n\nReferences:\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q653",
    "questionText": "A research firm archives experimental datasets generated by automated laboratory equipment. Each dataset is about 10 MB in size and is initially accessed frequently for analysis within the first month. After this period, the access rate drops significantly, but the data must remain immediately retrievable if needed. Due to compliance policies, each dataset must be retained in AWS storage for exactly 4 years before deletion. The firm currently stores the data in Amazon S3 Standard storage and wants to minimize costs without compromising data availability or retrieval speed.\n\nWhich solution meets these requirements most cost-effectively?",
    "options": [
      {
        "text": "Define an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation",
        "isCorrect": false
      },
      {
        "text": "Define an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation",
        "isCorrect": true
      },
      {
        "text": "Configure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation",
        "isCorrect": false
      },
      {
        "text": "Set up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDefine an S3 Lifecycle policy that transitions datasets to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after creation and schedules the deletion of each object exactly 4 years after its creation\n\nThis solution provides a balanced cost-saving strategy and meets all business requirements. S3 Standard-IA offers significantly lower storage costs than S3 Standard for objects that are infrequently accessed but still need to be retrieved instantly. Since the files are accessed frequently only in the first 30 days and must remain available thereafter, moving them to S3 Standard-IA after 30 days is appropriate. Deleting the files after 4 years using a lifecycle rule ensures compliance and cost efficiency.\n\nIncorrect options:\n\nDefine an S3 Lifecycle policy that transitions datasets to S3 Glacier Instant Retrieval 30 days after creation and schedules the deletion of each object exactly 4 years after its creation - Though S3 Glacier Instant Retrieval (GIR) allows instant access, it is more expensive per GB than S3 Standard-IA for infrequent access. It is typically used for archival data that needs occasional but instant access, not as a substitute for IA. S3 Glacier Instant Retrieval offers significantly lower storage costs at $0.004 per GB per month, compared to $0.0125 per GB per month for S3 Standard-IAâ€”making it about 68% cheaper for storage. However, Glacier Instant Retrieval charges $0.03 per GB retrieved and $0.01 per 1,000 GET requests, while Standard-IA charges only $0.01 per GB retrieved and $0.001 per 1,000 GETs. So, retrieving 100 GB with 10,000 GET requests would cost just $1.01 with Standard-IA, but $3.10 with Glacier Instant Retrieval. Therefore, while GIR is cheaper to store, Standard-IA is more cost-effective when data is accessed even occasionally.\n\nConfigure an S3 Lifecycle policy to migrate datasets to S3 Glacier Flexible Retrieval after 30 days and delete them automatically 4 years after creation - S3 Glacier Flexible Retrieval is designed for archiving data that does not require instant access. Although it is low-cost, its retrieval latency is not immediate and can take minutes to hours depending on the retrieval tier. The requirement clearly states that files must be immediately accessible throughout the 4-year retention period, making this storage class unsuitable.\n\nSet up an S3 Lifecycle configuration to transfer all datasets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days, and permanently delete them 4 years after creation - Although One Zone-IA offers a lower storage price than S3 Standard-IA, it stores data in only one Availability Zone. This poses a data durability risk, especially since the datasets cannot be recreated. The scenario requires resilience and immediate retrieval, which One Zone-IA may not reliably provide if an AZ failure occurs.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\n\nhttps://aws.amazon.com/s3/pricing/\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-infreq-data-access",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q654",
    "questionText": "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets.\n\nHow can you provide these users access in the least possible time, with minimal changes?",
    "options": [
      {
        "text": "Update the Amazon S3 bucket policy",
        "isCorrect": false
      },
      {
        "text": "Create a group, attach the policy to the group and place the users in the group",
        "isCorrect": true
      },
      {
        "text": "Create a policy and assign it manually to the 50 users",
        "isCorrect": false
      },
      {
        "text": "Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a group, attach the policy to the group and place the users in the group\n\nAn IAM group is a collection of IAM users. You can use groups to specify permissions for a collection of users, which can make those permissions easier to manage for those users. For example, you could have a group called Admins and give that group the types of permissions that administrators typically need. Any user in that group automatically has the permissions that are assigned to the group. If a new user joins your organization and should have administrator privileges, you can assign the appropriate permissions by adding the user to that group.\n\nHere creating a group, assigning users to that group and attaching policies to that group is the best way.\n\nIncorrect options:\n\nUpdate the Amazon S3 bucket policy - Updating the Amazon S3 bucket policy could work but would not scale, as the size of the S3 bucket policy is limited (Bucket policies are limited to 20 KB in size).\n\nCreate a policy and assign it manually to the 50 users - An IAM user is an entity that you create in AWS. The IAM user represents the person or service who uses the IAM user to interact with AWS. Primary use for IAM users is to give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI. A user in AWS consists of a name, a password to sign in to the AWS Management Console, and up to two access keys that can be used with the API or CLI.\n\nA policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied.\n\nIdentity-based policies â€“ Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity.\n\nResource-based policies â€“ Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts.\n\nCreating a policy and assigning it manually to users would work but would be hard to scale and manage.\n\nCreate an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM with AWS MFA - AWS MFA adds extra security because it requires users to provide unique authentication from an AWS supported MFA mechanism in addition to their regular sign-in credentials when they access AWS websites or services. AWS MFA cannot help in terms of granting read/write access to only 50 of the IAM users.\n\nReferences:\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q655",
    "questionText": "A global financial services provider operates data analytics workloads across multiple AWS Regions. The company stores regulated datasets in Amazon S3 buckets and requires visibility into security and compliance configurations. As part of a new audit initiative, the compliance team must identify all S3 buckets across the environment that do not have versioning enabled. The solution must scale across all Regions and accounts with minimal manual intervention.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      {
        "text": "Configure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets",
        "isCorrect": false
      },
      {
        "text": "Enable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets",
        "isCorrect": false
      },
      {
        "text": "Create a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket",
        "isCorrect": false
      },
      {
        "text": "Enable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nEnable Amazon S3 Storage Lens with advanced metrics and recommendations. Use the per-bucket dashboard to filter and view versioning status across Regions and identify all buckets that do not have versioning enabled\n\nS3 Storage Lens provides a centralized, organization-wide view of storage usage and activity metrics, including versioning status. When advanced metrics are enabled, the dashboard offers per-bucket insights that allow users to filter and identify buckets where versioning is not turned on. This capability spans all Regions and accounts within an organization, making it highly scalable and efficient. The visual dashboard and downloadable CSV reports simplify compliance and operational reviews, helping teams detect misconfigured buckets quickly without needing to manually inspect each one. This solution offers minimal operational overhead and aligns well with the requirement to monitor versioning across a large-scale, distributed S3 environment.\n\nIncorrect options:\n\nConfigure an AWS CloudTrail trail across all Regions. Create an Amazon EventBridge rule that filters for PutBucketVersioning and DeleteBucketVersioning API calls. Trigger an AWS Lambda function to analyze the bucket configurations and generate a report of unversioned buckets - CloudTrail logs only capture changes and API activity. It does not continuously monitor or provide a current-state overview of versioning settings across all buckets. This approach requires custom scripting and does not scale well across accounts and Regions.\n\nEnable IAM Access Analyzer for all Regions. Review the analyzer reports to identify S3 buckets without versioning enabled and configure IAM policies to restrict access to such buckets - IAM Access Analyzer is used to evaluate resource sharing and access configurations. It does not check for or report on bucket-level settings such as versioning.\n\nCreate a centralized Amazon S3 Multi-Region Access Point for all buckets. Use this access point to perform versioning checks programmatically by inspecting objects' metadata from each bucket - S3 Multi-Region Access Points simplify cross-Region data access but do not provide visibility into bucket configuration or versioning status. It is not designed for compliance checks.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\n\nhttps://aws.amazon.com/s3/storage-lens/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q656",
    "questionText": "A digital media company wants to track user engagement across its streaming platform by capturing events such as video starts, pauses, and search queries. These events must be ingested and analyzed in real time to improve user experience and optimize recommendations. The platform experiences unpredictable spikes in traffic during popular content releases. The company needs a highly scalable and serverless solution that can seamlessly adjust to changing workloads without manual provisioning.\n\nWhich solution will meet these requirements in the MOST efficient and scalable way?",
    "options": [
      {
        "text": "Use an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time",
        "isCorrect": true
      },
      {
        "text": "Use Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically",
        "isCorrect": false
      },
      {
        "text": "Use Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals",
        "isCorrect": false
      },
      {
        "text": "Deploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse an Amazon Kinesis Data Streams stream in on-demand capacity mode to ingest user engagement data. Configure an AWS Lambda function as a consumer to process the events in real time\n\nAmazon Kinesis Data Streams with on-demand capacity mode is ideal for workloads with unpredictable or spiky traffic. It automatically provisions and manages the capacity needed to handle varying ingestion volumes without manual intervention. Paired with AWS Lambda as a consumer, it enables real-time processing of clickstream data in a serverless fashion. This combination ensures low-latency ingestion and processing with minimal operational overhead, making it highly suitable for real-time analytics use cases with fluctuating traffic.\n\nIncorrect options:\n\nUse Amazon Kinesis Data Firehose to ingest user events. Set the destination as Amazon S3. Use Amazon Athena with scheduled queries to analyze the data periodically - Amazon Kinesis Data Firehose is a fully managed service for delivering streaming data to destinations like Amazon S3 and Amazon Redshift. However, it does not support real-time event processing directly; it buffers data before delivery. Furthermore, AWS Glue is not optimized for low-latency, real-time processing of streaming events. This approach introduces delays and does not meet the requirement for real-time analysis.\n\nUse Amazon Simple Notification Service (Amazon SNS) to publish clickstream events. Subscribe an Amazon SQS standard queue to receive the events. Process the events in batches with AWS Glue jobs scheduled at fixed intervals - This approach uses Amazon SNS and SQS for message delivery and AWS Glue for batch processing. While SNS and SQS can distribute messages, they are not ideal for high-throughput real-time ingestion of clickstream data. AWS Glue is primarily suited for batch ETL jobs, not for low-latency real-time processing. This solution does not provide the scalability and responsiveness required for real-time analytics on fluctuating clickstream traffic.\n\nDeploy a fleet of Amazon EC2 instances running Apache Kafka to ingest clickstream data. Set up custom scripts to manually scale the Kafka cluster based on CPU usage. Use Amazon Athena to run periodic queries on stored clickstream logs - Using self-managed Apache Kafka on EC2 introduces significant operational overhead and manual scaling challenges, especially under unpredictable traffic spikes. While Kafka is a capable ingestion tool, managing EC2-based clusters lacks the elasticity and serverless benefits provided by managed AWS services like Kinesis. Additionally, Athena is designed for querying static data stored in Amazon S3, not for real-time streaming analytics.\n\nReferences:\n\nhttps://docs.aws.amazon.com/streams/latest/dev/introduction.html\n\nhttps://aws.amazon.com/blogs/aws/amazon-kinesis-data-streams-on-demand-stream-data-at-scale-without-managing-capacity/\n\nhttps://docs.aws.amazon.com/msk/latest/developerguide/what-is-msk.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q657",
    "questionText": "A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is sent to Amazon SQS.\n\nAs a solutions architect, which of the following solutions would you recommend?",
    "options": [
      {
        "text": "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "isCorrect": false
      },
      {
        "text": "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue",
        "isCorrect": true
      },
      {
        "text": "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling",
        "isCorrect": false
      },
      {
        "text": "Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to prioritize polling for the pro queue over the lite queue\n\nAWS recommends using separate queues to provide prioritization of work. Therefore, for the given use case, you need to create an Amazon SQS standard queue for processing pro users' photos and another Amazon SQS standard queue for processing lite users' photos. Then you can configure Amazon EC2 instances to prioritize polling for the pro queue over the lite queue.\n\n\nvia - https://aws.amazon.com/sqs/features/\n\nIncorrect options:\n\nCreate two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling\n\nCreate two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling\n\nAmazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long-polling doesnâ€™t return a response until a message arrives in the message queue, or the long poll times out. Since long polling or short polling cannot impact the priority of processing for the two queues, so both these options are incorrect.\n\nCreate one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up Amazon EC2 instances to prioritize visibility settings so pro photos are processed first - To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. Setting visibility timeout to zero can result in the same pro photo being processed by more than one consumer. This does not help in prioritizing the processing of pro photos over the lite photos.\n\n\nvia - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\n\nReferences:\n\nhttps://aws.amazon.com/sqs/features/\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q658",
    "questionText": "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS.\n\nAs a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
    "options": [
      {
        "text": "Use Amazon EC2 on-demand instances",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 reserved instances (RI)",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 dedicated instances",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 dedicated hosts",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse Amazon EC2 dedicated hosts\n\nYou can use Dedicated Hosts to launch Amazon EC2 instances on physical servers that are dedicated for your use. Dedicated Hosts give you additional visibility and control over how instances are placed on a physical server, and you can reliably use the same physical server over time. As a result, Dedicated Hosts enable you to use your existing server-bound software licenses like Windows Server and address corporate compliance and regulatory requirements.\n\nIncorrect options:\n\nUse Amazon EC2 dedicated instances - Dedicated instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. Your dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. Dedicated instances may share hardware with other instances from the same AWS account that are not dedicated instances. Dedicated instances cannot be used for existing server-bound software licenses.\n\nUse Amazon EC2 on-demand instances\n\nUse Amazon EC2 reserved instances (RI)\n\nAmazon EC2 presents a virtual computing environment, allowing you to use web service interfaces to launch instances with a variety of operating systems, load them with your custom application environment, manage your networkâ€™s access permissions, and run your image using as many or few systems as you desire.\n\nAmazon EC2 provides the following purchasing options to enable you to optimize your costs based on your needs:\n\nOn-Demand Instances â€“ Pay, by the second, for the instances that you launch.\n\nReserved Instances (RI) â€“ Reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years.\n\nNeither on-demand instances nor reserved instances can be used for existing server-bound software licenses.\n\nReferences:\n\nhttps://aws.amazon.com/ec2/dedicated-hosts/\n\nhttps://aws.amazon.com/ec2/dedicated-hosts/faqs/\n\nhttps://aws.amazon.com/ec2/pricing/dedicated-instances/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q659",
    "questionText": "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use AWS Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters.\n\nAs a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
    "options": [
      {
        "text": "VPC Peering connection",
        "isCorrect": false
      },
      {
        "text": "AWS VPN CloudHub",
        "isCorrect": true
      },
      {
        "text": "Software VPN",
        "isCorrect": false
      },
      {
        "text": "VPC Endpoint",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAWS VPN CloudHub\n\nIf you have multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub. This enables your remote sites to communicate with each other, and not just with the VPC. Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. The VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices.\n\nPer the given use-case, the corporate headquarters has an AWS Direct Connect connection to the VPC and the branch offices have Site-to-Site VPN connections to the VPC. Therefore using the AWS VPN CloudHub, branch offices can send and receive data with each other as well as with their corporate headquarters.\n\nAWS VPN CloudHub:\n\nvia - https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html\n\nIncorrect options:\n\nVPC Endpoint - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet.\nWhen you use VPC endpoint, the traffic between your VPC and the other AWS service does not leave the Amazon network, therefore this option cannot be used to send and receive data between the remote branch offices of the company.\n\nVPC Peering connection - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network.\nVPC peering facilitates a connection between two VPCs within the AWS network, therefore this option cannot be used to send and receive data between the remote branch offices of the company.\n\nSoftware VPN - Amazon VPC offers you the flexibility to fully manage both sides of your Amazon VPC connectivity by creating a VPN connection between your remote network and a software VPN appliance running in your Amazon VPC network. Since Software VPN just handles connectivity between the remote network and Amazon VPC, therefore it cannot be used to send and receive data between the remote branch offices of the company.\n\nReferences:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-vpn-cloudhub-network-to-amazon.html\n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q660",
    "questionText": "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance.\n\nWhich combination of steps should the solutions architect take? (Select two)",
    "options": [
      {
        "text": "Set up AWS Database Migration Service (AWS DMS) to ingest the data",
        "isCorrect": false
      },
      {
        "text": "Set up AWS Lambda with AWS Step Functions to process the data",
        "isCorrect": false
      },
      {
        "text": "Set up Amazon Kinesis Data Streams to ingest the data",
        "isCorrect": true
      },
      {
        "text": "Provision Amazon EC2 instances in an Auto Scaling group to process the data",
        "isCorrect": false
      },
      {
        "text": "Set up AWS Fargate with Amazon ECS to process the data",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nSet up Amazon Kinesis Data Streams to ingest the data\n\nSet up AWS Fargate with Amazon ECS to process the data\n\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\n\nAWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.\n\nFor the given use case, we can use Kinesis Data Streams as the ingestion layer and the containerized ECS application on AWS Fargate as the processing layer. Both these components are serverless and can scale to offer the desired performance.\n\nIncorrect options:\n\nSet up AWS Database Migration Service (AWS DMS) to ingest the data - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. DMS cannot be used for real-time data ingestion. Hence, this option is incorrect.\n\nSet up AWS Lambda with AWS Step Functions to process the data - The maximum timeout value for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the execution of your Lambda function. Since the use case talks about a job that runs for 30 minutes, Lambda is not an option here.\n\nProvision Amazon EC2 instances in an Auto Scaling group to process the data - The given requirement is for a serverless solution to process the data. Hence, provisioning an Amazon EC2 instance is clearly not the right solution.\n\nReference:\n\nhttps://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q661",
    "questionText": "A biotechnology company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze genomes for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud.\n\nAs a solutions architect, which of the following networking components would you recommend on the Amazon EC2 instances running these HPC workflows?",
    "options": [
      {
        "text": "Elastic Fabric Adapter (EFA)",
        "isCorrect": true
      },
      {
        "text": "Elastic Network Interface (ENI)",
        "isCorrect": false
      },
      {
        "text": "Elastic Network Adapter (ENA)",
        "isCorrect": false
      },
      {
        "text": "Elastic IP Address (EIP)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nElastic Fabric Adapter (EFA)\n\nAn Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate High Performance Computing (HPC) and machine learning applications. It enhances the performance of inter-instance communication that is critical for scaling HPC and machine learning applications. EFA devices provide all Elastic Network Adapter (ENA) devices functionalities plus a new OS bypass hardware interface that allows user-space applications to communicate directly with the hardware-provided reliable transport functionality.\n\nHow Elastic Fabric Adapter Works:\n\nvia - https://aws.amazon.com/hpc/efa/\n\nIncorrect options:\n\nElastic Network Interface (ENI) - An Elastic Network Interface (ENI) is a logical networking component in a VPC that represents a virtual network card. You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. The ENI is the simplest networking component available on AWS and is insufficient for HPC workflows.\n\nElastic Network Adapter (ENA) - Elastic Network Adapter (ENA) devices support enhanced networking via single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities. Although enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies, still EFA is a better fit for the given use-case because the EFA device provides all the functionality of an ENA device, plus hardware support for applications to communicate directly with the EFA device without involving the instance kernel (OS-bypass communication) using an extended programming interface.\n\nElastic IP Address (EIP) - An Elastic IP address (EIP) is a static IPv4 address associated with your AWS account. An Elastic IP address is a public IPv4 address, which is reachable from the internet. It is not a networking device that can be used to facilitate HPC workflows.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html\n\nhttps://aws.amazon.com/hpc/efa/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q662",
    "questionText": "An IT company is using Amazon Simple Queue Service (Amazon SQS) queues for decoupling the various components of application architecture. As the consuming components need additional time to process Amazon Simple Queue Service (Amazon SQS) messages, the company wants to postpone the delivery of new messages to the queue for a few seconds.\n\nAs a solutions architect, which of the following solutions would you suggest to the company?",
    "options": [
      {
        "text": "Use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds",
        "isCorrect": false
      },
      {
        "text": "Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds",
        "isCorrect": false
      },
      {
        "text": "Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds",
        "isCorrect": false
      },
      {
        "text": "Use delay queues to postpone the delivery of new messages to the queue for a few seconds",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nUse delay queues to postpone the delivery of new messages to the queue for a few seconds\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\nDelay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.\n\n\nvia - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\n\nIncorrect options:\n\nUse Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds - Amazon SQS FIFO (First-In-First-Out) queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. You cannot use Amazon SQS FIFO queues to postpone the delivery of new messages to the queue for a few seconds.\n\nUse dead-letter queues to postpone the delivery of new messages to the queue for a few seconds - Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. You cannot use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds.\n\nUse visibility timeout to postpone the delivery of new messages to the queue for a few seconds - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html",
    "awsService": "SQS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q663",
    "questionText": "A technology startup has stabilized its cloud infrastructure after a successful product launch. The backend services are now running at a predictable rate with minimal scaling events. The application architecture includes workloads running on Amazon EC2, AWS Lambda functions for asynchronous processing, container workloads on AWS Fargate, and machine learning inference models deployed with Amazon SageMaker. The company is now focusing on reducing long-term operational expenses without redesigning its architecture. The company wants to apply long-term pricing discounts with the least administrative overhead and the broadest service coverage possible using the fewest number of savings plans.\n\nWhich combination of savings plans will satisfy these requirements? (Select two)",
    "options": [
      {
        "text": "Create a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly",
        "isCorrect": false
      },
      {
        "text": "Subscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads",
        "isCorrect": false
      },
      {
        "text": "Purchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type",
        "isCorrect": false
      },
      {
        "text": "Purchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services",
        "isCorrect": true
      },
      {
        "text": "Purchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nPurchase a Compute Savings Plan that provides cost savings for usage across EC2, Fargate, and Lambda services\n\nThe Compute Savings Plan is the most flexible type of Savings Plan offered by AWS, and it applies to a broad range of compute servicesâ€”including Amazon EC2, AWS Fargate, and AWS Lambda. Unlike EC2 Instance Savings Plans, Compute Savings Plans are not tied to a specific instance family or Region, making them ideal for organizations with varied or evolving compute needs. In this scenario, since the company is using EC2, Fargate, and Lambda in a steady state, a Compute Savings Plan provides a unified way to reduce costs across all three services with minimal administrative effort. It allows the company to maintain its existing architecture without modification while achieving long-term cost savings.\n\nPurchase a SageMaker Savings Plan that applies discounted pricing to SageMaker training, inference, and notebook instances\n\nAmazon SageMaker is not included under Compute Savings Plans, so workloads running on SageMaker require a SageMaker-specific Savings Plan for cost optimization. The SageMaker Savings Plan provides flexible discounts for various SageMaker components including training jobs, real-time inference, batch transform, and SageMaker Studio notebooks. By committing to a one- or three-year usage plan, companies can achieve significant cost reductionsâ€”up to 64%â€”compared to On-Demand pricing. In this use case, since SageMaker is used for inference workloads that are stable and predictable, a dedicated SageMaker Savings Plan is the most efficient and operationally simple approach to reduce costs.\n\nIncorrect options:\n\nCreate a Reserved Instance for each EC2 instance and subscribe to AWS Support to monitor Reserved Instance utilization monthly - While Reserved Instances can offer discounts for EC2, they are less flexible than Savings Plans and do not cover services like Fargate, Lambda, or SageMaker. Additionally, subscribing to AWS Support does not contribute to cost savings.\n\nSubscribe to a hybrid deployment discount plan that includes discounts for both AWS and on-premises Kubernetes workloads - No such hybrid deployment savings plan currently exists that combines AWS native compute resources with on-premises services. This option is not technically valid and does not address the company's AWS-only workload requirements.\n\nPurchase an EC2 Instance Savings Plan that covers EC2 and containerized tasks on Amazon ECS running with Fargate launch type - EC2 Instance Savings Plans apply only to EC2 instance families and do not apply to Fargate workloads. Fargate usage requires Compute Savings Plans for cost optimization.\n\nReferences:\n\nhttps://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html\n\nhttps://aws.amazon.com/savingsplans/ml-pricing/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q664",
    "questionText": "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year.\n\nAs a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
    "options": [
      {
        "text": "Amazon EFS Infrequent Access",
        "isCorrect": true
      },
      {
        "text": "Amazon EFS Standard",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Standard",
        "isCorrect": false
      },
      {
        "text": "Amazon S3 Standard-IA",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon EFS Infrequent Access\n\nAmazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by selecting a lifecycle policy that matches your needs.\n\nHow Amazon EFS Infrequent Access Works:\n\nvia - https://aws.amazon.com/efs/features/infrequent-access/\n\nIncorrect options:\n\nAmazon EFS Standard - Amazon EFS Infrequent Access is more cost-effective than EFS Standard for the given use-case, therefore this option is incorrect.\n\nAmazon S3 Standard\n\nAmazon S3 Standard-IA\n\nBoth these options are object-based storage, whereas the given use-case requires a POSIX compliant file storage solution. Hence these two options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/efs/features/infrequent-access/",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q665",
    "questionText": "A global enterprise has onboarded multiple departments into isolated AWS accounts that are part of a unified AWS Organizations structure. Recently, a critical operational alert was missed because it was delivered to the root userâ€™s email address of an account, which is only monitored intermittently. The enterprise wants to redesign its notification handling process to ensure that future communications - categorized by billing, security, and operational relevance - are received promptly by the appropriate teams. The solution should align with AWS security best practices and offer centralized oversight without depending on individual users.\n\nWhich solution meets these requirements in the most secure and scalable way?",
    "options": [
      {
        "text": "Configure each AWS accountâ€™s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications",
        "isCorrect": true
      },
      {
        "text": "Change each AWS accountâ€™s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console",
        "isCorrect": false
      },
      {
        "text": "Set up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as â€œbilling,â€ â€œsecurity,â€ or â€œoperations.â€ Keep the current root email addresses as they are, and rely on this service to triage alerts",
        "isCorrect": false
      },
      {
        "text": "Assign each AWS accountâ€™s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nConfigure each AWS accountâ€™s root user to use an alias that redirects messages to a centralized mailbox monitored by platform administrators. Then assign alternate contacts for each account using company-managed distribution lists for billing, security, and operations to handle service-specific notifications\n\nThis option outlines a secure and scalable approach by combining two AWS best practices: centralized root email aliasing and the use of alternate contacts. By configuring each AWS accountâ€™s root user email as an alias that redirects to a centrally monitored mailbox, the organization maintains control over critical notifications without relying on individual users. This ensures availability, continuity, and a clear audit trail. Simultaneously, assigning AWS alternate contacts with team-based distribution lists for billing, security, and operations enables role-specific alerts to reach the appropriate teams directly. This approach prevents alert fatigue, improves response time, and aligns with AWS-recommended practices for separating duties and minimizing root user reliance. Overall, it ensures that notifications are handled efficiently, securely, and in a way that scales with organizational growth.\n\nIncorrect options:\n\nChange each AWS accountâ€™s root email to a unique departmental email list and configure IAM notification settings to send alerts based on service type. Do not use AWS alternate contacts since notifications are already routed by service in the IAM console - While routing messages through departmental distribution lists may appear beneficial, AWS does not allow IAM settings to route operational alerts by service type. Critical AWS notifications such as billing suspension alerts and security findings are sent only to the root email or the alternate contacts, not through IAM configurations. Moreover, ignoring the alternate contacts feature misses the opportunity to use AWS-native role-based routing.\n\nSet up a centralized email forwarding service with rules that inspect notification content and forward emails to the appropriate team based on keywords such as â€œbilling,â€ â€œsecurity,â€ or â€œoperations.â€ Keep the current root email addresses as they are, and rely on this service to triage alerts - This option depends on keyword-based filtering and a custom forwarding solution, which introduces operational overhead, potential delays, and risk of misrouting critical notifications. It also retains reliance on root email addresses without formally assigning alternate contacts in AWS, meaning AWS service notifications (e.g., billing alerts) may still go only to the root email. This violates the principle of using AWS-native contact configuration mechanisms for high-assurance routing.\n\nAssign each AWS accountâ€™s root user email to a single designated member of the respective department (e.g., security lead or billing analyst). Encourage these individuals to monitor the email accounts regularly. Also configure alternate contacts with the same individual email addresses - This solution introduces significant risk by binding critical communications to individuals. It creates a single point of failureâ€”if the individual is unavailable, on leave, or leaves the company, essential alerts might be missed. It also makes auditing and account recovery more difficult. This setup does not scale well and violates security best practices for root account usage, which should be strictly limited.\n\nReferences:\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/aws-startup-security-baseline/acct-01.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices.html\n\nhttps://docs.aws.amazon.com/accounts/latest/reference/accounts-welcome.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q666",
    "questionText": "A digital media firm is scaling its cloud footprint and wants to isolate development, testing, and production workloads using separate AWS accounts. It also wants a centralized approach to managing networking infrastructure such as subnets and gateways, without repeating configurations in every account. Additionally, the solution must enforce security best practicesâ€”like mandatory logging and guardrailsâ€”when new accounts are created. The firm prefers a low-maintenance, governance-driven setup.\n\nWhich solution best meets these goals while minimizing operational overhead?",
    "options": [
      {
        "text": "Use AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)",
        "isCorrect": true
      },
      {
        "text": "Use AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually",
        "isCorrect": false
      },
      {
        "text": "Use AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance",
        "isCorrect": false
      },
      {
        "text": "Use AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Control Tower to create and govern accounts. Deploy a centralized VPC in a shared networking account and share its subnets across workload accounts using AWS Resource Access Manager (AWS RAM)\n\nThis approach aligns perfectly with AWS multi-account best practices. AWS Control Tower handles account creation and governance while applying automatic guardrails such as Service Control Policies (SCPs), AWS Config rules, and CloudTrail logging. A centralized VPC in a separate networking account can host shared resources, and subnets can be shared across accounts using AWS RAM. This design centralizes NAT gateways, internet access, and routing configurationâ€”reducing duplication and cost. It provides isolation, control, and complianceâ€”all with minimal administrative burden.\n\n\nvia - https://docs.aws.amazon.com/controltower/latest/userguide/what-is-control-tower.html\n\nIncorrect options:\n\nUse AWS Organizations to create new accounts and a shared networking account with a central VPC. Share the VPC subnets via AWS RAM and rely on service control policies (SCPs) to enforce guardrails manually - While using AWS Organizations and RAM to share a centralized VPC is technically valid, this option lacks automation and governance features like those provided by AWS Control Tower. SCPs can be used to enforce some guardrails, but applying them manually introduces operational overhead and increases the risk of misconfiguration. Control Tower automates these tasks, including audit logging, baseline configuration, and landing zone deployment, making it the better fit for minimal effort.\n\nUse AWS Control Tower to launch accounts. Deploy separate VPCs in each workload account and centralize security inspection by using Gateway Load Balancers to route traffic through a shared security appliance - Although Gateway Load Balancer (GWLB) is an advanced solution for centralized traffic inspection (e.g., for firewalls), this architecture significantly increases complexity. Managing and maintaining GWLB across multiple workload VPCs, setting up route tables, and integrating third-party appliances require deep networking expertise and ongoing effort. While Control Tower simplifies account creation, it doesnâ€™t mitigate the operational burden introduced by deploying and managing distributed VPCs and inspection paths. This is over-engineered for the given use case, which prioritizes low operational overhead.\n\nUse AWS Service Catalog to define pre-approved VPC templates. Launch one VPC per workload account from the catalog, and enforce networking guardrails using AWS Config conformance packs - While AWS Service Catalog and Config conformance packs offer strong governance, this approach requires each workload account to maintain its own VPC. This leads to duplicated NAT gateways, internet gateways, and routing policies, which increase cost and management effort. Additionally, Service Catalog lacks native account provisioning or centralized logging â€” both of which are automatically handled by Control Tower. This solution introduces more complexity than necessary for the stated goal of central networking with guardrails.\n\nReferences:\n\nhttps://docs.aws.amazon.com/controltower/latest/userguide/what-is-control-tower.html\n\nhttps://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\n\nhttps://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q667",
    "questionText": "The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements.\n\nWhich of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)",
    "options": [
      {
        "text": "You can't detach an instance store volume from one instance and attach it to a different instance",
        "isCorrect": true
      },
      {
        "text": "Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation",
        "isCorrect": false
      },
      {
        "text": "You can specify instance store volumes for an instance when you launch or restart it",
        "isCorrect": false
      },
      {
        "text": "An instance store is a network storage type",
        "isCorrect": false
      },
      {
        "text": "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nYou can't detach an instance store volume from one instance and attach it to a different instance\n\nYou can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance. The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists.\n\nIf you create an Amazon Machine Image (AMI) from an instance, the data on its instance store volumes isn't preserved\n\nIf you create an AMI from an instance, the data on its instance store volumes isn't preserved and isn't present on the instance store volumes of the instances that you launch from the AMI.\n\nIncorrect options:\n\nInstance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation - When you stop, hibernate, or terminate an instance, every block of storage in the instance store is reset. Therefore, this option is incorrect.\n\nYou can specify instance store volumes for an instance when you launch or restart it - You can specify instance store volumes for an instance only when you launch it.\n\nAn instance store is a network storage type - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer.\n\nReference:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q668",
    "questionText": "A company is deploying a publicly accessible web application. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be offloaded from the Amazon EC2 instances.\n\nWhich solution should a solutions architect implement to address these requirements in the most secure manner?",
    "options": [
      {
        "text": "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "isCorrect": true
      },
      {
        "text": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nSet up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer\n\nA load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. This increases the availability of your application. You add one or more listeners to your load balancer.\n\nWith a Network Load Balancer, you can offload the decryption/encryption of Transport Layer Security (TLS) traffic from your application servers to the Network Load Balancer, which helps you optimize the performance of your backend application servers while keeping your workloads secure. Additionally, Network Load Balancers preserve the source IP of the clients to the back-end applications, while terminating Transport Layer Security (TLS) on the load balancer.\n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.\n\nThe NLB has to be accessible over the internet and hence has to be in a public subnet and will act as a single point-of-contact for all incoming traffic. NLB will forward the incoming traffic to the Amazon EC2 instances managed by the ASG in the private subnet.\n\nExam Alert:\n\nYou should note that the Application Load Balancer also supports Transport Layer Security (TLS) offloading. The Classic Load Balancer supports SSL offloading.\n\nIncorrect options:\n\nSet up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer - The Auto Scaling group with its target EC2 instances should be in the private subnet to avoid access to EC2 instances over the public internet. Having EC2 instances in the public subnet would weaken the security posture of the application. Hence, this option is incorrect.\n\nSet up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer\n\nSet up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer\n\nNLB should be in the public subnet as it represents the internet-facing component of the web tier. Therefore, both these options are incorrect.\n\nReference:\n\nhttps://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q669",
    "questionText": "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory.\n\nWhich AWS Directory Service is the best fit for this requirement?",
    "options": [
      {
        "text": "Active Directory Connector",
        "isCorrect": false
      },
      {
        "text": "Simple Active Directory (Simple AD)",
        "isCorrect": false
      },
      {
        "text": "AWS Transit Gateway",
        "isCorrect": false
      },
      {
        "text": "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nAWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)\n\nAWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, is powered by Windows Server 2012 R2. When you select and launch this directory type, it is created as a highly available pair of domain controllers connected to your virtual private cloud (VPC).\n\nWith AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, including Microsoft SharePoint and custom .NET and SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO).\n\nAWS Managed Microsoft AD is your best choice if you need actual Active Directory features to support AWS applications or Windows workloads, including Amazon Relational Database Service for Microsoft SQL Server. It's also best if you want a standalone AD in the AWS Cloud that supports Office 365 or you need an LDAP directory to support your Linux applications.\n\nIncorrect options:\n\nActive Directory Connector - AD Connector is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory without caching any information in the cloud. AD Connector is your best choice when you want to use your existing on-premises directory with compatible AWS services.\n\nSimple Active Directory (Simple AD) - Simple AD is a standalone directory in the cloud, where you create and manage user identities and manage access to applications. Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. However, note that Simple AD does not support features such as multi-factor authentication (MFA), trust relationships with other domains, Active Directory Administrative Center, PowerShell support, Active Directory recycle bin, group managed service accounts, and schema extensions for POSIX and Microsoft applications.\n\nAWS Transit Gateway - AWS Transit Gateway connects VPCs and on-premises networks through a central hub. Transit Gateway is not an Active Directory service.\n\nReferences:\n\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\n\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html\n\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html",
    "awsService": "General",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q670",
    "questionText": "A global enterprise maintains a hybrid cloud environment and wants to transfer large volumes of data between its on-premises data center and Amazon S3 for backup and analytics workflows. The company has already established a Direct Connect (DX) connection to AWS and wants to ensure high-bandwidth, low-latency, and secure private connectivity without traversing the public internet. The architecture must be designed to access Amazon S3 directly from on-premises systems using this DX connection.\n\nWhich configuration should the network engineering team implement to allow direct access to Amazon S3 from the on-premises data center using Direct Connect?",
    "options": [
      {
        "text": "Use a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network",
        "isCorrect": false
      },
      {
        "text": "Configure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect",
        "isCorrect": false
      },
      {
        "text": "Provision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center",
        "isCorrect": true
      },
      {
        "text": "Use a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nProvision a Public Virtual Interface (Public VIF) on the Direct Connect connection to access Amazon S3 public IP addresses from the on-premises data center\n\nTo access Amazon S3 from an on-premises data center using AWS Direct Connect, the correct approach is to configure a Public Virtual Interface (Public VIF). A Public VIF allows your on-premises router to access public AWS services, such as Amazon S3 and DynamoDB, over the dedicated Direct Connect connection rather than the public internet. When you establish a Public VIF, AWS advertises a set of public IP prefixes corresponding to its services, including Amazon S3. This allows your data center to communicate with these services using public IPs routed privately over Direct Connect, delivering improved performance, lower latency, and enhanced security. Public VIFs are specifically designed for scenarios where you want to connect on-premises infrastructure to AWS public endpoints over private, high-bandwidth links.\n\n\nvia - https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html\n\nIncorrect options:\n\nUse a Private Virtual Interface (Private VIF) on the Direct Connect connection and create a VPC endpoint to route traffic to S3 over the private network - A Private Virtual Interface (Private VIF) is used to connect an on-premises network to VPC resources, such as EC2 instances or RDS databases, over private IP space. However, Amazon S3 is a public AWS service, and Private VIFs cannot natively route traffic to public S3 endpoints. While S3 access via interface VPC endpoints is possible from within the VPC, it does not extend to on-premises access over Private VIF. Therefore, Private VIF is not suitable for directly accessing S3 from on-prem.\n\nConfigure a VPN connection over the public internet to AWS and route S3 traffic through the tunnel instead of using Direct Connect - Using a VPN connection over the public internet to route traffic to S3 bypasses the benefits of Direct Connect, such as low latency, dedicated bandwidth, and private connectivity. VPN traffic traverses the public internet and therefore violates the requirements of the given use case.\n\nUse a Transit Gateway with Direct Connect Gateway to route on-premises traffic through a VPC and then to Amazon S3 using private IP addressing - While Transit Gateway with Direct Connect Gateway is a valid solution for routing on-premises traffic to multiple VPCs over a private Direct Connect connection, it does not enable direct private IP access to Amazon S3, because S3 is a public AWS service with public IP addresses. Even if you configure interface VPC endpoints (AWS PrivateLink) to S3 in a connected VPC, the traffic remains local to that VPC and is not extended to the on-premises network via Transit Gateway. To access S3 from on-premises using Direct Connect, you must provision a Public Virtual Interface, which advertises AWS public service IP ranges to your data center.\n\nReference:\n\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q671",
    "questionText": "An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company runs its reports on the same database. The engineering team has noticed sluggish performance on the database when the analytics reporting process is in progress.\n\nAs an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST cost-optimal solution to improve the performance?",
    "options": [
      {
        "text": "Create a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica",
        "isCorrect": true
      },
      {
        "text": "Create a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica",
        "isCorrect": false
      },
      {
        "text": "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "isCorrect": false
      },
      {
        "text": "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a read-replica with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the read replica\n\nAmazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of database instance called a read replica from a source database instance. The source database instance becomes the primary database instance. Updates made to the primary database instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica.\n\nAmazon RDS Read Replicas:\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\n\nvia - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\nYou can use read replicas to improve the performance of your Amazon RDS MySQL DB by handling business reporting or data warehousing scenarios where you might want business reporting queries to run against your read replica, rather than your production database instance.\n\nYou can create up to five read replicas from one DB instance. For replication to operate effectively, each read replica should have the same amount of compute and storage resources as the source database instance. If you scale the source database instance, also scale the read replicas.\n\n\nvia - https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html\n\nIncorrect options:\n\nCreate a read-replica with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the read replica - As mentioned in the explanation above, you should create a read-replica with the same compute capacity and the same storage capacity as the primary.\n\nCreate a standby instance in a multi-AZ configuration with the same compute capacity and the same storage capacity as the primary. Point the reporting queries to run against the standby instance\n\nCreate a standby instance in a multi-AZ configuration with half compute capacity and half storage capacity as the primary. Point the reporting queries to run against the standby instance\n\nMulti-AZ deployments are not a read scaling solution, so you cannot use a standby to serve read traffic. The standby is there just for failover. Hence both these options are incorrect.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\nhttps://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q672",
    "questionText": "An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading.\n\nAs a solutions architect, which of the following services would you recommend for the given use case?",
    "options": [
      {
        "text": "Amazon ElastiCache for Redis",
        "isCorrect": false
      },
      {
        "text": "Amazon DynamoDB Accelerator (DAX)",
        "isCorrect": false
      },
      {
        "text": "AWS Global Accelerator",
        "isCorrect": false
      },
      {
        "text": "Amazon ElastiCache for Memcached",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nAmazon ElastiCache for Memcached\n\nAmazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store and cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.\n\nMemcached is an open-source, distributed, in-memory key-value store that can retrieve data in milliseconds. Caching site information with Memcached can help you improve the performance and scalability of your site while controlling cost.\n\nChoose Memcached if the following apply to you:\n\nYou need the simplest model possible.\n\nYou need to run large nodes with multiple cores or threads (support for multi-threading).\n\nYou need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases.\n\nYou need to cache objects.\n\n\nvia - https://aws.amazon.com/elasticache/redis-vs-memcached/\n\nIncorrect options:\n\nAmazon ElastiCache for Redis - Redis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps.\n\nRedis does not support multi-threading, so this option is not the right fit for the given use case.\n\nAmazon DynamoDB Accelerator (DAX) - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases.\n\nAWS Global Accelerator - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. This option has been added as a distractor, it has nothing to do with database caching.\n\nReferences:\n\nhttps://aws.amazon.com/caching/aws-caching/\n\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\n\nhttps://aws.amazon.com/elasticache/redis-vs-memcached/",
    "awsService": "DynamoDB",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q673",
    "questionText": "A global logistics provider operates several legacy applications on virtual machines (VMs) within a private data center. Due to accelerated business growth and limited capacity in its existing infrastructure, the provider decides to migrate select applications to AWS. The company opts for a lift-and-shift strategy for its non-mission-critical systems to meet tight migration deadlines. The solution must support rapid migration without requiring extensive application refactoring.\n\nWhich combination of actions will best support this migration approach? (Select three)",
    "options": [
      {
        "text": "Use AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations",
        "isCorrect": false
      },
      {
        "text": "Use AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs",
        "isCorrect": true
      },
      {
        "text": "Shut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation",
        "isCorrect": false
      },
      {
        "text": "Perform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover",
        "isCorrect": true
      },
      {
        "text": "Launch a cutover instance after completing testing and confirming that replication is up-to-date",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nUse AWS Application Migration Service (MGN). Install the AWS Replication Agent on the source VMs\n\nAWS Application Migration Service (MGN) is a highly automated lift-and-shift (rehost) solution that simplifies, expedites, and reduces the cost of migrating applications to AWS. It allows companies to lift-and-shift a large number of physical, virtual, or cloud servers without compatibility issues, performance disruption, or long cutover windows. Application Migration Service replicates source servers into your AWS account.\n\nAWS Application Migration Service (MGN) is the recommended AWS tool for lift-and-shift VM migrations. It supports real-time replication of on-premises workloads to AWS with minimal disruption. The Replication Agent is installed on each source VM to handle continuous data transfer. This eliminates the need for image creation or application rewrites, making it ideal for quick, large-scale migrations.\n\nPerform the initial replication. Launch test instances in AWS to validate the migrated VMs before final cutover\n\nThis reflects a standard step in AWS MGN migration workflows. After the initial sync is complete, test instances are launched to validate the integrity and functionality of the migrated VMs. This is critical for quality assurance before moving workloads into production and helps reduce the risk of failure after cutover.\n\n\nvia - https://docs.aws.amazon.com/mgn/latest/ug/migration-workflow-gs.html\n\nLaunch a cutover instance after completing testing and confirming that replication is up-to-date\n\nThis represents the final step in AWS MGN migrations. After validation testing and a final sync of changes from the on-premises environment, a cutover instance is launched to go live in production. The original VM can be decommissioned once the EC2 instance is confirmed to be working as intended.\n\nIncorrect options:\n\nUse AWS CloudEndure Disaster Recovery to continuously replicate the VMs to AWS and then promote the target instances for production use - While CloudEndure Disaster Recovery provides continuous replication and rapid failover to AWS, it is intended primarily for disaster recovery scenarios, not one-time lift-and-shift migrations. It lacks the account structure, automation, and built-in testing workflows offered by AWS Application Migration Service (MGN), which is optimized specifically for migration use cases. Choosing this option adds unnecessary operational complexity and misuses the DR tooling for a migration task.\n\nUse Amazon EC2 Auto Scaling to automatically re-create the VMs in AWS by launching replacement instances with matching configurations - Amazon EC2 Auto Scaling is designed for horizontal scaling of stateless applications based on performance metrics like CPU or memory usage. It does not support importing or replicating on-premises VMs, nor does it facilitate image migration or workload conversion. Attempting to use Auto Scaling as a migration mechanism would require manual image creation and deep customization, defeating the purpose of a rapid, low-effort lift-and-shift.\n\nShut down the source virtual machines and immediately provision EC2 replacement instances using manual AMI creation - Manually shutting down VMs and creating Amazon Machine Images (AMIs) adds significant risk and overhead. This approach introduces downtime, requires manual reconfiguration, and increases the chance of misalignment between source and target systems. Moreover, it contradicts the benefit of non-disruptive, live migration provided by AWS MGN. This method is not suited for time-sensitive lift-and-shift efforts.\n\nReferences:\n\nhttps://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html\n\nhttps://docs.aws.amazon.com/mgn/latest/ug/migration-workflow-gs.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q674",
    "questionText": "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on Amazon RDS Multi-AZ capabilities.\n\nWhich of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)",
    "options": [
      {
        "text": "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests",
        "isCorrect": false
      },
      {
        "text": "Amazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby",
        "isCorrect": true
      },
      {
        "text": "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason",
        "isCorrect": true
      },
      {
        "text": "For automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database",
        "isCorrect": false
      },
      {
        "text": "Updates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nAmazon RDS applies operating system updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby\n\nRunning a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:\n\nPerform maintenance on the standby.\n\nPromote the standby to primary.\n\nPerform maintenance on the old primary, which becomes the new standby.\n\nWhen you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.\n\nAmazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason\n\nYou also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete.\n\nAnother implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure.\n\nIncorrect options:\n\nFor automated backups, I/O activity is suspended on your primary database since backups are not taken from standby database - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby.\n\nTo enhance read scalability, a Multi-AZ standby instance can be used to serve read requests - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations.\n\nUpdates to your database Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous â€œstandbyâ€ replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.\n\nReference:\n\nhttps://aws.amazon.com/rds/faqs/",
    "awsService": "RDS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q675",
    "questionText": "An edtech startup runs its course-management platform inside a private subnet in a VPC on AWS. The application uses Amazon Cognito user pools for authentication. Now, the team wants to extend the application so that authenticated users can upload and access personal course-related documents in Amazon S3. The solution must ensure scalable, fine-grained and secure access control to the S3 bucket and maintain private network architecture for the application.\n\nWhich combination of steps will enable secure S3 integration for this workload? (Select two)",
    "options": [
      {
        "text": "Create an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate",
        "isCorrect": true
      },
      {
        "text": "Use the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket",
        "isCorrect": false
      },
      {
        "text": "Create an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3",
        "isCorrect": true
      },
      {
        "text": "Configure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access",
        "isCorrect": false
      },
      {
        "text": "Attach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nCreate an Amazon Cognito identity pool to allow federated identities. Use it to generate temporary AWS credentials that grant S3 access when users successfully authenticate\n\nThis option represents the standard and secure pattern for enabling Amazon Cognitoâ€“authenticated users to access AWS services like Amazon S3. While the Cognito user pool handles user authentication (e.g., verifying usernames and passwords), it cannot itself issue AWS credentials for service access. For this, a Cognito identity pool is required. The identity pool integrates with the user pool and generates temporary AWS credentials. These credentials can then be scoped using IAM roles and policies to allow secure, fine-grained access to S3â€”such as granting users permission to upload documents only to specific S3 prefixes based on their identity. This approach adheres to the principle of least privilege and avoids long-lived credentials.\n\n\nvia - https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\n\nCreate an Amazon S3 VPC endpoint in the VPC where the application is hosted to enable private connectivity between the application and S3\n\nThe application is running in a private subnet, which lacks internet access. Without internet access or a NAT gateway, the application cannot reach public AWS services like Amazon S3 unless an alternative route is provided. A VPC Gateway Endpoint for S3 enables private, secure communication between the VPC and S3 over the AWS backbone, without requiring internet access or exposing traffic to the public internet. This also ensures lower latency and enhances security by avoiding traversal through external networks. It is a highly recommended practice when workloads in private subnets need access to S3, especially in regulated or production environments.\n\nIncorrect options:\n\nUse the existing Amazon Cognito user pool to directly grant users permission to upload and download objects in the S3 bucket - Amazon Cognito user pools provide authentication, not authorization for AWS resources like S3. You cannot directly assign S3 permissions using user pools alone. You must integrate the user pool with an identity pool, which generates temporary AWS credentials mapped to IAM roles with S3 access. This option omits a critical step and would not allow secure access to S3.\n\n\nvia - https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\n\nConfigure an AWS Lambda function that proxies user uploads to S3. Invoke the Lambda function after each user login to isolate the S3 access - While technically possible, using a Lambda function as a proxy for every upload or download introduces unnecessary latency, cost, and operational complexity. It also doesn't leverage native Cognito + IAM integration for scalable, fine-grained access control. This pattern may be used in tightly controlled environments but is not ideal or scalable for typical user-based S3 access patterns.\n\nAttach an S3 bucket policy that allows access only if requests include a custom HTTP header containing a valid Cognito user ID - While adding conditional logic to S3 bucket policies using request headers might sound like a custom access control strategy, Amazon S3 bucket policies do not natively support evaluating Cognito user IDs via custom headers for authorization. S3 supports conditions based on IAM policy context keys, such as aws:userid or aws:username, but those must be passed via authenticated requests with IAM credentials, not manually crafted headers. Trusting user-provided headers introduces security risks and does not securely validate identity. Proper integration relies on IAM roles tied to Cognito identity pools, not custom header logic.\n\nReferences:\n\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html\n\nhttps://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html#amazons3-policy-keys\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q676",
    "questionText": "A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would like you, the solution architect, to advise them on the encryption scheme to adopt.\n\nWhat do you recommend?",
    "options": [
      {
        "text": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "isCorrect": false
      },
      {
        "text": "Server-side encryption with AWS KMS keys (SSE-KMS)",
        "isCorrect": false
      },
      {
        "text": "Server-side encryption with customer-provided keys (SSE-C)",
        "isCorrect": false
      },
      {
        "text": "Client Side Encryption",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nClient Side Encryption\n\nClient-side encryption is the act of encrypting your data locally to help ensure its security in transit and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client. When your objects are encrypted in this manner, your objects aren't exposed to any third party, including AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as being encrypted, it only detects typical objects.\n\nIncorrect options:\n\nServer-side encryption with AWS KMS keys (SSE-KMS) - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom.\n\nServer-side encryption with Amazon S3 managed keys (SSE-S3) - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key.\n\nServer-side encryption with customer-provided keys (SSE-C) - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q677",
    "questionText": "A retail startup runs a high-traffic order processing system on AWS. The architecture includes a frontend web tier using EC2 instances behind an Application Load Balancer, a processing tier powered by EC2 instances, and a data layer using Amazon DynamoDB. The frontend and processing tiers are decoupled using Amazon SQS. Recently, the engineering team observed that during unpredictable traffic surges, order processing slows down significantly, SQS queue depth increases rapidly, and the processing-tier EC2 instances hit 100% CPU usage.\n\nWhich solution will help improve the applicationâ€™s responsiveness and scalability during peak load periods?",
    "options": [
      {
        "text": "Use Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue",
        "isCorrect": false
      },
      {
        "text": "Add Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput",
        "isCorrect": false
      },
      {
        "text": "Use an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue",
        "isCorrect": true
      },
      {
        "text": "Use scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse an EC2 Auto Scaling group with a target tracking policy to automatically scale the processing tier. Configure the policy to monitor the ApproximateNumberOfMessages in the SQS queue\n\nThis option directly addresses the root cause of the performance issue - CPU saturation in the processing tier and message backlog in the SQS queue - while aligning with best practices for decoupled architectures on AWS. In this design, Amazon EC2 Auto Scaling uses a target tracking policy to dynamically adjust the number of EC2 instances based on the ApproximateNumberOfMessages metric from the SQS queue. As the queue depth grows due to surging customer activity, Auto Scaling automatically launches additional worker instances to handle the increased load, ensuring that the backlog is processed quickly and efficiently. This approach is ideal for workloads with unpredictable or bursty traffic patterns, and it removes the need for manual intervention or fixed scaling schedules. It ensures better responsiveness, system resilience, and cost efficiency.\n\nIncorrect options:\n\nUse Amazon EventBridge to schedule batch processing jobs for the queue. Configure the event rule to invoke EC2-based workers every 10 minutes to process messages in the SQS queue - While Amazon EventBridge can trigger actions on a schedule, this design is incompatible with real-time, high-volume message processing requirements. Using a fixed interval batch model (e.g., every 10 minutes) delays response time and fails to handle variable or sudden traffic spikes effectively. SQS is built for near-real-time decoupling, and EC2 workers should be automatically scaled based on queue depth, not on static time-based triggers. This method would cause inconsistent latency and accumulation of unprocessed messages under load.\n\nAdd Amazon Kinesis Data Streams to buffer order events from the web tier. Configure the processing tier to consume records from the stream and use enhanced fan-out for high throughput - Although Amazon Kinesis Data Streams is a powerful solution for high-throughput event ingestion, this approach changes the core architecture and introduces unnecessary complexity for a system that already uses Amazon SQS for decoupling. The challenge in the scenario is not the ingestion layer but the under-provisioned EC2 processing tier that fails to scale during unpredictable peak traffic. Migrating to Kinesis requires new consumers, partitioning logic, and data retention management, which is not justified when the existing queue-based design can be easily scaled using Auto Scaling policies tied to the SQS queue depth. Therefore, switching to Kinesis adds operational burden without solving the root cause: CPU bottlenecks in the processing tier.\n\nUse scheduled Auto Scaling for the processing tier based on past peak periods. Use average CPU utilization to define scaling thresholds - Scheduled scaling assumes that peak times are predictable, but the scenario explicitly states they are variable and unpredictable. While CPU utilization is a valid metric, it lags behind and only reflects stress once it occurs, not proactively. Also, scheduled scaling doesnâ€™t adapt in real-time, making it ineffective for responding to dynamic queue growth.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-scaling.html\n\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html\n\nhttps://docs.aws.amazon.com/streams/latest/dev/introduction.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q678",
    "questionText": "A startup wants to create a highly available architecture for its multi-tier application. Currently, the startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these requirements while minimizing the underlying infrastructure maintenance effort.\n\nWhat will you recommend?",
    "options": [
      {
        "text": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone",
        "isCorrect": false
      },
      {
        "text": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "isCorrect": true
      },
      {
        "text": "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
        "isCorrect": false
      },
      {
        "text": "Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration\n\nAmazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.\n\n\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nApplication Load Balancer automatically distributes your incoming traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets.\n\n\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n\nIn a multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous â€œstandbyâ€ replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby to keep both in sync and protect your latest database updates against DB instance failure.\n\n\nvia - https://aws.amazon.com/rds/features/multi-az/\n\nTo create a highly available architecture for the given use case, you need to set up an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones and then point the Application Load Balancer to the target group having the Amazon EC2 instances.\n\nIncorrect options:\n\nCreate an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone - A read replica cannot be used to enhance the availability of an Amazon RDS MySQL DB. You must use the multi-AZ configuration of Amazon RDS MySQL for this use case.\n\nCreate an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration - Having the Amazon EC2 instances in a single Availability Zone will not create a highly available solution. In the case of an outage for the entire Availability Zone, the Amazon EC2 instances would be unreachable. Hence this option is incorrect.\n\nProvision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs - This option has been added as a distractor. It requires significant monitoring and development effort to keep the Amazon EC2 instances highly available as well as keep the MySQL DBs in sync.\n\nReferences:\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n\nhttps://aws.amazon.com/rds/features/multi-az/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q679",
    "questionText": "As a Solutions Architect, you would like to completely secure the communications between your Amazon CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly.\n\nWhat do you recommend?",
    "options": [
      {
        "text": "Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution",
        "isCorrect": false
      },
      {
        "text": "Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group",
        "isCorrect": false
      },
      {
        "text": "Make the Amazon S3 bucket public",
        "isCorrect": false
      },
      {
        "text": "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy",
        "isCorrect": true
      }
    ],
    "explanation": "Correct option:\n\nCreate an origin access identity (OAI) and update the Amazon S3 Bucket Policy\n\nTo restrict access to content that you serve from Amazon S3 buckets, you need to follow the following steps:\n\n\nCreate a special Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution.\nConfigure your Amazon S3 bucket permissions so that Amazon CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users canâ€™t use a direct URL to the Amazon S3 bucket to access a file there.\n\n\nAfter you take these steps, users can only access your files through Amazon CloudFront, not directly from the Amazon S3 bucket.\n\nIn general, if youâ€™re using an Amazon S3 bucket as the origin for a Amazon CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you restrict access by using, for example, Amazon CloudFront signed URLs or signed cookies, you also wonâ€™t want people to be able to view files by simply using the direct Amazon S3 URL for the file. Instead, you want them to only access the files by using the Amazon CloudFront URL, so your content remains protected.\n\nIncorrect options:\n\nUpdate the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront security group - Amazon S3 buckets don't have security groups, hence this is an incorrect option.\n\nMake the Amazon S3 bucket public - If the Amazon S3 bucket is made public, it can be accessed by anyone directly. This is not the requirement.\n\nCreate a bucket policy to only authorize the IAM role attached to the Amazon CloudFront distribution -  You cannot attach IAM roles to the Amazon CloudFront distribution. Here you need to use an OAI.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q680",
    "questionText": "A media company is modernizing its legacy image processing application by migrating it from an on-premises environment to AWS. The application handles a high volume of image transformation jobs, generating large output files. To support rapid growth, the company wants a cloud-native solution that automatically scales, minimizes manual intervention, and avoids managing servers or infrastructure. The team also wants to improve workflow automation to handle task sequencing and job state transitions.\n\nWhich solution best meets these requirements while ensuring the least operational overhead?",
    "options": [
      {
        "text": "Deploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes",
        "isCorrect": false
      },
      {
        "text": "Use AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3",
        "isCorrect": true
      },
      {
        "text": "Use a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx",
        "isCorrect": false
      },
      {
        "text": "Use Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse AWS Batch to process image jobs. Orchestrate the workflow using AWS Step Functions and store output files in Amazon S3\n\nThis solution offers the least operational overhead and fully leverages AWS-managed services. AWS Batch automatically provisions and scales compute resources for job execution without requiring infrastructure management. Step Functions provides a visual, serverless way to orchestrate complex workflows with built-in state management and retries. Amazon S3 is ideal for storing large output files due to its durability, scalability, and integration with Batch and Step Functions. This stack meets all requirements: scalable, automated, and infrastructure-free.\n\nIncorrect options:\n\nDeploy Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed EC2 worker nodes for image processing. Use Amazon SQS to queue jobs and store processed outputs in Amazon EBS volumes - While Amazon EKS offers powerful orchestration for containerized workloads, self-managed EC2 nodes introduce significant infrastructure management burdenâ€”contradicting the company's goal of minimal operational overhead. This setup requires patching, capacity planning, and node lifecycle management. Additionally, using Amazon EBS for storing large shared outputs is not optimal for distributed processing scenarios. The operational complexity of EKS with EC2 makes this solution less desirable for a serverless-first use case.\n\nUse a combination of AWS Lambda functions and EC2 Spot Instances for processing. Store processed images in Amazon FSx - Although AWS Lambda is great for lightweight processing, it's not designed for compute-heavy or long-running image processing workloads - especially those involving large file sizes. EC2 Spot Instances are cost-effective but can be interrupted at any time, making them risky for non-idempotent or large processing jobs. Additionally, Amazon FSx is better suited for Windows-based or high-performance POSIX-compliant file systems, which adds complexity unless strictly required. This hybrid setup also requires managing Spot capacity and lifecycle.\n\nUse Amazon EC2 Auto Scaling groups with a static fleet of instances for image processing. Trigger each job through Step Functions and store results on attached EBS volumes - Even though EC2 Auto Scaling automates instance launch and termination, it still requires manual management of instance images, EBS volume provisioning, software patching, and monitoring. The need to orchestrate instance behavior and EBS lifecycle management increases operational effort. EBS is not a good fit for scalable or concurrent access to processed files, and managing storage at the instance level introduces complexity. This architecture contradicts the requirement to avoid infrastructure management.\n\nReferences:\n\nhttps://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html\n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\n\nhttps://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q681",
    "questionText": "The systems administrator at a company wants to set up a highly available architecture for a bastion host solution.\n\nAs a solutions architect, which of the following options would you recommend as the solution?",
    "options": [
      {
        "text": "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "isCorrect": true
      },
      {
        "text": "Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "isCorrect": false
      },
      {
        "text": "Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "isCorrect": false
      },
      {
        "text": "Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group\n\nNetwork Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers â€“ within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.\n\nIncluding bastion hosts in your VPC environment enables you to securely connect to your Linux instances without exposing your environment to the Internet. After you set up your bastion hosts, you can access the other instances in your VPC through Secure Shell (SSH) connections on Linux. Bastion hosts are also configured with security groups to provide fine-grained ingress control.\n\nYou need to remember that Bastion Hosts are using the SSH protocol, which is a TCP based protocol on port 22. They must be publicly accessible.\n\nHere, the correct answer is to use a Network Load Balancer, which supports TCP traffic, and will automatically allow you to connect to the Amazon EC2 instance in the backend.\n\nIncorrect options:\n\nCreate an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group - An elastic IP address (EIP) can only be attached to one Amazon EC2 instance at a time, so it won't provide you a highly available setup on its own. Note that if we had two Elastic IPs and two Bastion Hosts, this would work.\n\nCreate a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.\n\nVPC Endpoints are not used on top of Amazon EC2 instances. They're a way to access AWS services privately within your VPC (without using the public internet). This is a distractor.\n\nCreate a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts managed by an Auto Scaling Group - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets â€“ Amazon EC2 instances, containers, IP addresses and AWS Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nAn Application Load Balancer only supports HTTP traffic, which is layer 7, while the SSH protocol is based on TCP and is layer 4. So, the Application Load Balancer doesn't work.\n\nReferences:\n\nhttps://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q682",
    "questionText": "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an Availability Zone (AZ).\n\nWhich of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
    "options": [
      {
        "text": "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1",
        "isCorrect": true
      },
      {
        "text": "Assign an Amazon EC2 Instance Role to perform the necessary API calls",
        "isCorrect": true
      },
      {
        "text": "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it",
        "isCorrect": true
      },
      {
        "text": "Create a Spot Fleet request",
        "isCorrect": false
      },
      {
        "text": "Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group",
        "isCorrect": false
      },
      {
        "text": "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nCreate an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1\n\nAmazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.\n\nSo we have an Auto Scaling Group with desired=1, across two AZ, so that if an instance goes down, it is automatically recreated in another AZ. So this option is correct.\n\nCreate an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it\n\nApplication Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets â€“ Amazon EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nAn Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.\n\nNow, between the ALB and the Elastic IP. If we use an ALB, things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one Amazon EC2 instance. Instead, to minimize costs, we must use an Elastic IP.\n\nAssign an Amazon EC2 Instance Role to perform the necessary API calls\n\nFor that Elastic IP to be attached to our Amazon EC2 instance, we must use an EC2 user data script, and our Amazon EC2 instance must have the correct IAM permissions to perform the API call, so we need an Amazon EC2 instance role.\n\nIncorrect options:\n\nCreate a Spot Fleet request - A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price.\n\nThe Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.\n\nSpot Fleets requests would not fit our purpose as we are looking at a critical application. Spot instances can be terminated. So this option is incorrect.\n\nCreate an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2 - An Auto Scaling Group with desired=2 would create two instances, and this won't work for us as our monolith application is not made to work with two instances as per the given use-case.\n\nCreate an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group - If we use an Application Load Balancer (ALB), things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one Amazon EC2 instance. So this option is not correct.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q683",
    "questionText": "A global photography startup hosts a static image-sharing site on an Amazon S3 bucket. The website allows users from different parts of the world to upload, view, and download photos through their mobile devices. As the platform has gained popularity, users have started experiencing latency issues, especially when uploading and downloading images. The team needs a solution to enhance global performance but wants to implement it with minimal development effort and without redesigning the application.\n\nWhich solution will most effectively address the performance issues with the least operational overhead?",
    "options": [
      {
        "text": "Deploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users",
        "isCorrect": true
      },
      {
        "text": "Migrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency",
        "isCorrect": false
      },
      {
        "text": "Create multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket",
        "isCorrect": false
      },
      {
        "text": "Enable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nDeploy an Amazon CloudFront distribution with the S3 bucket as the origin to improve download speeds. Enable S3 Transfer Acceleration to reduce upload latency for global users\n\nThis solution provides a low-effort, AWS-native solution that significantly improves the performance of both downloads and uploads for a global user baseâ€”without requiring changes to the application architecture. By configuring Amazon CloudFront, a content delivery network (CDN), the company ensures that image downloads are served from edge locations close to the users, reducing latency and improving load times. To address upload performance, enabling S3 Transfer Acceleration allows users to upload images via AWS edge locations, which then route data over optimized AWS backbone networks to the S3 bucket. This minimizes latency for uploads, especially from distant regions. Both features are simple to enable, require no infrastructure provisioning, and are specifically designed for S3-based static sitesâ€”making this solution the most effective and efficient choice.\n\nIncorrect options:\n\nMigrate the website from S3 to Amazon EC2 instances in multiple Regions. Use an Application Load Balancer with AWS Global Accelerator to distribute global traffic and reduce latency - While technically feasible, this approach involves significant infrastructure changes, including deploying and managing EC2 instances, setting up web servers, maintaining application code, and configuring load balancing. It also deviates from the static website model hosted in S3, increasing operational complexity and cost. This option is over-engineered for the problem and does not meet the requirement for minimal implementation effort.\n\nCreate multiple S3 buckets in different Regions and replicate image data based on user location. Configure CloudFront to upload and download from the nearest bucket - This approach attempts to manually geo-distribute data using cross-Region replication and routing logic. However, it introduces significant configuration complexity, including replication management, custom logic to determine upload destinations, and download redirection. Additionally, CloudFront is designed for read-only access, not for direct uploads. Uploads via CloudFront require advanced signed URLs and are not recommended for typical user-submitted content. This option fails the simplicity requirement.\n\nEnable AWS Global Accelerator on the S3 bucket to accelerate both uploads and downloads. Reconfigure the website to route requests through the accelerator - AWS Global Accelerator does not support S3 buckets as direct endpoints. Itâ€™s designed for improving performance for global applications deployed on EC2, ALB, or NLB, not for S3 static websites. Thereâ€™s no direct integration between Global Accelerator and S3 unless the S3 bucket is fronted by another service like CloudFront or ALB. Therefore, this option is not technically valid and doesnâ€™t meet the stated requirements.\n\nReferences:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html\n\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/about-endpoints.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design High-Performing Architectures"
  },
  {
    "id": "q684",
    "questionText": "You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The replication protocol currently uses the Amazon EC2 public IP addresses.\n\nWhat can you do to decrease the replication cost?",
    "options": [
      {
        "text": "Use the Amazon EC2 instances private IP for the replication",
        "isCorrect": true
      },
      {
        "text": "Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication",
        "isCorrect": false
      },
      {
        "text": "Create a Private Link between the two Amazon EC2 instances",
        "isCorrect": false
      },
      {
        "text": "Use an Elastic Fabric Adapter (EFA)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse the Amazon EC2 instances private IP for the replication\n\nThe source of the cost is that traffic between two EC2 instances is going over the public internet, thus incurring high costs. Here, the correct answer is to use Private IP, so that the network remains private, for a minimal cost.\n\nIncorrect options:\n\nAssign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication - Using Elastic IPs will not solve the problem as the traffic will still be going over the public internet.\n\nCreate a Private Link between the two Amazon EC2 instances - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.\n\nPrivate Link is a distractor in this question. Private Link is leveraged to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering and allowing the connections between the two to remain within the AWS network.\n\nUse an Elastic Fabric Adapter (EFA) - The Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run HPC applications requiring high levels of inter-instance communications, like computational fluid dynamics, weather modeling, and reservoir simulation, at scale on AWS. This option is not relevant to the given use-case.\n\nReferences:\n\nhttps://aws.amazon.com/privatelink/\n\nhttps://aws.amazon.com/hpc/efa/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q685",
    "questionText": "Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second.\n\nHow can you efficiently prevent attackers from overwhelming your application?",
    "options": [
      {
        "text": "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule",
        "isCorrect": true
      },
      {
        "text": "Use AWS Shield Advanced and setup a rate-based rule",
        "isCorrect": false
      },
      {
        "text": "Define a network access control list (network ACL) on your Application Load Balancer",
        "isCorrect": false
      },
      {
        "text": "Configure Sticky Sessions on the Application Load Balancer",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nUse an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule\n\nAWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.\n\nThe correct answer is to use WAF (which has integration on top of your ALB) and define a rate-based rule.\n\nIncorrect options:\n\nConfigure Sticky Sessions on the Application Load Balancer - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets â€“ Amazon EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\n\nSticky Sessions on your Application Load Balancer is a distractor here. Sticky sessions are a mechanism to route requests from the same client to the same target. Application Load Balancer supports sticky sessions using load balancer generated cookies. If you enable sticky sessions, the same target receives the request and can use the cookie to recover the session context.\n\nDefine a network access control list (network ACL) on your Application Load Balancer - A network access control list (network ACL) does not work, as this only helps to block specific IPs. On top of things, network access control list (network ACL) is defined at the subnet level, and not for an Application Load Balancer.\n\nUse AWS Shield Advanced and setup a rate-based rule - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield - Standard and Advanced.\n\nAWS Shield Advanced provides enhanced resource-specific detection and employs advanced mitigation and routing techniques for sophisticated or larger attacks.\n\nAWS Shield Advanced will give you DDoS protection overall, and you cannot set up rate-based rules in Shield.\n\nReferences:\n\nhttps://aws.amazon.com/waf/\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\n\nhttps://aws.amazon.com/shield/\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  },
  {
    "id": "q686",
    "questionText": "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS.\n\nAs a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?",
    "options": [
      {
        "text": "Amazon Simple Notification Service (Amazon SNS)",
        "isCorrect": false
      },
      {
        "text": "Amazon Simple Queue Service (Amazon SQS) Standard",
        "isCorrect": false
      },
      {
        "text": "Amazon MQ",
        "isCorrect": true
      },
      {
        "text": "Amazon SQS FIFO (First-In-First-Out)",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nAmazon MQ\n\nAmazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systemsâ€“often using different programming languages, and on different platformsâ€“to communicate and exchange information. If an organization is using messaging with existing applications and wants to move the messaging service to the cloud quickly and easily, AWS recommends Amazon MQ for such a use case. So this is the correct option.\n\nIncorrect options:\n\nAmazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS does not provide support for migration from RabbitMQ as its a fully managed pub/sub messaging service. Hence this option is incorrect.\n\nAmazon Simple Queue Service (Amazon SQS) Standard - Amazon SQS Standard offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. SQS Standard does not provide support for migration from RabbitMQ. Hence this option is incorrect.\n\nAmazon SQS FIFO (First-In-First-Out) - Amazon SQS FIFO (First-In-First-Out) has all the capabilities of the standard queue. They are used when the order of operations and events is critical, or where duplicates can't be tolerated. SQS FIFO does not provide support for migration from RabbitMQ. Hence this option is incorrect.\n\nReference:\n\nhttps://aws.amazon.com/amazon-mq/\n\nhttps://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/",
    "awsService": "SNS",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q687",
    "questionText": "A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)",
    "options": [
      {
        "text": "Use Amazon CloudFront signed URLs",
        "isCorrect": true
      },
      {
        "text": "Require HTTPS for communication between Amazon CloudFront and your custom origin",
        "isCorrect": false
      },
      {
        "text": "Require HTTPS for communication between Amazon CloudFront and your S3 origin",
        "isCorrect": false
      },
      {
        "text": "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers",
        "isCorrect": false
      },
      {
        "text": "Use Amazon CloudFront signed cookies",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nUse Amazon CloudFront signed URLs\n\nMany companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee.\n\nTo securely serve this private content by using Amazon CloudFront, you can do the following:\n\nRequire that your users access your private content by using special Amazon CloudFront signed URLs or signed cookies.\n\nA signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content.\n\nSo this is a correct option.\n\nUse Amazon CloudFront signed cookies\n\nAmazon CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website.\n\nSo this is also a correct option.\n\nIncorrect options:\n\nRequire HTTPS for communication between Amazon CloudFront and your custom origin\n\nRequire HTTPS for communication between Amazon CloudFront and your S3 origin\n\nRequiring HTTPS for communication between Amazon CloudFront and your custom origin (or S3 origin) only enables secure access to the underlying content.\n\nYou cannot use HTTPS to restrict access to your private content.\n\nSo both these options are incorrect.\n\nForward HTTPS requests to the origin server by using the ECDSA or RSA ciphers\n\nThis option is just added as a distractor.\n\nYou cannot use HTTPS to restrict access to your private content.\n\nReference:\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html",
    "awsService": "S3",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q688",
    "questionText": "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to accomplish this.\n\nWhich of the following settings of the VPC need to be enabled? (Select two)",
    "options": [
      {
        "text": "enableVpcSupport",
        "isCorrect": false
      },
      {
        "text": "enableVpcHostnames",
        "isCorrect": false
      },
      {
        "text": "enableDnsHostnames",
        "isCorrect": true
      },
      {
        "text": "enableDnsDomain",
        "isCorrect": false
      },
      {
        "text": "enableDnsSupport",
        "isCorrect": true
      }
    ],
    "explanation": "Correct options:\n\nenableDnsHostnames\n\nenableDnsSupport\n\nA private hosted zone is a container for records for a domain that you host in one or more Amazon virtual private clouds (VPCs). You create a hosted zone for a domain (such as example.com), and then you create records to tell Amazon Route 53 how you want traffic to be routed for that domain within and among your VPCs.\n\nFor each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true:\n\nenableDnsHostnames\n\nenableDnsSupport\n\nIncorrect options:\n\nenableVpcSupport\n\nenableVpcHostnames\n\nenableDnsDomain\n\nThe options enableVpcSupport, enableVpcHostnames and enableDnsDomain have been added as distractors.\n\nReference:\n\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html",
    "awsService": "VPC",
    "source": "practice",
    "section": "Design Resilient Architectures"
  },
  {
    "id": "q689",
    "questionText": "A healthcare analytics firm operates a backend application within a private subnet of its VPC. The application is fronted by an Application Load Balancer (ALB) and accesses Amazon S3 to store medical reports. The VPC includes both a NAT gateway and an internet gateway, but the company's strict compliance policy prohibits any data traffic from traversing the internet. The team must redesign the architecture to comply with the security policy and improve cost-efficiency.\n\nWhich solution best satisfies these requirements in the most cost-effective manner?",
    "options": [
      {
        "text": "Create an S3 interface VPC endpoint and modify the security group to allow access from the applicationâ€™s private subnet. Route all S3 traffic through the interface endpoint",
        "isCorrect": false
      },
      {
        "text": "Modify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway",
        "isCorrect": false
      },
      {
        "text": "Create a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint",
        "isCorrect": true
      },
      {
        "text": "Create a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances",
        "isCorrect": false
      }
    ],
    "explanation": "Correct options:\n\nCreate a gateway VPC endpoint for Amazon S3 and update the route table for the private subnet to direct S3 traffic through the endpoint\n\nThis solution provides a secure and cost-effective method for private workloads in a VPC to access Amazon S3 without routing traffic over the internet. A gateway VPC endpoint for S3 integrates directly with the VPC route tables, enabling communication between EC2 instances and S3 over the AWS private network. This eliminates the need for a NAT gateway or internet gateway for S3 API calls, aligning with the company's security policy that prohibits internet-based traffic. Moreover, gateway endpoints are free of hourly and per-GB data processing charges, making them more cost-efficient than interface endpoints or NAT-based solutions. This solution offers high scalability, no operational complexity, and strong compliance with security requirements.\n\n\nvia - https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html\n\nIncorrect options:\n\nCreate an S3 interface VPC endpoint and modify the security group to allow access from the applicationâ€™s private subnet. Route all S3 traffic through the interface endpoint - While an interface VPC endpoint (powered by AWS PrivateLink) is valid for many AWS services, it is not the most cost-effective or appropriate choice for accessing Amazon S3. For S3, AWS recommends using a gateway VPC endpoint, which is purpose-built, incurs no hourly data processing charges, and integrates directly with route tables. In contrast, interface endpoints for S3 introduce per-hour and per-GB data processing fees and are more commonly used when you need fine-grained control with security groups, and not for high-volume object storage access. Choosing an interface endpoint here violates both the cost-efficiency and minimal overhead goals of the question.\n\n\nvia - https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-endpoints-for-s3\n\nModify the S3 bucket policy to allow requests only from the Elastic IP address associated with the NAT gateway - While this restricts access from specific IPs, it still causes S3 API calls to traverse the public internet via the NAT gateway, violating the security policy that prohibits internet-based traffic. Furthermore, it incurs additional NAT gateway data processing costs. This approach may restrict access, but does not solve the core compliance issue.\n\nCreate a VPC peering connection with another VPC that has direct access to S3. Forward the S3 API requests through the peered VPC using proxy EC2 instances - This approach is overly complex and expensive. VPC peering does not support transitive routing or S3-specific integration. Proxying S3 traffic through EC2 instances introduces unnecessary latency, potential bottlenecks, and violates AWS's best practice of using native integration points like VPC endpoints for accessing S3. Additionally, it involves managing EC2 proxy infrastructure, which increases cost and operational burden.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-endpoints-for-s3\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Cost-Optimized Architectures"
  },
  {
    "id": "q690",
    "questionText": "A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on Amazon S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a Amazon DynamoDB table.\n\nHow would you go about providing private access to these AWS resources which are not part of this custom VPC?",
    "options": [
      {
        "text": "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC",
        "isCorrect": false
      },
      {
        "text": "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC",
        "isCorrect": true
      },
      {
        "text": "Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address",
        "isCorrect": false
      },
      {
        "text": "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC",
        "isCorrect": false
      }
    ],
    "explanation": "Correct option:\n\nCreate a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC\n\nEndpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.\n\nA VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.\n\nThere are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.\n\nA gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:\n\nAmazon S3\n\nAmazon DynamoDB\n\nIncorrect options:\n\nCreate a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for Amazon DynamoDB and then add it as a target in the route table of the custom VPC\n\nCreate a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these services by adding these as targets in the route table of the custom VPC\n\nAmazon DynamoDB supports AWS PrivateLink. With AWS PrivateLink, you can simplify private network connectivity between virtual private clouds (VPCs), DynamoDB, and your on-premises data centers using interface VPC endpoints and private IP addresses. So, Amazon DynamoDB supports both interface endpoints as well as gateway endpoints. However, to use the interface endpoints, you need to connect to the given services using the private IP address, instead of creating an entry as a target in the route table of the custom VPC. Therefore, both these options are incorrect.\n\nCreate a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the private IP address - Origin Access Identity (OAI) is used within the context of Amazon CloudFront. To restrict access to content that you serve from Amazon S3 buckets, you can create a special Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution. You cannot use OAI to facilitate access to Amazon S3 from a VPC.\n\nReferences:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\n\nhttps://aws.amazon.com/about-aws/whats-new/2024/03/amazon-dynamodb-aws-privatelink/",
    "awsService": "EC2",
    "source": "practice",
    "section": "Design Secure Architectures"
  }
]